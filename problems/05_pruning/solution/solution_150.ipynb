{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gbfEkN0xaW0"
   },
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLD4yli0xaXD"
   },
   "source": [
    "![pruning.png](attachment:pruning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G6Wrhn3xaXG"
   },
   "source": [
    "## Wstęp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXj-mYZIxaXJ"
   },
   "source": [
    "Deep Learning to dziedzina mocno eksperymentalna, a co za tym idzie, dla danego problemu może istnieć wiele zadowalających rozwiązań (zazwyczaj nieoptymalnych). Często zdarza się, że architektury sieci neuronowych są nieproporcjonalnie duże do stopnia skomplikowania zadania. Okazuje się, że możemy odchudzić modele z tylko niewielką stratą poprawności predykcji.\n",
    "\n",
    "**Pruning** (przycinanie) sieci polega na usunięciu poszczególnych wag lub całych neuronów. Istnieje wiele zalet tej metody:\n",
    "- zmniejszenie rozmiaru sieci,\n",
    "- przyspieszenie inferencji,\n",
    "- przeciwdziałanie przeuczeniu,\n",
    "- polepszenie wyników.\n",
    "\n",
    "Aby skutecznie zmniejszyć rozmiar sieci, musimy wyzerować odpowiednio dużo elementów w jej macierzach wag. Dzięki temu, będziemy w stanie lepiej skompresować model w pamięci. Jednak samo wyzerowanie wag nie wystarczy do przyspieszenia inferencji. Należy dodatkowo zaimplementować i skutecznie wykorzystać liczenie macierzy rzadkich (Sparse Matrix). Inną metodą przycinania może być usuwanie całych neuronów - zmniejszając w ten sposób faktyczny rozmiar macierzy wag.\n",
    "\n",
    "W tym zadaniu skupimy się **tylko** na **zerowaniu wag** w modelu. **Nie można zmieniać architektury sieci** (np. poprzez usunięcie neuronu lub całej ukrytej warstwy). Rozpatrzymy ten problem na przykładzie **regresji**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_i6SISRxaXL"
   },
   "source": [
    "## Zadanie\n",
    "\n",
    "Zaimplementuj funkcję `your_pruning_algorithm(model : torch.nn.Module) -> pruned_model: torch.nn.Module`, która przyjmie na wejściu zaimplementowany poniżej model i zwróci jego wyczyszczoną wersję - tzn. z jak największą liczbą wyzerowanych parametrów modelu (wag i stałych [weights and biases]), przy zachowaniu jak najniższego średniego błędu kwadratowego (MSE) predykcji.\n",
    "\n",
    "Poniżej w notatniku znajdziesz komórkę, w której znajduje się miejsce na Twoją funkcję. Komórki, które będziesz miał zmodyfikować będą bardzo jasno oznaczone!\n",
    "\n",
    "Oceniany będziesz na podstawie wyniku poniższej funkcji (im wyższa wartość tym lepiej):\n",
    "\n",
    "$$\n",
    "\\mathrm{score}(s, \\epsilon) = \\begin{cases}\n",
    "    0 & \\text{jeżeli } \\epsilon > 1000 \\\\\n",
    "    (1 - \\frac{\\epsilon}{1000})^{1.5} \\cdot s ^{1.5} & \\text{w.p.p.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "- $s$ - liczba zerowych parametrów modelu podzielona przez liczbę wszystkich parametrów modelu (sparsity)\n",
    "- $\\epsilon$ - średni błąd kwadratowy na zbiorze testowym (MSE)\n",
    "\n",
    "To kryterium i wszystkie funkcje, o których mowa powyżej, są zaimplementowane poniżej przez nas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ympxzU8exaXO"
   },
   "source": [
    "## Ograniczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VQmP6hRxaXP"
   },
   "source": [
    "- Twoja funkcja powinna zwracać model w maksymalnie 5 minut używając Google Colab z GPU.\n",
    "- Plik z wagami powinien być zapisany funkcją `save_parameters` pod nazwą `model_parameters.pkl`.\n",
    "\n",
    "- **Nie możesz** zmieniać architektury modelu, tzn. musi on mieć dokładnie:\n",
    "    - warstwę wejściową o rozmiarze 128\n",
    "    - warstwę ukrytą o rozmiarze 1024\n",
    "    - funkcję aktywacji Sigmoid\n",
    "    - warstwę wyjściową o rozmiarze 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrQ4kfBFxaXQ"
   },
   "source": [
    "## Pliki zgłoszeniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EshU3as2xaXR"
   },
   "source": [
    "* Ten notebook\n",
    "* Parametry (wagi) modelu, zapisane funkcją `save_parameters`. **Nie zmieniaj** nazwy wygenerowanego pliku: `model_parameters.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfKHYHwKxaXS"
   },
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFFf8_ycxaXT"
   },
   "source": [
    "Oceniony zostanie dostarczony przez Ciebie plik z wagami. Powinieneś dostarczyć jednak również działający notebook, który po uruchomieniu wszystkich komórek z flagą `FINAL_EVALUATION_MODE` ustawioną na `True` wyprodukuje plik z wagami `model_parameters.pkl` w czasie poniżej 5 minut (liczonym na Google Colab z dostępem do GPU).\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 1.5 punkta. Jeśli dostaniesz `score` poniżej 0.085, to dostaniesz 0, a jeśli powyżej 0.95 to dostaniesz 1.5 punkt. Pomiędzy tymi wartościami, twój wynik rośnie liniowo z wartością `score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOxhj9Y-I3jp"
   },
   "source": [
    "# Kod startowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkZTjzUsxaXW"
   },
   "outputs": [],
   "source": [
    "FINAL_EVALUATION_MODE = False  # Podczas sprawdzania ustawimy tą flagę na True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzYwVRMKxaXZ"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGPyTemjs4Hf"
   },
   "source": [
    "## Ładowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGT6a6HNLUsb"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "# Funkcja ładująca dane treningowe i walidacyjne jako np.array\n",
    "def load_data_from_file(x_train_path, y_train_path, x_valid_path, y_valid_path):\n",
    "    X_train = np.load(x_train_path)\n",
    "    y_train = np.load(y_train_path)\n",
    "\n",
    "    X_valid = np.load(x_valid_path)\n",
    "    y_valid = np.load(y_valid_path)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "# Klasa zbioru dancyh\n",
    "class InMemDataset(Dataset):\n",
    "    def __init__(self, xs, ys, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.dataset = []\n",
    "        for i in tqdm(range(len(xs))):\n",
    "            self.dataset.append(\n",
    "                (\n",
    "                    torch.tensor(xs[i]).to(device).float(),\n",
    "                    torch.tensor(ys[i]).to(device).float(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wUnuyGDJ7Lv"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Wczytajmy dane i stwórzmy dataloadery\n",
    "X_train, y_train, X_valid, y_valid = load_data_from_file(\n",
    "    \"train_data/X_train.npy\",\n",
    "    \"train_data/y_train.npy\",\n",
    "    \"valid_data/X_valid.npy\",\n",
    "    \"valid_data/y_valid.npy\",\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "_train = InMemDataset(X_train, y_train, device)\n",
    "\n",
    "_valid = InMemDataset(X_valid, y_valid, device)\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(_train, batch_size=batch_size, shuffle=True),\n",
    "    \"valid\": DataLoader(_valid, batch_size=batch_size, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvPGwD__xaYa"
   },
   "source": [
    "## Kod z kryterium oceniającym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdmTv4tBxaYa"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "# Całkowite kryterium zdefiniowane w treści zadania\n",
    "def score(mse_loss, sparsity, mse_weight=1.5, sparsity_weight=1.5):\n",
    "\n",
    "    if type(mse_loss) == np.ndarray:\n",
    "        mse_loss[mse_loss > 1000] = 1000\n",
    "    else:\n",
    "        if mse_loss > 1000:\n",
    "            mse_loss = 1000\n",
    "\n",
    "    score = (1 - mse_loss / 1000) ** mse_weight * sparsity**sparsity_weight\n",
    "    return score\n",
    "\n",
    "\n",
    "# Stosunek wyzerowanych wag do wszystkich\n",
    "\n",
    "\n",
    "def get_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "# Błąd średniokwadratowy (MSE)\n",
    "def compute_error(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    losses = 0\n",
    "    num_of_el = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = model(x)\n",
    "            num_of_el += x.shape[0] * y.shape[1]\n",
    "            losses += model.loss(outputs, y, reduction=\"sum\")\n",
    "\n",
    "    return losses / num_of_el\n",
    "\n",
    "\n",
    "def points(score):\n",
    "    def scale(x, lower=0.085, upper=0.95, max_points=1.5):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower) * max_points\n",
    "\n",
    "    return scale(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2xI0m8vSDPD"
   },
   "source": [
    "## Architektura Modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejmg01YQSDkP"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "# Zdefiniujmy architekturę naszej sieci\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, input, target, reduction=\"mean\"):\n",
    "        mse_loss = nn.MSELoss(reduction=reduction)\n",
    "        return mse_loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKHmJ6Vx_yvS"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "# Inicializacja wag sieci\n",
    "def init_weights(m):\n",
    "    \"\"\"Initialize the weights in the module m.\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# Funkcja do zapisywania wag modelu do pliku - pamiętaj, aby Twój plik z wagami nazywał się: model_parameters.pkl\n",
    "def save_parameters(model, file_name=\"model_parameters.pkl\", to_file=True):\n",
    "\n",
    "    params_to_save = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params_to_save[name] = param.to(\"cpu\")\n",
    "\n",
    "    if not to_file:\n",
    "        return params_to_save\n",
    "\n",
    "    with open(f\"{file_name}\", \"wb\") as f:\n",
    "        pickle.dump(params_to_save, f)\n",
    "\n",
    "\n",
    "# Funkcja do wczytywania wag modelu z pliku\n",
    "def load_parameters(\n",
    "    model, file_name=\"model_parameters.pkl\", from_file=True, params=None\n",
    "):\n",
    "\n",
    "    if from_file:\n",
    "        with open(f\"{file_name}\", \"rb\") as f:\n",
    "            params_to_load = pickle.load(f)\n",
    "    else:\n",
    "        params_to_load = params\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            param[...] = params_to_load[name].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxDq_UaTSIqE"
   },
   "source": [
    "## Trening Modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJDPgiFDA52R"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "# Funkcja do trenowania modelu\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    data_loaders: dict[str, DataLoader],\n",
    "    num_epochs: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[torch.Tensor, float]:\n",
    "    \"\"\"Funkcja do trenowania modelu.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Sieć neuronowa do wytrenowania.\n",
    "        data_loaders (dict[str, DataLoader]): Dictionary zawierający DataLoadery dla zbiorów treninowego i walidacyjnego.\n",
    "        num_epochs (int): Liczba epok do treningu.\n",
    "        optimizer (torch.optim.Optimizer): Optymalizator do trenowania modelu.\n",
    "        verbose (bool, optional): Jeżeli True, pokazuje postęp treningu.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, float]: Tuple zawierający najlepszy zestaw parametrów modelu znalezionych podczas treningu oraz odpowiadającą wartość funkcji straty na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "    if FINAL_EVALUATION_MODE:\n",
    "        verbose = False\n",
    "\n",
    "    best_epoch = None\n",
    "    best_params = None\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        _iter = 1\n",
    "        for inputs, targets in data_loaders[\"train\"]:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = model.loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if _iter % 10 == 0:\n",
    "                    print(f\"Minibatch {_iter:>6}    |  loss {loss.item():>5.2f}  |\")\n",
    "\n",
    "            _iter += 1\n",
    "\n",
    "        val_loss = compute_error(model, data_loaders[\"valid\"])\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_epoch = epoch\n",
    "            best_val_loss = val_loss\n",
    "            best_params = [copy.deepcopy(p.detach().cpu()) for p in model.parameters()]\n",
    "\n",
    "        if verbose:\n",
    "            clear_output(True)\n",
    "            m = f\"After epoch {epoch:>2} | valid loss: {val_loss:>5.2f}\"\n",
    "            print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))\n",
    "\n",
    "    if best_params is not None:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\nLoading best params on validation set in epoch {best_epoch} with loss {best_val_loss:.2f}\"\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            for param, best_param in zip(model.parameters(), best_params):\n",
    "                param[...] = best_param\n",
    "\n",
    "    return best_params, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "id3_39t7CoQH"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "initial_model = MLP().to(device)\n",
    "initial_model.apply(init_weights)\n",
    "\n",
    "optimizer = SGD(initial_model.parameters(), lr=0.01, momentum=0.95, weight_decay=0.001)\n",
    "\n",
    "best_params, best_val_loss = train_model(\n",
    "    initial_model, loaders, num_epochs=100, optimizer=optimizer, verbose=True\n",
    ")\n",
    "\n",
    "loss = compute_error(initial_model, loaders[\"valid\"])\n",
    "m = f\"| Validation loss: {loss:>5.2f} |\"\n",
    "print(\"{0}\\n{1}\\n{0}\".format(\"-\" * len(m), m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_74BJA4xaYi"
   },
   "source": [
    "# Przykładowe rozwiązanie\n",
    "Poniżej prezentujemy proste rozwiązanie, które w oczywisty sposób nie jest optymalne. Służy tylko temu, aby było wiadomo, w jaki sposób ma działać cały notatnik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH3pZhZ4xaYj"
   },
   "outputs": [],
   "source": [
    "def starter_pruning_algorithm(model):\n",
    "    with torch.no_grad():\n",
    "        model.layers[0].weight[:, 0:2] = 0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rKZnjHIxaYj"
   },
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    # Zróbmy głęboką kopię, aby nie zmieniać wag wytrenowanego modelu\n",
    "    model_to_prune = copy.deepcopy(initial_model)\n",
    "\n",
    "    # Przytnijmy wagi przykładowym rozwiązaniem\n",
    "    model_to_prune = starter_pruning_algorithm(model_to_prune)\n",
    "\n",
    "    # Zapisywanie parametrów modelu (tutaj zmieniliśmy nazwę pliku, Ty zapisuj jako \"model_parameters.pkl\")\n",
    "    save_parameters(model_to_prune, \"starter_model_parameters.pkl\")\n",
    "\n",
    "    # Zobaczmy teraz jak wczytać wcześniej zapisane wagi do nowo utworzonego modelu\n",
    "    new_model = MLP().to(device)\n",
    "    loss = compute_error(new_model, loaders[\"valid\"])\n",
    "    print(f\"Nowy model ma loss {loss:.3f}\")\n",
    "\n",
    "    # Wczytywanie parametrów modelu\n",
    "    load_parameters(new_model, \"starter_model_parameters.pkl\")\n",
    "    loss = compute_error(new_model, loaders[\"valid\"])\n",
    "    print(f\"Po wczytaniu parametrów model ma loss {loss:.3f}\")\n",
    "\n",
    "    mse = compute_error(new_model, loaders[\"valid\"])\n",
    "    sparsity = get_sparsity(new_model)\n",
    "\n",
    "    print(f\"MSE modelu: {mse:.3f} Sparsity: {sparsity:.3f}\")\n",
    "    model_score = score(mse, sparsity)\n",
    "    print(f\"Wynik twojego modelu to {model_score:.3f}!\")\n",
    "    print(f\"Twoje rozwiązanie zdobywa {points(model_score):.3f}/1.5 punktów!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqNhSEw5Z0Ak"
   },
   "source": [
    "# Twoje rozwiązanie\n",
    "\n",
    "Ta sekcja to jedyne miejsce, gdzie możesz zmieniać kod!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXc25J9-_r8J"
   },
   "outputs": [],
   "source": [
    "def your_pruning_algorithm(model):\n",
    "    \"\"\"\n",
    "    Zaawansowany algorytm przycinania sieci neuronowej wykorzystujący technikę aproksymacji\n",
    "    przez sieć liniową i strategiczne ustawianie wag.\n",
    "\n",
    "    Strategia:\n",
    "    1. Trenujemy prostą sieć liniową (bez warstw ukrytych) na tych samych danych\n",
    "    2. Wykorzystujemy wagi tej sieci do skonstruowania rzadkiej reprezentacji w oryginalnej sieci\n",
    "    3. Tworzymy \"mostek\" przez pierwszą warstwę używając małych wag na przekątnej\n",
    "    4. Kompensujemy skalowanie w ostatniej warstwie\n",
    "\n",
    "    Args:\n",
    "        model: Model MLP do przycięcia\n",
    "\n",
    "    Returns:\n",
    "        model: Przycięty model z zachowaną funkcjonalnością\n",
    "    \"\"\"\n",
    "\n",
    "    # Hiperparametry algorytmu\n",
    "    eps = 0.001  # Mała wartość dla wag \"mostka\" w pierwszej warstwie\n",
    "    gamma = 0.25  # Współczynnik skalowania dla kompensacji\n",
    "\n",
    "    # Import optymalizatora (umieszczony lokalnie aby nie zaśmiecać globalnej przestrzeni nazw)\n",
    "    from torch.optim import Adam\n",
    "\n",
    "    # KROK 1: Tworzenie i trenowanie prostej sieci liniowej\n",
    "    # Tworzymy sieć liniową bezpośrednio mapującą wejście (128 wymiarów) na wyjście (10 klas)\n",
    "    # Ta sieć będzie służyć jako \"wzorzec\" dla naszej aproksymacji\n",
    "    linear_approximation = nn.Linear(128, 10).to(device)\n",
    "\n",
    "    # Definiujemy funkcję straty dla sieci liniowej (MSE - Mean Squared Error)\n",
    "    linear_approximation.loss = lambda input, target, reduction=\"mean\": nn.MSELoss(\n",
    "        reduction=reduction\n",
    "    )(input, target)\n",
    "\n",
    "    # Konfigurujemy optymalizator Adam z learning rate 0.01\n",
    "    optimizer = Adam(linear_approximation.parameters(), lr=0.01)\n",
    "\n",
    "    # Trenujemy sieć liniową przez 300 epok\n",
    "    # Ta sieć nauczy się najlepszej liniowej aproksymacji problemu\n",
    "    print(\"Trenowanie sieci liniowej jako wzorca aproksymacji...\")\n",
    "    train_model(\n",
    "        linear_approximation, loaders, epochs=300, optimizer=optimizer, verbose=True\n",
    "    )\n",
    "\n",
    "    # Wyświetlamy wagi nauczonej sieci liniowej dla celów diagnostycznych\n",
    "    print(\"Wagi nauczonej sieci liniowej:\")\n",
    "    print(\"Weight shape:\", linear_approximation.weight.shape)\n",
    "    print(\"Bias shape:\", linear_approximation.bias.shape)\n",
    "\n",
    "    # KROK 2: Implementacja strategii przycinania\n",
    "    with torch.no_grad():  # Wyłączamy obliczanie gradientów dla efektywności\n",
    "\n",
    "        # KROK 2a: Zerowanie wszystkich wag w warstwach, które będziemy modyfikować\n",
    "        # Pierwsza warstwa (layers[0]): 128 -> 128 (warstwa ukryta)\n",
    "        model.layers[0].weight.data.zero_()  # Zerujemy macierz wag 128x128\n",
    "        model.layers[0].bias.data.zero_()  # Zerujemy bias 128-wymiarowy\n",
    "\n",
    "        # Trzecia warstwa (layers[2]): 128 -> 10 (warstwa wyjściowa)\n",
    "        model.layers[2].weight.data.zero_()  # Zerujemy macierz wag 10x128\n",
    "        model.layers[2].bias.data.zero_()  # Zerujemy bias 10-wymiarowy\n",
    "\n",
    "        # KROK 2b: Tworzenie \"mostka\" w pierwszej warstwie\n",
    "        # Ustawiamy małe wartości na przekątnej macierzy wag pierwszej warstwy\n",
    "        # To pozwala informacji przepływać przez sieć, ale w kontrolowany sposób\n",
    "        for i in range(128):\n",
    "            model.layers[0].weight.data[i, i] = eps\n",
    "            # Każdy neuron w warstwie ukrytej będzie otrzymywał tylko jedną cechę wejściową\n",
    "            # przeskalowaną przez małą wartość eps\n",
    "\n",
    "        # KROK 2c: Przenoszenie wiedzy z sieci liniowej do ostatniej warstwy\n",
    "        # Kompensujemy skalowanie wprowadzone przez eps i gamma\n",
    "        for j in range(10):  # Dla każdej klasy wyjściowej\n",
    "            for i in range(128):  # Dla każdego neuronu w warstwie ukrytej\n",
    "                # Przenosimy wagę z sieci liniowej, skalując ją odpowiednio\n",
    "                # Dzielimy przez (eps * gamma) aby skompensować skalowanie\n",
    "                model.layers[2].weight.data[j, i] = linear_approximation.weight[\n",
    "                    j, i\n",
    "                ] / (eps * gamma)\n",
    "\n",
    "        # KROK 2d: Ustawianie biasów w ostatniej warstwie\n",
    "        # Kopiujemy biasy z sieci liniowej z dodatkową korekcją\n",
    "        for j in range(10):\n",
    "            # Podstawowy bias z sieci liniowej minus korekta związana z sumą wag\n",
    "            # Korekta -torch.sum(...)/2 kompensuje systematyczne przesunięcie\n",
    "            model.layers[2].bias.data[j] = (\n",
    "                linear_approximation.bias.data[j]\n",
    "                - torch.sum(model.layers[2].weight.data[j]) / 2\n",
    "            )\n",
    "\n",
    "    # KROK 3: Zapisanie parametrów przyciętonego modelu\n",
    "    print(\"Zapisywanie parametrów przyciętonego modelu...\")\n",
    "    save_parameters(model, \"model_parameters.pkl\")\n",
    "\n",
    "    # Zwracamy zmodyfikowany model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wykonanie algorytmu przycinania na kopii oryginalnego modelu\n",
    "print(\"Rozpoczynanie procesu przycinania modelu...\")\n",
    "model_to_prune = copy.deepcopy(initial_model)\n",
    "your_pruning_algorithm(model_to_prune)\n",
    "print(\"Proces przycinania zakończony pomyślnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYUundRLxaYl"
   },
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBXCuWc-xaYm"
   },
   "source": [
    "Poniższy kod będzie służył ewaluacji rozwiązania. Po wysłaniu rozwiązania do nas, zostanie wykonana funkcja `evaluate_algorithm(X_valid, y_valid)`, t.j. prawie identyczny kod jak niżej będzie się uruchamiał na katalogu zdjęć `test_data` dostępnym tylko dla sprawdzających zadania.\n",
    "\n",
    "Upewnij się przed wysłaniem, że cały notebook (również z ustawioną flagą `FINAL_EVALUATION_MODE = True`) wykonuje się od początku do końca bez błędów, bez ingerencji użytkownika i zapisuje wagi w pliku `model_parameters.pkl` po wykonaniu polecenia `Run All`. Sprawdź też, czy `validation_script.py` zwraca spodziewany wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwbAeKfwxaYm"
   },
   "outputs": [],
   "source": [
    "def evaluate(X_test, y_test):\n",
    "    \"\"\"Validator\"\"\"\n",
    "    test_model = MLP().to(device)\n",
    "    load_parameters(test_model)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    _test = InMemDataset(X_test, y_test, device)\n",
    "    test_loader = DataLoader(_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    mse = compute_error(test_model, test_loader)\n",
    "    sparsity = get_sparsity(test_model)\n",
    "\n",
    "    print(f\"Model had error: {mse:.3f} and sparsity: {sparsity:.3f}\")\n",
    "    model_score = score(mse, sparsity)\n",
    "\n",
    "    return model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWUVLrS8xaYn"
   },
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    model_score = evaluate(X_valid, y_valid)\n",
    "    print(f\"Your solution gets score {model_score:.3f} on validation set.\")\n",
    "    print(f\"Your solution gets {points(model_score):.3f}/1.5 points on validation set!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EGPyTemjs4Hf",
    "W2xI0m8vSDPD",
    "sxDq_UaTSIqE"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
