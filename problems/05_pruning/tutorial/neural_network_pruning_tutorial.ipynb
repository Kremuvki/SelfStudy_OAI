{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåø Tutorial: Deep Learning for Neural Network Pruning\n",
    "\n",
    "![Neural Network Pruning](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Neural_network.svg/500px-Neural_network.svg.png)\n",
    "\n",
    "## Welcome to the Fascinating World of Neural Network Pruning! ‚úÇÔ∏è\n",
    "\n",
    "In this comprehensive tutorial, you'll learn:\n",
    "- üåø What is neural network pruning and why it matters\n",
    "- üßÆ The mathematics behind sparsity and compression\n",
    "- ü§ñ How to use advanced techniques for intelligent weight selection\n",
    "- üíª Hands-on implementation with PyTorch\n",
    "- üéØ Visualization of pruning effects on model performance\n",
    "- üß™ Interactive exercises to build your skills\n",
    "\n",
    "By the end, you'll be ready to implement sophisticated pruning algorithms that maintain model performance while dramatically reducing model size!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1. [üéì Understanding Neural Network Pruning](#1--understanding-neural-network-pruning)\n",
    "2. [üßÆ Mathematical Foundation](#2--mathematical-foundation)\n",
    "3. [üîß Setting Up the Environment](#3--setting-up-the-environment)\n",
    "4. [üèóÔ∏è Model Architecture](#4--model-architecture)\n",
    "5. [‚úÇÔ∏è Classical Pruning Approaches](#5--classical-pruning-approaches)\n",
    "6. [ü§ñ Advanced Pruning Techniques](#6--advanced-pruning-techniques)\n",
    "7. [üß† Linear Approximation Strategy](#7--linear-approximation-strategy)\n",
    "8. [üéØ Custom Scoring Function](#8--custom-scoring-function)\n",
    "9. [üíº Complete Solution Strategy](#9--complete-solution-strategy)\n",
    "10. [üé® Visualizing Pruning Effects](#10--visualizing-pruning-effects)\n",
    "11. [üéÆ Interactive Exercises](#11--interactive-exercises)\n",
    "12. [üöÄ Advanced Techniques](#12--advanced-techniques)\n",
    "13. [üìñ Summary and Next Steps](#13--summary-and-next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üéì Understanding Neural Network Pruning\n",
    "\n",
    "### What is Neural Network Pruning?\n",
    "\n",
    "Imagine you have a beautiful, fully grown tree üå≥. While every branch and leaf contributes to the tree's function, you might find that you can carefully trim certain branches without significantly affecting the tree's health or appearance. **Neural network pruning** works on the same principle!\n",
    "\n",
    "In deep learning, we often create networks that are **over-parameterized** - they have many more parameters than strictly necessary. Pruning allows us to:\n",
    "\n",
    "- **Reduce model size** üìâ (fewer parameters to store)\n",
    "- **Speed up inference** ‚ö° (fewer computations)\n",
    "- **Prevent overfitting** üéØ (simpler models generalize better)\n",
    "- **Improve interpretability** üîç (focus on important connections)\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Sparsity**: The fraction of parameters that are zero (higher = more pruned)\n",
    "- **Structured vs Unstructured**: Removing entire neurons/channels vs individual weights\n",
    "- **Magnitude-based**: Removing weights with smallest absolute values\n",
    "- **Gradient-based**: Removing weights with least impact on loss\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "Given a neural network with parameters $\\theta$, pruning creates a **sparse** version $\\theta_{sparse}$ where:\n",
    "\n",
    "$$\\theta_{sparse}[i] = \\begin{cases} \n",
    "\\theta[i] & \\text{if weight is important} \\\\\n",
    "0 & \\text{if weight is pruned}\n",
    "\\end{cases}$$\n",
    "\n",
    "The **sparsity** is defined as:\n",
    "\n",
    "$$s = \\frac{\\text{number of zero parameters}}{\\text{total number of parameters}}$$\n",
    "\n",
    "### Why Should We Care?\n",
    "\n",
    "üöÄ **Mobile Deployment**: Smaller models fit on phones and edge devices  \n",
    "üí∞ **Cost Reduction**: Less computation = lower cloud costs  \n",
    "üå± **Environmental Impact**: Fewer operations = less energy consumption  \n",
    "‚ö° **Real-time Applications**: Faster inference for time-critical tasks  \n",
    "üéØ **Better Understanding**: Sparse models reveal which connections matter most\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üßÆ Mathematical Foundation\n",
    "\n",
    "Before diving into implementation, let's understand the mathematical framework behind pruning.\n",
    "\n",
    "### The Core Trade-off\n",
    "\n",
    "Pruning is fundamentally about balancing two competing objectives:\n",
    "\n",
    "1. **Model Performance**: We want to maintain low prediction error\n",
    "2. **Model Sparsity**: We want to zero out as many weights as possible\n",
    "\n",
    "This creates a **multi-objective optimization problem**:\n",
    "\n",
    "$$\\min_{\\theta_{sparse}} \\left[ \\mathcal{L}(\\theta_{sparse}) + \\lambda \\cdot \\text{Complexity}(\\theta_{sparse}) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}$ is our loss function (e.g., MSE for regression)\n",
    "- $\\lambda$ controls the sparsity-performance trade-off\n",
    "- $\\text{Complexity}$ measures how \"complex\" our model is\n",
    "\n",
    "### Our Specific Scoring Function\n",
    "\n",
    "In our pruning challenge, we use a sophisticated scoring function that captures this trade-off:\n",
    "\n",
    "$$\\text{score}(s, \\epsilon) = \\begin{cases}\n",
    "0 & \\text{if } \\epsilon > 1000 \\\\\n",
    "(1 - \\frac{\\epsilon}{1000})^{1.5} \\cdot s^{1.5} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where:\n",
    "- $s$ = sparsity (fraction of zero weights)\n",
    "- $\\epsilon$ = MSE on test set\n",
    "\n",
    "### Understanding the Scoring Function\n",
    "\n",
    "This function is designed to:\n",
    "- **Reward high sparsity**: $s^{1.5}$ grows super-linearly with sparsity\n",
    "- **Penalize high error**: $(1 - \\frac{\\epsilon}{1000})^{1.5}$ decreases as MSE increases\n",
    "- **Set hard limits**: Score is 0 if MSE > 1000 (model completely broken)\n",
    "- **Balance both objectives**: Neither sparsity nor accuracy alone is sufficient\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "This is a **discrete, non-convex optimization problem**. We need to decide for each weight: keep it or zero it out. With thousands of parameters, this creates an enormous search space!\n",
    "\n",
    "Traditional approaches:\n",
    "- **Magnitude pruning**: Remove smallest weights üìè\n",
    "- **Gradient-based**: Remove weights with small gradients üìà\n",
    "- **Structured pruning**: Remove entire neurons/filters üèóÔ∏è\n",
    "\n",
    "**Our Innovation**: Use intelligent approximation strategies to find near-optimal sparse representations! üß†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Setting Up the Environment\n",
    "\n",
    "Let's start by importing all the necessary libraries and setting up our environment. We'll be working with PyTorch for neural networks and various other libraries for data processing and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for our neural network pruning tutorial\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set up device - GPU greatly speeds up neural network training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"üíæ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèóÔ∏è Model Architecture\n",
    "\n",
    "Let's understand the specific neural network we'll be working with. Our model is a **Multi-Layer Perceptron (MLP)** with a specific architecture that we cannot change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Multi-Layer Perceptron (MLP) architecture\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for regression task.\n",
    "\n",
    "    Architecture (FIXED - cannot be changed):\n",
    "    - Input layer: 128 features\n",
    "    - Hidden layer: 1024 neurons with Sigmoid activation\n",
    "    - Output layer: 10 targets (regression outputs)\n",
    "\n",
    "    Total parameters: 128*1024 + 1024 + 1024*10 + 10 = 142,378 parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(128, 1024),  # First layer: 128 -> 1024\n",
    "            nn.Sigmoid(),  # Activation function\n",
    "            nn.Linear(1024, 10),  # Output layer: 1024 -> 10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, input, target, reduction=\"mean\"):\n",
    "        \"\"\"Mean Squared Error loss for regression\"\"\"\n",
    "        mse_loss = nn.MSELoss(reduction=reduction)\n",
    "        return mse_loss(input, target)\n",
    "\n",
    "\n",
    "# Create our model and move to device\n",
    "model = MLP().to(device)\n",
    "print(f\"üß† Created MLP model!\")\n",
    "\n",
    "# Count and display parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "\n",
    "# Break down parameters by layer\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"   {name}: {param.shape} = {param.numel():,} parameters\")\n",
    "\n",
    "print(f\"\\\\nüéØ This model performs regression: 128 inputs ‚Üí 10 outputs\")\n",
    "print(f\"üìè Model size in memory: ~{total_params * 4 / 1024:.1f} KB (float32)\")\n",
    "\n",
    "# Test the model with dummy data\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(32, 128).to(device)  # Batch of 32 samples\n",
    "    output = model(dummy_input)\n",
    "    print(f\"\\\\n‚úÖ Model test successful!\")\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üéØ Understanding the Scoring System\n",
    "\n",
    "Before we start pruning, let's implement and understand the evaluation functions that will measure our success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(model):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity of a model (fraction of weights that are zero).\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "\n",
    "    Returns:\n",
    "        float: Sparsity value between 0 and 1 (1 = all weights are zero)\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "\n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "def compute_error(model, data_loader):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error on a dataset.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: DataLoader with test data\n",
    "\n",
    "    Returns:\n",
    "        float: Average MSE across all samples\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            outputs = model(x)\n",
    "            total_samples += x.shape[0] * y.shape[1]\n",
    "            total_loss += model.loss(outputs, y, reduction=\"sum\").item()\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "def score(mse_loss, sparsity, mse_weight=1.5, sparsity_weight=1.5):\n",
    "    \"\"\"\n",
    "    Calculate the final score that balances MSE and sparsity.\n",
    "\n",
    "    This is the exact function used for evaluation!\n",
    "\n",
    "    Args:\n",
    "        mse_loss: Mean squared error on test set\n",
    "        sparsity: Fraction of zero weights (0 to 1)\n",
    "        mse_weight: Exponent for MSE term (default 1.5)\n",
    "        sparsity_weight: Exponent for sparsity term (default 1.5)\n",
    "\n",
    "    Returns:\n",
    "        float: Score (higher is better)\n",
    "    \"\"\"\n",
    "    # Handle different input types\n",
    "    if isinstance(mse_loss, np.ndarray):\n",
    "        mse_loss = np.clip(mse_loss, 0, 1000)\n",
    "    else:\n",
    "        mse_loss = min(max(mse_loss, 0), 1000)\n",
    "\n",
    "    # If MSE is too high, score is 0\n",
    "    if mse_loss >= 1000:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate the balanced score\n",
    "    score_value = (1 - mse_loss / 1000) ** mse_weight * sparsity**sparsity_weight\n",
    "    return score_value\n",
    "\n",
    "\n",
    "def points(score_value):\n",
    "    \"\"\"\n",
    "    Convert score to final points (0 to 1.5).\n",
    "\n",
    "    Args:\n",
    "        score_value: Score from score() function\n",
    "\n",
    "    Returns:\n",
    "        float: Points between 0 and 1.5\n",
    "    \"\"\"\n",
    "\n",
    "    def scale(x, lower=0.085, upper=0.95, max_points=1.5):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower) * max_points\n",
    "\n",
    "    return scale(score_value)\n",
    "\n",
    "\n",
    "print(\"üéØ Scoring functions implemented!\")\n",
    "print(\"   - get_sparsity(): Calculates fraction of zero weights\")\n",
    "print(\"   - compute_error(): Calculates MSE on dataset\")\n",
    "print(\"   - score(): Combines MSE and sparsity into final score\")\n",
    "print(\"   - points(): Converts score to assignment points\")\n",
    "\n",
    "# Let's test these functions with our model\n",
    "model_sparsity = get_sparsity(model)\n",
    "print(f\"\\\\nüìä Current model statistics:\")\n",
    "print(\n",
    "    f\"   Sparsity: {model_sparsity:.3f} ({model_sparsity*100:.1f}% of weights are zero)\"\n",
    ")\n",
    "\n",
    "# Create some dummy data to test error calculation\n",
    "\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, size=1000):\n",
    "        self.X = torch.randn(size, 128)\n",
    "        self.y = torch.randn(size, 10)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].to(device), self.y[idx].to(device)\n",
    "\n",
    "\n",
    "dummy_loader = DataLoader(DummyDataset(500), batch_size=32, shuffle=False)\n",
    "dummy_mse = compute_error(model, dummy_loader)\n",
    "dummy_score = score(dummy_mse, model_sparsity)\n",
    "\n",
    "print(f\"   MSE on dummy data: {dummy_mse:.3f}\")\n",
    "print(f\"   Combined score: {dummy_score:.3f}\")\n",
    "print(f\"   Assignment points: {points(dummy_score):.3f}/1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ‚úÇÔ∏è Classical Pruning Approaches\n",
    "\n",
    "Before we dive into advanced techniques, let's understand and implement traditional pruning methods. This will help us appreciate why more sophisticated approaches are needed.\n",
    "\n",
    "### Magnitude-Based Pruning: The Classic Approach\n",
    "\n",
    "The most intuitive pruning method is **magnitude-based pruning**:\n",
    "1. Calculate the absolute value of each weight\n",
    "2. Sort weights by magnitude (smallest first)\n",
    "3. Zero out the smallest weights until desired sparsity is reached\n",
    "4. Keep the largest weights (they're presumably most important)\n",
    "\n",
    "Let's implement this and see how it performs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_based_pruning(model, target_sparsity=0.9):\n",
    "    \"\"\"\n",
    "    Prune model weights based on magnitude (smallest weights are removed).\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to prune\n",
    "        target_sparsity: Fraction of weights to zero out (between 0 and 1)\n",
    "\n",
    "    Returns:\n",
    "        model: Pruned model (modified in-place)\n",
    "    \"\"\"\n",
    "    # Collect all weights and their absolute values\n",
    "    all_weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            all_weights.extend(param.data.abs().flatten().cpu().numpy())\n",
    "\n",
    "    # Find the threshold: weights below this value will be pruned\n",
    "    all_weights = np.array(all_weights)\n",
    "    threshold = np.percentile(all_weights, target_sparsity * 100)\n",
    "\n",
    "    print(f\"üî™ Magnitude-based pruning:\")\n",
    "    print(f\"   Target sparsity: {target_sparsity:.1%}\")\n",
    "    print(f\"   Pruning threshold: {threshold:.6f}\")\n",
    "    print(f\"   Weights below {threshold:.6f} will be set to zero\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruned_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"weight\" in name or \"bias\" in name:\n",
    "                mask = param.data.abs() < threshold\n",
    "                pruned_count += mask.sum().item()\n",
    "                total_count += param.numel()\n",
    "                param.data[mask] = 0\n",
    "\n",
    "    actual_sparsity = pruned_count / total_count\n",
    "    print(f\"   Actual sparsity achieved: {actual_sparsity:.1%}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def random_pruning(model, target_sparsity=0.9):\n",
    "    \"\"\"\n",
    "    Randomly prune model weights (baseline comparison).\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to prune\n",
    "        target_sparsity: Fraction of weights to zero out\n",
    "\n",
    "    Returns:\n",
    "        model: Pruned model (modified in-place)\n",
    "    \"\"\"\n",
    "    print(f\"üé≤ Random pruning:\")\n",
    "    print(f\"   Target sparsity: {target_sparsity:.1%}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"weight\" in name or \"bias\" in name:\n",
    "                # Create random mask\n",
    "                mask = torch.rand_like(param.data) < target_sparsity\n",
    "                param.data[mask] = 0\n",
    "\n",
    "    actual_sparsity = get_sparsity(model)\n",
    "    print(f\"   Actual sparsity achieved: {actual_sparsity:.1%}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Let's test different pruning methods on copies of our model\n",
    "print(\"üß™ Testing classical pruning methods...\")\n",
    "\n",
    "# First, let's create a simple trained model (we'll just use random weights for demo)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Initialize model weights\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# Test magnitude-based pruning\n",
    "print(\"\\\\n\" + \"=\" * 50)\n",
    "model_magnitude = MLP().to(device)\n",
    "model_magnitude.apply(init_weights)\n",
    "\n",
    "# Record original performance\n",
    "original_sparsity = get_sparsity(model_magnitude)\n",
    "original_mse = compute_error(model_magnitude, dummy_loader)\n",
    "original_score = score(original_mse, original_sparsity)\n",
    "\n",
    "print(f\"üìä Original model:\")\n",
    "print(f\"   Sparsity: {original_sparsity:.1%}\")\n",
    "print(f\"   MSE: {original_mse:.3f}\")\n",
    "print(f\"   Score: {original_score:.3f}\")\n",
    "\n",
    "# Apply magnitude-based pruning\n",
    "model_magnitude = magnitude_based_pruning(model_magnitude, target_sparsity=0.95)\n",
    "\n",
    "# Record pruned performance\n",
    "pruned_sparsity = get_sparsity(model_magnitude)\n",
    "pruned_mse = compute_error(model_magnitude, dummy_loader)\n",
    "pruned_score = score(pruned_mse, pruned_sparsity)\n",
    "\n",
    "print(f\"\\\\nüìä After magnitude-based pruning:\")\n",
    "print(\n",
    "    f\"   Sparsity: {pruned_sparsity:.1%} (‚Üë{(pruned_sparsity-original_sparsity)*100:.1f}pp)\"\n",
    ")\n",
    "print(f\"   MSE: {pruned_mse:.3f} (‚Üë{pruned_mse-original_mse:.3f})\")\n",
    "print(\n",
    "    f\"   Score: {pruned_score:.3f} ({'‚Üë' if pruned_score > original_score else '‚Üì'}{abs(pruned_score-original_score):.3f})\"\n",
    ")\n",
    "\n",
    "# Compare with random pruning\n",
    "print(\"\\\\n\" + \"=\" * 50)\n",
    "model_random = MLP().to(device)\n",
    "model_random.apply(init_weights)\n",
    "model_random = random_pruning(model_random, target_sparsity=0.95)\n",
    "\n",
    "random_sparsity = get_sparsity(model_random)\n",
    "random_mse = compute_error(model_random, dummy_loader)\n",
    "random_score = score(random_mse, random_sparsity)\n",
    "\n",
    "print(f\"\\\\nüìä After random pruning:\")\n",
    "print(f\"   Sparsity: {random_sparsity:.1%}\")\n",
    "print(f\"   MSE: {random_mse:.3f}\")\n",
    "print(f\"   Score: {random_score:.3f}\")\n",
    "\n",
    "print(\n",
    "    f\"\\\\nüèÜ Winner: {'Magnitude-based' if pruned_score > random_score else 'Random'} pruning!\"\n",
    ")\n",
    "print(\"(Though both are quite naive approaches...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üß† Linear Approximation Strategy\n",
    "\n",
    "Now comes the exciting part! Instead of using simple heuristics like magnitude-based pruning, we'll use a sophisticated **linear approximation strategy**.\n",
    "\n",
    "### The Big Idea üí°\n",
    "\n",
    "What if we could find a way to make our complex 3-layer network (128 ‚Üí 1024 ‚Üí 10) behave like a simple linear model (128 ‚Üí 10) while maintaining most of its expressive power?\n",
    "\n",
    "The key insight is:\n",
    "1. **Train a linear approximation** of the entire network (128 ‚Üí 10 directly)\n",
    "2. **Use the hidden layer as a \"bridge\"** to transfer this linear knowledge\n",
    "3. **Create a sparse representation** that maintains the linear approximation's behavior\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "For many regression tasks, the underlying function can be well-approximated by a linear model. Our hidden layer with 1024 neurons is **over-parameterized** - we don't need all that complexity!\n",
    "\n",
    "By training a linear model first, we learn the **essential linear relationships** in the data. Then we embed this knowledge into our larger network in a sparse way.\n",
    "\n",
    "### The Mathematical Strategy\n",
    "\n",
    "Our approach creates a specific sparse pattern:\n",
    "\n",
    "1. **First layer (128 ‚Üí 1024)**: Identity-like mapping with small coefficients\n",
    "2. **Second layer (1024 ‚Üí 10)**: Scaled version of the linear model's weights\n",
    "\n",
    "This creates a \\\"pathway\\\" through the network that implements the linear approximation while keeping most weights at zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_approximation(data_loader, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a linear model to approximate the target function.\n",
    "\n",
    "    This linear model will serve as our \\\"teacher\\\" for the pruning strategy.\n",
    "\n",
    "    Args:\n",
    "        data_loader: DataLoader with training data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        trained linear model\n",
    "    \"\"\"\n",
    "    # Create simple linear model (128 -> 10, no hidden layers)\n",
    "    linear_model = nn.Linear(128, 10).to(device)\n",
    "    linear_model.loss = lambda input, target, reduction=\"mean\": nn.MSELoss(\n",
    "        reduction=reduction\n",
    "    )(input, target)\n",
    "\n",
    "    # Initialize weights\n",
    "    torch.nn.init.xavier_normal_(linear_model.weight)\n",
    "    linear_model.bias.data.fill_(0.01)\n",
    "\n",
    "    # Train the linear model\n",
    "    optimizer = optim.Adam(linear_model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"üöÄ Training linear approximation...\")\n",
    "    print(f\"   Architecture: 128 ‚Üí 10 (linear)\")\n",
    "    print(\n",
    "        f\"   Parameters: {sum(p.numel() for p in linear_model.parameters())} (vs {142378} in full model)\"\n",
    "    )\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        linear_model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for inputs, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = linear_model(inputs)\n",
    "            loss = linear_model.loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"   Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    final_loss = compute_error(linear_model, data_loader)\n",
    "    print(f\"‚úÖ Linear approximation trained! Final MSE: {final_loss:.4f}\")\n",
    "\n",
    "    return linear_model\n",
    "\n",
    "\n",
    "def advanced_pruning_strategy(model, linear_model, eps=0.001, gamma=0.25):\n",
    "    \"\"\"\n",
    "    Advanced pruning using linear approximation strategy.\n",
    "\n",
    "    This implements the core idea from the solution:\n",
    "    1. Zero out most weights\n",
    "    2. Create a \\\"bridge\\\" through the hidden layer using diagonal connections\n",
    "    3. Transfer linear model knowledge to the output layer\n",
    "\n",
    "    Args:\n",
    "        model: Original MLP model to prune\n",
    "        linear_model: Trained linear approximation\n",
    "        eps: Small value for diagonal \\\"bridge\\\" weights\n",
    "        gamma: Scaling factor for compensation\n",
    "\n",
    "    Returns:\n",
    "        Pruned model\n",
    "    \"\"\"\n",
    "    print(f\"üî¨ Applying advanced linear approximation pruning...\")\n",
    "    print(f\"   Bridge coefficient (eps): {eps}\")\n",
    "    print(f\"   Scaling factor (gamma): {gamma}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # STEP 1: Zero out all weights we'll be modifying\n",
    "        model.layers[0].weight.data.zero_()  # First layer weights (128 x 1024)\n",
    "        model.layers[0].bias.data.zero_()  # First layer biases (1024)\n",
    "        model.layers[2].weight.data.zero_()  # Output layer weights (10 x 1024)\n",
    "        model.layers[2].bias.data.zero_()  # Output layer biases (10)\n",
    "\n",
    "        # STEP 2: Create diagonal \\\"bridge\\\" in first layer\n",
    "        # This allows information to flow through specific pathways\n",
    "        for i in range(128):  # Only use first 128 neurons of hidden layer\n",
    "            model.layers[0].weight.data[i, i] = eps\n",
    "\n",
    "        print(f\"   Created diagonal bridge: {128} connections with weight {eps}\")\n",
    "\n",
    "        # STEP 3: Transfer linear model knowledge to output layer\n",
    "        # We need to scale appropriately to compensate for eps and gamma\n",
    "        for j in range(10):  # For each output\n",
    "            for i in range(128):  # For each input feature\n",
    "                # Scale the linear model weight by the bridge scaling\n",
    "                model.layers[2].weight.data[j, i] = linear_model.weight[j, i] / (\n",
    "                    eps * gamma\n",
    "                )\n",
    "\n",
    "        print(f\"   Transferred linear weights with scaling factor: {1/(eps*gamma):.1f}\")\n",
    "\n",
    "        # STEP 4: Set output biases with correction\n",
    "        for j in range(10):\n",
    "            # Copy bias from linear model with correction for systematic shift\n",
    "            bias_correction = torch.sum(model.layers[2].weight.data[j]) / 2\n",
    "            model.layers[2].bias.data[j] = linear_model.bias.data[j] - bias_correction\n",
    "\n",
    "        print(f\"   Set output biases with correction\")\n",
    "\n",
    "    # Calculate final sparsity\n",
    "    final_sparsity = get_sparsity(model)\n",
    "    print(f\"‚úÖ Advanced pruning complete!\")\n",
    "    print(f\"   Final sparsity: {final_sparsity:.1%}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Let's test this approach with our dummy data\n",
    "print(\"üß™ Testing Linear Approximation Strategy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train linear approximation\n",
    "linear_model = train_linear_approximation(dummy_loader, epochs=50, lr=0.01)\n",
    "\n",
    "# Test linear model performance\n",
    "linear_mse = compute_error(linear_model, dummy_loader)\n",
    "linear_score = score(linear_mse, 0.0)  # Linear model has 0% sparsity\n",
    "print(f\"\\\\nüìä Linear model performance:\")\n",
    "print(f\"   MSE: {linear_mse:.4f}\")\n",
    "print(f\"   Score (no sparsity): {linear_score:.4f}\")\n",
    "\n",
    "# Create and prune a new MLP model\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "advanced_model = MLP().to(device)\n",
    "advanced_model.apply(init_weights)\n",
    "\n",
    "# Apply our advanced pruning strategy\n",
    "advanced_model = advanced_pruning_strategy(advanced_model, linear_model)\n",
    "\n",
    "# Evaluate the pruned model\n",
    "advanced_mse = compute_error(advanced_model, dummy_loader)\n",
    "advanced_sparsity = get_sparsity(advanced_model)\n",
    "advanced_score = score(advanced_mse, advanced_sparsity)\n",
    "\n",
    "print(f\"\\\\nüìä Advanced pruned model performance:\")\n",
    "print(f\"   MSE: {advanced_mse:.4f} (vs {linear_mse:.4f} linear)\")\n",
    "print(f\"   Sparsity: {advanced_sparsity:.1%}\")\n",
    "print(f\"   Score: {advanced_score:.4f}\")\n",
    "print(f\"   Points: {points(advanced_score):.3f}/1.5\")\n",
    "\n",
    "# Compare with magnitude-based pruning\n",
    "print(f\"\\\\nüèÜ Comparison:\")\n",
    "print(f\"   Advanced strategy score: {advanced_score:.4f}\")\n",
    "print(f\"   Magnitude pruning score: {pruned_score:.4f}\")\n",
    "print(f\"   Improvement: {advanced_score - pruned_score:.4f} points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üé® Visualizing Pruning Effects\n",
    "\n",
    "Understanding what pruning does to our model is crucial. Let's create visualizations to see the structure and sparsity patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weight_distribution(model, title=\"Model Weight Distribution\"):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of weights in the model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Collect all weights\n",
    "    all_weights = []\n",
    "    layer_weights = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            weights = param.data.cpu().numpy().flatten()\n",
    "            all_weights.extend(weights)\n",
    "            layer_weights[name] = weights\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # Plot 1: Overall weight distribution\n",
    "    axes[0, 0].hist(all_weights, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[0, 0].set_title(\"All Weights Distribution\")\n",
    "    axes[0, 0].set_xlabel(\"Weight Value\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "    axes[0, 0].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Zero\")\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Plot 2: Layer-wise weight distributions\n",
    "    for i, (name, weights) in enumerate(layer_weights.items()):\n",
    "        if i < 3:  # Only plot first 3 layers\n",
    "            axes[0, 1].hist(weights, bins=30, alpha=0.5, label=name, density=True)\n",
    "    axes[0, 1].set_title(\"Layer-wise Weight Distributions\")\n",
    "    axes[0, 1].set_xlabel(\"Weight Value\")\n",
    "    axes[0, 1].set_ylabel(\"Density\")\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Plot 3: Sparsity per layer\n",
    "    layer_names = []\n",
    "    sparsity_values = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            total = param.numel()\n",
    "            zeros = torch.sum(param == 0).item()\n",
    "            sparsity = zeros / total\n",
    "            layer_names.append(name.replace(\"layers.\", \"\"))\n",
    "            sparsity_values.append(sparsity)\n",
    "\n",
    "    axes[1, 0].bar(range(len(layer_names)), sparsity_values, alpha=0.7)\n",
    "    axes[1, 0].set_title(\"Sparsity per Layer\")\n",
    "    axes[1, 0].set_xlabel(\"Layer\")\n",
    "    axes[1, 0].set_ylabel(\"Sparsity\")\n",
    "    axes[1, 0].set_xticks(range(len(layer_names)))\n",
    "    axes[1, 0].set_xticklabels(layer_names, rotation=45)\n",
    "\n",
    "    # Plot 4: Weight matrix heatmap (first layer only)\n",
    "    first_layer_weights = model.layers[0].weight.data.cpu().numpy()\n",
    "    # Show only a subset for visibility\n",
    "    subset = first_layer_weights[:64, :64]  # Top-left 64x64 block\n",
    "    im = axes[1, 1].imshow(subset, cmap=\"RdBu\", aspect=\"auto\")\n",
    "    axes[1, 1].set_title(\"First Layer Weight Matrix (64x64 subset)\")\n",
    "    axes[1, 1].set_xlabel(\"Input Features\")\n",
    "    axes[1, 1].set_ylabel(\"Hidden Neurons\")\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_pruning_patterns(models_dict):\n",
    "    \"\"\"\n",
    "    Compare pruning patterns across different methods.\n",
    "\n",
    "    Args:\n",
    "        models_dict: Dictionary of {name: model} pairs\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, len(models_dict), figsize=(5 * len(models_dict), 8))\n",
    "    if len(models_dict) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    for col, (name, model) in enumerate(models_dict.items()):\n",
    "        # Plot 1: Weight magnitude vs position\n",
    "        all_weights = []\n",
    "        positions = []\n",
    "\n",
    "        pos = 0\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if \"weight\" in param_name:\n",
    "                weights = param.data.cpu().numpy().flatten()\n",
    "                all_weights.extend(np.abs(weights))\n",
    "                positions.extend(range(pos, pos + len(weights)))\n",
    "                pos += len(weights)\n",
    "\n",
    "        axes[0, col].scatter(positions, all_weights, alpha=0.5, s=1)\n",
    "        axes[0, col].set_title(f\"{name}\\\\nWeight Magnitudes\")\n",
    "        axes[0, col].set_xlabel(\"Parameter Index\")\n",
    "        axes[0, col].set_ylabel(\"Absolute Weight Value\")\n",
    "        axes[0, col].set_yscale(\"log\")\n",
    "\n",
    "        # Plot 2: Sparsity breakdown\n",
    "        layer_sparsity = []\n",
    "        layer_names = []\n",
    "\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if \"weight\" in param_name or \"bias\" in param_name:\n",
    "                total = param.numel()\n",
    "                zeros = torch.sum(param == 0).item()\n",
    "                sparsity = zeros / total\n",
    "                layer_sparsity.append(sparsity)\n",
    "                layer_names.append(param_name.split(\".\")[-1])\n",
    "\n",
    "        bars = axes[1, col].bar(range(len(layer_sparsity)), layer_sparsity, alpha=0.7)\n",
    "        axes[1, col].set_title(f\"{name}\\\\nLayer Sparsity\")\n",
    "        axes[1, col].set_xlabel(\"Layer\")\n",
    "        axes[1, col].set_ylabel(\"Sparsity\")\n",
    "        axes[1, col].set_xticks(range(len(layer_names)))\n",
    "        axes[1, col].set_xticklabels(layer_names, rotation=45)\n",
    "\n",
    "        # Add sparsity values on bars\n",
    "        for bar, sparsity in zip(bars, layer_sparsity):\n",
    "            height = bar.get_height()\n",
    "            axes[1, col].text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.01,\n",
    "                f\"{sparsity:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Let's visualize our different pruning approaches\n",
    "print(\"üé® Visualizing Pruning Effects\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create models with different pruning strategies for comparison\n",
    "models_to_compare = {}\n",
    "\n",
    "# Original model (no pruning)\n",
    "original_model = MLP().to(device)\n",
    "original_model.apply(init_weights)\n",
    "models_to_compare[\"Original\"] = original_model\n",
    "\n",
    "# Magnitude-based pruned model\n",
    "magnitude_model = MLP().to(device)\n",
    "magnitude_model.apply(init_weights)\n",
    "magnitude_based_pruning(magnitude_model, target_sparsity=0.90)\n",
    "models_to_compare[\"Magnitude\"] = magnitude_model\n",
    "\n",
    "# Advanced pruned model\n",
    "advanced_model_viz = MLP().to(device)\n",
    "advanced_model_viz.apply(init_weights)\n",
    "advanced_pruning_strategy(advanced_model_viz, linear_model)\n",
    "models_to_compare[\"Advanced\"] = advanced_model_viz\n",
    "\n",
    "# Show weight distributions for the advanced model\n",
    "print(\"\\\\nüìä Weight Distribution Analysis:\")\n",
    "visualize_weight_distribution(advanced_model_viz, \"Advanced Pruning Strategy\")\n",
    "\n",
    "# Compare all pruning patterns\n",
    "print(\"\\\\nüîç Pruning Pattern Comparison:\")\n",
    "analyze_pruning_patterns(models_to_compare)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\\\nüìà Summary Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "for name, model in models_to_compare.items():\n",
    "    sparsity = get_sparsity(model)\n",
    "    mse = compute_error(model, dummy_loader)\n",
    "    score_val = score(mse, sparsity)\n",
    "\n",
    "    print(f\"{name:>10}: Sparsity={sparsity:.1%}, MSE={mse:.4f}, Score={score_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéÆ Interactive Exercises\n",
    "\n",
    "Now it's your turn to experiment and learn! Try these challenges to deepen your understanding of neural network pruning.\n",
    "\n",
    "### üéØ Exercise 1: Hyperparameter Tuning\n",
    "\n",
    "The linear approximation strategy has several hyperparameters. Let's explore how they affect performance:\n",
    "\n",
    "1. **eps** (bridge coefficient): Try values like 0.1, 0.01, 0.001, 0.0001\n",
    "2. **gamma** (scaling factor): Try values like 0.1, 0.25, 0.5, 1.0\n",
    "3. **Training epochs**: Try different numbers of epochs for the linear model\n",
    "\n",
    "Use the cell below to experiment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Experiment Playground - Try different hyperparameters!\n",
    "\n",
    "# Define hyperparameter combinations to test\n",
    "hyperparams_to_try = [\n",
    "    {\"eps\": 0.01, \"gamma\": 0.1, \"epochs\": 30, \"name\": \"High eps, Low gamma\"},\n",
    "    {\"eps\": 0.001, \"gamma\": 0.25, \"epochs\": 50, \"name\": \"Medium eps, Medium gamma\"},\n",
    "    {\"eps\": 0.0001, \"gamma\": 0.5, \"epochs\": 70, \"name\": \"Low eps, High gamma\"},\n",
    "    {\"eps\": 0.001, \"gamma\": 0.25, \"epochs\": 100, \"name\": \"Medium eps, More training\"},\n",
    "]\n",
    "\n",
    "print(\"üî¨ Testing different hyperparameter combinations...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, params in enumerate(hyperparams_to_try):\n",
    "    print(f\"\\\\nüìä Testing combination {i+1}/4: {params['name']}\")\n",
    "\n",
    "    # Train linear model with specified epochs\n",
    "    linear_model_exp = train_linear_approximation(\n",
    "        dummy_loader, epochs=params[\"epochs\"], lr=0.01\n",
    "    )\n",
    "\n",
    "    # Create and prune model with specified hyperparameters\n",
    "    test_model = MLP().to(device)\n",
    "    test_model.apply(init_weights)\n",
    "\n",
    "    # Apply pruning with custom hyperparameters\n",
    "    advanced_pruning_strategy(\n",
    "        test_model, linear_model_exp, eps=params[\"eps\"], gamma=params[\"gamma\"]\n",
    "    )\n",
    "\n",
    "    # Evaluate performance\n",
    "    test_mse = compute_error(test_model, dummy_loader)\n",
    "    test_sparsity = get_sparsity(test_model)\n",
    "    test_score = score(test_mse, test_sparsity)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"name\": params[\"name\"],\n",
    "            \"eps\": params[\"eps\"],\n",
    "            \"gamma\": params[\"gamma\"],\n",
    "            \"epochs\": params[\"epochs\"],\n",
    "            \"mse\": test_mse,\n",
    "            \"sparsity\": test_sparsity,\n",
    "            \"score\": test_score,\n",
    "            \"points\": points(test_score),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"   Results: MSE={test_mse:.4f}, Sparsity={test_sparsity:.1%}, Score={test_score:.4f}\"\n",
    "    )\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"üß™ Hyperparameter Experiment Results\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "names = [r[\"name\"] for r in results]\n",
    "scores = [r[\"score\"] for r in results]\n",
    "mses = [r[\"mse\"] for r in results]\n",
    "sparsities = [r[\"sparsity\"] for r in results]\n",
    "eps_values = [r[\"eps\"] for r in results]\n",
    "\n",
    "# Plot 1: Scores comparison\n",
    "bars1 = axes[0, 0].bar(range(len(names)), scores, alpha=0.7, color=\"skyblue\")\n",
    "axes[0, 0].set_title(\"Final Scores\")\n",
    "axes[0, 0].set_xlabel(\"Configuration\")\n",
    "axes[0, 0].set_ylabel(\"Score\")\n",
    "axes[0, 0].set_xticks(range(len(names)))\n",
    "axes[0, 0].set_xticklabels([f\"Config {i+1}\" for i in range(len(names))], rotation=45)\n",
    "\n",
    "# Add score values on bars\n",
    "for bar, score in zip(bars1, scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 0.001,\n",
    "        f\"{score:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# Plot 2: MSE vs Sparsity trade-off\n",
    "scatter = axes[0, 1].scatter(\n",
    "    sparsities, mses, c=scores, cmap=\"viridis\", s=100, alpha=0.7\n",
    ")\n",
    "axes[0, 1].set_title(\"MSE vs Sparsity Trade-off\")\n",
    "axes[0, 1].set_xlabel(\"Sparsity\")\n",
    "axes[0, 1].set_ylabel(\"MSE\")\n",
    "plt.colorbar(scatter, ax=axes[0, 1], label=\"Score\")\n",
    "\n",
    "# Plot 3: Effect of eps parameter\n",
    "axes[1, 0].scatter(eps_values, scores, s=100, alpha=0.7, color=\"orange\")\n",
    "axes[1, 0].set_title(\"Effect of eps Parameter\")\n",
    "axes[1, 0].set_xlabel(\"eps (bridge coefficient)\")\n",
    "axes[1, 0].set_ylabel(\"Score\")\n",
    "axes[1, 0].set_xscale(\"log\")\n",
    "\n",
    "# Plot 4: Points earned\n",
    "bars2 = axes[1, 1].bar(\n",
    "    range(len(names)), [points(s) for s in scores], alpha=0.7, color=\"lightgreen\"\n",
    ")\n",
    "axes[1, 1].set_title(\"Assignment Points Earned\")\n",
    "axes[1, 1].set_xlabel(\"Configuration\")\n",
    "axes[1, 1].set_ylabel(\"Points (out of 1.5)\")\n",
    "axes[1, 1].set_xticks(range(len(names)))\n",
    "axes[1, 1].set_xticklabels([f\"Config {i+1}\" for i in range(len(names))], rotation=45)\n",
    "axes[1, 1].axhline(y=1.5, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Maximum\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\\\nüìà Detailed Results:\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Configuration {i+1}: {result['name']}\")\n",
    "    print(f\"   eps={result['eps']}, gamma={result['gamma']}, epochs={result['epochs']}\")\n",
    "    print(f\"   MSE={result['mse']:.4f}, Sparsity={result['sparsity']:.1%}\")\n",
    "    print(f\"   Score={result['score']:.4f}, Points={result['points']:.3f}/1.5\")\n",
    "    print()\n",
    "\n",
    "best_config = max(results, key=lambda x: x[\"score\"])\n",
    "print(f\"üèÜ Best configuration: {best_config['name']}\")\n",
    "print(f\"   Final score: {best_config['score']:.4f}\")\n",
    "print(f\"   Assignment points: {best_config['points']:.3f}/1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üöÄ Advanced Topics and Extensions\n",
    "\n",
    "While our linear approximation strategy is powerful, there are many other advanced pruning techniques worth exploring:\n",
    "\n",
    "### üéì Knowledge Distillation\n",
    "- Train a large \"teacher\" model, then distill knowledge into a smaller \"student\"\n",
    "- The student learns to mimic the teacher's outputs, enabling better compression\n",
    "\n",
    "### üé∞ Lottery Ticket Hypothesis\n",
    "- Some sparse subnetworks can achieve similar performance to the full network\n",
    "- Finding these \"winning tickets\" can lead to highly efficient models\n",
    "\n",
    "### üèóÔ∏è Structured Pruning\n",
    "- Instead of individual weights, remove entire neurons, channels, or layers\n",
    "- Provides actual speedup (not just memory savings) without specialized hardware\n",
    "\n",
    "### üîÑ Iterative Pruning\n",
    "- Gradually increase sparsity over multiple rounds\n",
    "- Train ‚Üí Prune ‚Üí Train ‚Üí Prune... achieves better results than one-shot pruning\n",
    "\n",
    "### üéØ Gradient-Based Pruning\n",
    "- Use gradient information to determine weight importance\n",
    "- Methods like SNIP, GraSP, and Synflow analyze gradients for better pruning decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üìñ Summary and Next Steps\n",
    "\n",
    "Congratulations! üéâ You've learned how to use advanced techniques for neural network pruning!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **üåø Neural Network Pruning Fundamentals**:\n",
    "   - Understanding sparsity and its benefits\n",
    "   - The trade-off between model size and performance\n",
    "   - Different types of pruning approaches\n",
    "\n",
    "2. **üßÆ Mathematical Framework**:\n",
    "   - Complex scoring functions balancing multiple objectives\n",
    "   - The challenge of discrete optimization in neural networks\n",
    "   - Why simple heuristics often fail\n",
    "\n",
    "3. **ü§ñ Linear Approximation Strategy**:\n",
    "   - Training simplified models as \"teachers\"\n",
    "   - Creating sparse pathways through hidden layers\n",
    "   - Intelligent weight transfer and scaling\n",
    "\n",
    "4. **üíª Implementation Skills**:\n",
    "   - PyTorch model manipulation and weight setting\n",
    "   - Custom evaluation functions and metrics\n",
    "   - Visualization and analysis of pruning effects\n",
    "\n",
    "5. **üî¨ Experimental Methodology**:\n",
    "   - Hyperparameter tuning for pruning algorithms\n",
    "   - Comparing different pruning strategies\n",
    "   - Understanding the performance trade-offs\n",
    "\n",
    "### For the Solution Implementation:\n",
    "\n",
    "You now have all the knowledge to implement the complete solution! The key components are:\n",
    "\n",
    "```python\n",
    "def your_pruning_algorithm(model):\n",
    "    # 1. Train a linear approximation of the target function\n",
    "    linear_model = train_linear_approximation(data_loader, epochs=300)\n",
    "    \n",
    "    # 2. Zero out weights in layers to be modified\n",
    "    model.layers[0].weight.data.zero_()  # First layer\n",
    "    model.layers[2].weight.data.zero_()  # Output layer\n",
    "    # (Keep biases and middle activation as-is)\n",
    "    \n",
    "    # 3. Create diagonal \"bridge\" in first layer\n",
    "    eps = 0.001\n",
    "    for i in range(128):\n",
    "        model.layers[0].weight.data[i, i] = eps\n",
    "    \n",
    "    # 4. Transfer linear knowledge to output layer with scaling\n",
    "    gamma = 0.25\n",
    "    for j in range(10):\n",
    "        for i in range(128):\n",
    "            model.layers[2].weight.data[j, i] = linear_model.weight[j, i] / (eps * gamma)\n",
    "    \n",
    "    # 5. Set output biases with correction\n",
    "    for j in range(10):\n",
    "        bias_correction = torch.sum(model.layers[2].weight.data[j]) / 2\n",
    "        model.layers[2].bias.data[j] = linear_model.bias.data[j] - bias_correction\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "### üöÄ Advanced Topics to Explore:\n",
    "\n",
    "- **Knowledge Distillation**: Using teacher-student training paradigms\n",
    "- **Lottery Ticket Hypothesis**: Finding sparse subnetworks that train well\n",
    "- **Structured Pruning**: Removing entire neurons/channels for real speedup\n",
    "- **Neural Architecture Search**: Automatically finding efficient architectures\n",
    "- **Quantization**: Reducing precision of weights and activations\n",
    "\n",
    "### üìö Useful Resources:\n",
    "\n",
    "- üìñ [The Lottery Ticket Hypothesis Paper](https://arxiv.org/abs/1803.03635)\n",
    "- üõ†Ô∏è [PyTorch Pruning Tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n",
    "- üìë [Magnitude-based Pruning Paper](https://arxiv.org/abs/1506.02626)\n",
    "- üéØ [SNIP: Single-shot Network Pruning](https://arxiv.org/abs/1810.02340)\n",
    "- üß† [What's Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/abs/1911.13299)\n",
    "\n",
    "**Good luck with your implementation!** üåü\n",
    "\n",
    "Remember: The key insight is that many neural networks are over-parameterized, and clever approximation strategies can maintain performance while dramatically reducing model complexity. Your linear approximation approach leverages the power of well-trained simple models to guide the sparsification of complex networks!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
