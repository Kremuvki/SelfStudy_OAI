{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≥ Tutorial: Deep Learning for Dependency Parsing\n",
    "\n",
    "![Dependency Tree](https://upload.wikimedia.org/wikipedia/commons/8/8e/Thistreeisillustratingdependencyrelations.jpg)\n",
    "\n",
    "## Welcome to the Fascinating World of Dependency Parsing! üîó\n",
    "\n",
    "In this comprehensive tutorial, you'll learn:\n",
    "- üå≥ What is dependency parsing and why it matters in NLP\n",
    "- üß† How transformer models like BERT can capture syntactic relationships\n",
    "- ü§ñ Building neural networks for syntactic analysis\n",
    "- üíª Hands-on implementation with PyTorch and HerBERT\n",
    "- üìä Evaluation metrics and tree construction algorithms\n",
    "- üß™ Interactive exercises to build your skills\n",
    "\n",
    "By the end, you'll be ready to implement sophisticated dependency parsing using neural networks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1. [üéì Understanding Dependency Parsing](#1--understanding-dependency-parsing)\n",
    "2. [üßÆ Mathematical Foundation](#2--mathematical-foundation)\n",
    "3. [üîß Setting Up the Environment](#3--setting-up-the-environment)\n",
    "4. [üèóÔ∏è Classical vs Neural Approaches](#4--classical-vs-neural-approaches)\n",
    "5. [ü§ñ BERT and Contextual Embeddings](#5--bert-and-contextual-embeddings)\n",
    "6. [üß† Building the Neural Models](#6--building-the-neural-models)\n",
    "7. [üéØ Training Strategy](#7--training-strategy)\n",
    "8. [üå≥ Tree Construction Algorithm](#8--tree-construction-algorithm)\n",
    "9. [üìä Evaluation Metrics](#9--evaluation-metrics)\n",
    "10. [üéÆ Interactive Exercises](#10--interactive-exercises)\n",
    "11. [üöÄ Advanced Techniques](#11--advanced-techniques)\n",
    "12. [üìñ Summary and Next Steps](#12--summary-and-next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üéì Understanding Dependency Parsing\n",
    "\n",
    "### What is Dependency Parsing?\n",
    "\n",
    "Imagine you're reading a sentence and trying to understand **who does what to whom** ü§î. Dependency parsing is exactly that - it's the process of analyzing the grammatical structure of sentences to understand the relationships between words.\n",
    "\n",
    "**Example**: \"Maria bought a beautiful red car yesterday.\"\n",
    "\n",
    "In this sentence:\n",
    "- **\"bought\"** is the main verb (root)\n",
    "- **\"Maria\"** is the subject of \"bought\"\n",
    "- **\"car\"** is the object of \"bought\"\n",
    "- **\"beautiful\"** and **\"red\"** modify \"car\"\n",
    "- **\"yesterday\"** modifies \"bought\"\n",
    "\n",
    "### Dependency Tree Structure\n",
    "\n",
    "```\n",
    "         bought\n",
    "    ______|______\n",
    "   |             |\n",
    " Maria          car\n",
    "             ____|____\n",
    "            |    |    |\n",
    "           a  beautiful red\n",
    "                      |\n",
    "                  yesterday\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Head**: The governing word in a relationship (e.g., \"bought\" governs \"Maria\")\n",
    "- **Dependent**: The word that depends on another (e.g., \"Maria\" depends on \"bought\")\n",
    "- **Root**: The main word of the sentence (usually the main verb)\n",
    "- **Tree Structure**: Each word has exactly one head (except root), forming a tree\n",
    "\n",
    "### Why is This Important?\n",
    "\n",
    "ü§ñ **Machine Translation**: Understanding structure helps translate correctly  \n",
    "üîç **Question Answering**: Finding relationships between entities  \n",
    "üìù **Text Summarization**: Identifying key information structures  \n",
    "üß† **Sentiment Analysis**: Understanding what modifies what  \n",
    "üìä **Information Extraction**: Finding structured data in text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üßÆ Mathematical Foundation\n",
    "\n",
    "### The Dependency Parsing Problem\n",
    "\n",
    "Given a sentence $S = w_1, w_2, ..., w_n$ with $n$ words, we want to find:\n",
    "\n",
    "1. **Root word** $r \\in \\{1, 2, ..., n\\}$\n",
    "2. **Dependency edges** $E = \\{(h_i, d_i)\\}$ where $h_i$ is the head and $d_i$ is the dependent\n",
    "\n",
    "### Tree Constraints\n",
    "\n",
    "A valid dependency tree must satisfy:\n",
    "\n",
    "1. **Exactly one root**: $|\\{w_i : head(w_i) = \\text{NULL}\\}| = 1$\n",
    "2. **Connected**: Every word is reachable from root\n",
    "3. **Acyclic**: No cycles in the dependency graph\n",
    "4. **Single head**: Each non-root word has exactly one head\n",
    "\n",
    "### Distance Matrix\n",
    "\n",
    "For tree construction, we define the **distance matrix** $D$ where:\n",
    "\n",
    "$$D_{i,j} = \\text{shortest path length between words } w_i \\text{ and } w_j$$\n",
    "\n",
    "Key properties:\n",
    "- $D_{i,i} = 0$ (distance to self)\n",
    "- $D_{i,j} = D_{j,i}$ (symmetric)\n",
    "- $D_{i,j} = 1$ if there's a direct edge between $w_i$ and $w_j$\n",
    "\n",
    "### Depth from Root\n",
    "\n",
    "The **depth** of word $w_i$ is:\n",
    "\n",
    "$$depth(w_i) = D_{root, i}$$\n",
    "\n",
    "- Root has depth 0\n",
    "- Direct children of root have depth 1\n",
    "- And so on...\n",
    "\n",
    "### The Neural Approach\n",
    "\n",
    "Instead of using traditional parsing algorithms, we'll use **neural networks** to:\n",
    "\n",
    "1. **Predict edge probabilities**: $P(edge_{i,j}) = f_{edge}(emb_i, emb_j)$\n",
    "2. **Predict root probabilities**: $P(root_i) = f_{root}(emb_i)$\n",
    "3. **Construct tree**: Use greedy algorithm with predicted probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Setting Up the Environment\n",
    "\n",
    "Let's start by importing all the necessary libraries and setting up our environment for dependency parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for dependency parsing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Transformers for BERT/HerBERT\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up device - CPU is fine for this tutorial\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentence structures for demonstration\n",
    "class Sentence:\n",
    "    \"\"\"Represents a sentence with words\"\"\"\n",
    "\n",
    "    def __init__(self, words: List[str]):\n",
    "        self.words = words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.words)\n",
    "\n",
    "\n",
    "class ParsedSentence(Sentence):\n",
    "    \"\"\"Represents a sentence with dependency structure\"\"\"\n",
    "\n",
    "    def __init__(self, words: List[str], edges: List[Tuple[int, int]], root: int):\n",
    "        super().__init__(words)\n",
    "        self.edges = edges  # List of (head, dependent) pairs\n",
    "        self.root = root  # Index of root word\n",
    "\n",
    "    def get_sorted_edges(self):\n",
    "        \"\"\"Return edges sorted by head index\"\"\"\n",
    "        return sorted(self.edges)\n",
    "\n",
    "    def pretty_print(self):\n",
    "        \"\"\"Print a simple visualization of the dependency tree\"\"\"\n",
    "        print(f\"Root: {self.words[self.root]} (index {self.root})\")\n",
    "        print(\"Edges:\")\n",
    "        for head, dep in self.get_sorted_edges():\n",
    "            print(f\"  {self.words[head]} -> {self.words[dep]}\")\n",
    "\n",
    "\n",
    "# Create sample sentences for demonstration\n",
    "sample_sentences = [\n",
    "    ParsedSentence(\n",
    "        words=[\"Maria\", \"bought\", \"a\", \"car\", \".\"],\n",
    "        edges=[(1, 0), (1, 3), (3, 2), (1, 4)],  # bought -> Maria, car, .\n",
    "        root=1,  # \"bought\" is root\n",
    "    ),\n",
    "    ParsedSentence(\n",
    "        words=[\"The\", \"cat\", \"sleeps\", \"peacefully\", \".\"],\n",
    "        edges=[(2, 1), (1, 0), (2, 3), (2, 4)],  # sleeps -> cat, peacefully, .\n",
    "        root=2,  # \"sleeps\" is root\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"üìö Sample sentences created:\")\n",
    "for i, sent in enumerate(sample_sentences):\n",
    "    print(f\"\\n{i+1}. {sent}\")\n",
    "    sent.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèóÔ∏è Classical vs Neural Approaches\n",
    "\n",
    "Before diving into neural methods, let's understand how dependency parsing was traditionally done.\n",
    "\n",
    "### Classical Approaches\n",
    "\n",
    "#### 1. Transition-Based Parsing\n",
    "- **Idea**: Process words left-to-right, making parsing decisions\n",
    "- **Actions**: SHIFT (move to next word), REDUCE (create dependency)\n",
    "- **Problem**: Greedy decisions can lead to errors\n",
    "\n",
    "#### 2. Graph-Based Parsing\n",
    "- **Idea**: Score all possible edges, find best tree\n",
    "- **Algorithm**: Maximum Spanning Tree (MST)\n",
    "- **Problem**: Scoring function is hand-crafted\n",
    "\n",
    "#### 3. Rule-Based Parsing\n",
    "- **Idea**: Use grammatical rules to determine dependencies\n",
    "- **Example**: \"Adjectives modify nouns\"\n",
    "- **Problem**: Rules don't capture all linguistic phenomena\n",
    "\n",
    "### Why Neural Approaches? ü§ñ\n",
    "\n",
    "‚ùå **Traditional Problems**:\n",
    "- Hand-crafted features\n",
    "- Limited context understanding\n",
    "- Difficulty handling ambiguity\n",
    "- Poor generalization\n",
    "\n",
    "‚úÖ **Neural Advantages**:\n",
    "- **Automatic feature learning**: No manual feature engineering\n",
    "- **Rich representations**: Contextual embeddings from BERT\n",
    "- **Flexible architectures**: Can model complex relationships\n",
    "- **End-to-end training**: Optimize for final task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a simple distance calculation function\n",
    "def calculate_tree_distances(parsed_sentence: ParsedSentence) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the distance matrix for a dependency tree.\n",
    "\n",
    "    This function computes the shortest path distance between every pair of words\n",
    "    in the dependency tree. This is a fundamental operation in dependency parsing.\n",
    "\n",
    "    Args:\n",
    "        parsed_sentence: A sentence with known dependency structure\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (n, n) where n is the number of words\n",
    "        distances[i][j] = shortest path from word i to word j\n",
    "    \"\"\"\n",
    "    n = len(parsed_sentence)\n",
    "    distances = np.zeros((n, n))\n",
    "\n",
    "    # Build adjacency list from edges\n",
    "    neighbors = [[] for _ in range(n)]\n",
    "    for head, dep in parsed_sentence.get_sorted_edges():\n",
    "        neighbors[head].append(dep)\n",
    "        neighbors[dep].append(head)  # Undirected graph for distance calculation\n",
    "\n",
    "    # DFS to compute distances from each node\n",
    "    def dfs(start, current, visited, dist):\n",
    "        visited[current] = True\n",
    "        for neighbor in neighbors[current]:\n",
    "            if not visited[neighbor]:\n",
    "                distances[start][neighbor] = dist + 1\n",
    "                dfs(start, neighbor, visited, dist + 1)\n",
    "\n",
    "    # Compute distances from each starting node\n",
    "    for i in range(n):\n",
    "        visited = [False] * n\n",
    "        distances[i][i] = 0  # Distance to self is 0\n",
    "        dfs(i, i, visited, 0)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "# Test our distance calculation\n",
    "print(\"üßÆ Testing distance calculation:\")\n",
    "for i, sent in enumerate(sample_sentences):\n",
    "    distances = calculate_tree_distances(sent)\n",
    "    print(f\"\\nSentence {i+1}: {sent}\")\n",
    "    print(\"Distance matrix:\")\n",
    "    print(distances.astype(int))\n",
    "\n",
    "    # Show depths (distances from root)\n",
    "    root_distances = distances[sent.root]\n",
    "    print(f\"Depths from root '{sent.words[sent.root]}': {root_distances.astype(int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ü§ñ BERT and Contextual Embeddings\n",
    "\n",
    "The key to our neural approach is using **HerBERT** (Polish BERT) to get rich, contextual representations of words.\n",
    "\n",
    "### Why HerBERT?\n",
    "\n",
    "üß† **Contextual Understanding**: Unlike traditional word embeddings, BERT considers the entire sentence context  \n",
    "üáµüá± **Polish Language**: HerBERT is specifically trained on Polish text  \n",
    "üéØ **Syntactic Knowledge**: BERT's hidden layers capture syntactic relationships  \n",
    "üîÑ **Subword Tokenization**: Handles unknown words gracefully\n",
    "\n",
    "### The BERT Architecture\n",
    "\n",
    "```\n",
    "Input: \"Maria kupuje samoch√≥d\"\n",
    "   ‚Üì Tokenization\n",
    "[CLS] Maria kupuje samoch√≥d [SEP]\n",
    "   ‚Üì Embedding Layer\n",
    "768-dimensional vectors\n",
    "   ‚Üì 12 Transformer Layers\n",
    "Contextual representations\n",
    "   ‚Üì Layer 7 (our choice)\n",
    "Rich syntactic features\n",
    "```\n",
    "\n",
    "### Why Layer 7?\n",
    "\n",
    "Research shows that **middle layers** of BERT capture the best syntactic information:\n",
    "- **Early layers** (1-3): Surface features, POS tags\n",
    "- **Middle layers** (4-8): Syntactic relationships, dependencies  \n",
    "- **Late layers** (9-12): Semantic features, task-specific info\n",
    "\n",
    "Layer 7 is our sweet spot for dependency parsing! üéØ\n",
    "\n",
    "### üîß Handling Subword Tokenization\n",
    "\n",
    "**The Challenge**: BERT tokenizes words into subwords (WordPiece)\n",
    "- \"programowanie\" ‚Üí [\"program\", \"##owanie\"]\n",
    "- \"najd≈Çu≈ºszy\" ‚Üí [\"naj\", \"##d≈Çu≈º\", \"##szy\"]\n",
    "\n",
    "**The Solution**: We need to **aggregate** subword embeddings back to word-level\n",
    "\n",
    "**Aggregation Strategies**:\n",
    "1. **Average**: Take mean of all subword embeddings ‚úÖ\n",
    "2. **First**: Use only the first subword embedding\n",
    "3. **Last**: Use only the last subword embedding\n",
    "4. **Max**: Take element-wise maximum\n",
    "\n",
    "We'll use **averaging** as it preserves information from all subwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HerBERT model and tokenizer\n",
    "print(\"üì¶ Loading HerBERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "bert_model.eval()  # Set to evaluation mode\n",
    "print(\"‚úÖ HerBERT loaded successfully!\")\n",
    "\n",
    "\n",
    "def get_bert_embeddings(sentences: List[str], layer_idx: int = 7):\n",
    "    \"\"\"\n",
    "    Extract embeddings from HerBERT for a list of sentences.\n",
    "\n",
    "    This function demonstrates the complete BERT processing pipeline:\n",
    "    1. Tokenization: Split sentences into subwords\n",
    "    2. Encoding: Convert to input IDs with special tokens\n",
    "    3. Forward pass: Get hidden states from all layers\n",
    "    4. Extraction: Get embeddings from specified layer\n",
    "\n",
    "    Args:\n",
    "        sentences: List of sentences as strings\n",
    "        layer_idx: Which BERT layer to extract (7 is good for syntax)\n",
    "\n",
    "    Returns:\n",
    "        List of tensors, each containing embeddings for one sentence\n",
    "    \"\"\"\n",
    "    # Tokenize and encode all sentences\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "    # Get embeddings from BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**encoded, output_hidden_states=True)\n",
    "\n",
    "    # Extract embeddings from specified layer\n",
    "    layer_embeddings = outputs.hidden_states[\n",
    "        layer_idx + 1\n",
    "    ]  # +1 because layer 0 is embeddings\n",
    "\n",
    "    # Process each sentence separately\n",
    "    sentence_embeddings = []\n",
    "    for i in range(len(sentences)):\n",
    "        # Get attention mask for this sentence\n",
    "        attention_mask = encoded[\"attention_mask\"][i].bool()\n",
    "\n",
    "        # Extract real tokens (excluding padding)\n",
    "        real_embeddings = layer_embeddings[i, attention_mask]\n",
    "\n",
    "        # Remove [CLS] and [SEP] tokens\n",
    "        clean_embeddings = real_embeddings[1:-1]  # Remove first and last tokens\n",
    "\n",
    "        sentence_embeddings.append(clean_embeddings)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# Test BERT embeddings\n",
    "test_sentences = [\"Maria kupuje samoch√≥d .\", \"Kot ≈õpi spokojnie .\"]\n",
    "print(f\"üß™ Testing BERT embeddings on: {test_sentences}\")\n",
    "\n",
    "embeddings = get_bert_embeddings(test_sentences)\n",
    "for i, (sent, emb) in enumerate(zip(test_sentences, embeddings)):\n",
    "    words = sent.split()\n",
    "    print(f\"\\nSentence {i+1}: {sent}\")\n",
    "    print(f\"  Words: {len(words)}\")\n",
    "    print(f\"  Embeddings shape: {emb.shape}\")\n",
    "    print(f\"  Embedding dimension: {emb.shape[-1]}\")\n",
    "\n",
    "    # Note: We might need to handle subword tokenization\n",
    "    # For now, let's see what we get\n",
    "    if emb.shape[0] != len(words):\n",
    "        print(f\"  ‚ö†Ô∏è  Mismatch: {emb.shape[0]} embeddings vs {len(words)} words\")\n",
    "        print(f\"  This is normal - BERT uses subword tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(sentence: str, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize a sentence and create alignment between words and subwords.\n",
    "\n",
    "    This function helps us understand how BERT's tokenization relates to\n",
    "    the original words in our sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence: Original sentence string\n",
    "        tokenizer: HerBERT tokenizer\n",
    "\n",
    "    Returns:\n",
    "        dict with original words, subword tokens, and alignment mapping\n",
    "    \"\"\"\n",
    "    # Split into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Tokenize each word separately to understand the mapping\n",
    "    word_to_subwords = []\n",
    "    all_subwords = []\n",
    "\n",
    "    for word in words:\n",
    "        # Tokenize individual word (add space prefix for proper tokenization)\n",
    "        subwords = tokenizer.tokenize(\" \" + word)[0:]  # Keep space token for Polish\n",
    "        if not subwords:  # Handle edge case\n",
    "            subwords = tokenizer.tokenize(word)\n",
    "\n",
    "        word_to_subwords.append(subwords)\n",
    "        all_subwords.extend(subwords)\n",
    "\n",
    "    # Create alignment: subword_idx -> word_idx\n",
    "    subword_to_word = []\n",
    "    for word_idx, subwords in enumerate(word_to_subwords):\n",
    "        subword_to_word.extend([word_idx] * len(subwords))\n",
    "\n",
    "    return {\n",
    "        \"words\": words,\n",
    "        \"subwords\": all_subwords,\n",
    "        \"word_to_subwords\": word_to_subwords,\n",
    "        \"subword_to_word\": subword_to_word,\n",
    "    }\n",
    "\n",
    "\n",
    "def aggregate_subword_embeddings(\n",
    "    embeddings: torch.Tensor,\n",
    "    subword_to_word: List[int],\n",
    "    num_words: int,\n",
    "    method: str = \"mean\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Aggregate subword embeddings to word-level embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Tensor of subword embeddings [num_subwords, embed_dim]\n",
    "        subword_to_word: List mapping subword indices to word indices\n",
    "        num_words: Number of original words\n",
    "        method: Aggregation method ('mean', 'first', 'last', 'max')\n",
    "\n",
    "    Returns:\n",
    "        Tensor of word embeddings [num_words, embed_dim]\n",
    "    \"\"\"\n",
    "    embed_dim = embeddings.shape[-1]\n",
    "    word_embeddings = torch.zeros(num_words, embed_dim)\n",
    "\n",
    "    for word_idx in range(num_words):\n",
    "        # Find all subwords belonging to this word\n",
    "        subword_indices = [i for i, w in enumerate(subword_to_word) if w == word_idx]\n",
    "\n",
    "        if subword_indices:\n",
    "            subword_embs = embeddings[subword_indices]\n",
    "\n",
    "            if method == \"mean\":\n",
    "                word_embeddings[word_idx] = torch.mean(subword_embs, dim=0)\n",
    "            elif method == \"first\":\n",
    "                word_embeddings[word_idx] = subword_embs[0]\n",
    "            elif method == \"last\":\n",
    "                word_embeddings[word_idx] = subword_embs[-1]\n",
    "            elif method == \"max\":\n",
    "                word_embeddings[word_idx] = torch.max(subword_embs, dim=0)[0]\n",
    "\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "# Test tokenization and alignment\n",
    "test_sentence = \"Programowanie jest fascynujƒÖce .\"\n",
    "print(f\"üîç Analyzing tokenization for: '{test_sentence}'\")\n",
    "\n",
    "alignment = tokenize_and_align(test_sentence, tokenizer)\n",
    "print(f\"\\nüìù Original words: {alignment['words']}\")\n",
    "print(f\"üî§ Subwords: {alignment['subwords']}\")\n",
    "print(f\"üìä Word-to-subwords mapping:\")\n",
    "for i, (word, subwords) in enumerate(\n",
    "    zip(alignment[\"words\"], alignment[\"word_to_subwords\"])\n",
    "):\n",
    "    print(f\"  {i}: '{word}' ‚Üí {subwords}\")\n",
    "\n",
    "print(f\"\\nüîó Subword-to-word alignment: {alignment['subword_to_word']}\")\n",
    "\n",
    "# Test aggregation\n",
    "embeddings = get_bert_embeddings([test_sentence])[0]\n",
    "print(f\"\\nüìä Subword embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "word_embeddings = aggregate_subword_embeddings(\n",
    "    embeddings, alignment[\"subword_to_word\"], len(alignment[\"words\"])\n",
    ")\n",
    "print(f\"üìä Word embeddings shape: {word_embeddings.shape}\")\n",
    "print(\n",
    "    f\"‚úÖ Successfully aggregated {embeddings.shape[0]} subwords to {word_embeddings.shape[0]} words!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üß† Building the Neural Models\n",
    "\n",
    "Now comes the exciting part! We'll build two neural networks to solve dependency parsing:\n",
    "\n",
    "1. **Distance Model**: Predicts if two words should be connected by an edge\n",
    "2. **Depth Model**: Predicts which word should be the root of the tree\n",
    "\n",
    "### Our Neural Architecture Strategy\n",
    "\n",
    "#### Distance Model üîó\n",
    "- **Input**: Concatenated embeddings of two words (768 √ó 2 = 1536 dimensions)\n",
    "- **Task**: Binary classification - edge or no edge?\n",
    "- **Output**: Probability that words i and j are connected\n",
    "\n",
    "#### Depth Model üå≥  \n",
    "- **Input**: Single word embedding (768 dimensions)\n",
    "- **Task**: Binary classification - root or not root?\n",
    "- **Output**: Probability that word i is the root\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "‚úÖ **Simplicity**: Two focused models are easier to train than one complex model  \n",
    "‚úÖ **Interpretability**: We can analyze what each model learns  \n",
    "‚úÖ **Flexibility**: Can train with different strategies and hyperparameters  \n",
    "‚úÖ **Efficiency**: Smaller models train faster\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "Both models follow a similar structure:\n",
    "- **Input Layer**: Takes BERT embeddings\n",
    "- **Hidden Layer**: 256 neurons with LeakyReLU activation\n",
    "- **Dropout**: 30% for regularization\n",
    "- **Output**: Single neuron with sigmoid activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that predicts whether two words should be connected by an edge.\n",
    "\n",
    "    Architecture:\n",
    "    - Input: Concatenated embeddings of two words (768*2 = 1536 dims)\n",
    "    - Hidden: 256 neurons with LeakyReLU and dropout\n",
    "    - Output: Single probability (sigmoid)\n",
    "\n",
    "    This model learns to identify syntactic relationships between word pairs!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=768 * 2, hidden_dim=256, dropout_rate=0.3):\n",
    "        super(DistanceModel, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # Input layer: concatenated word embeddings -> hidden layer\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # LeakyReLU: allows small gradients for negative values\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            # Regularization: randomly drop 30% of neurons during training\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            # Output layer: hidden -> probability\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            # Sigmoid: maps any real number to [0,1] probability\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, 1536] - concatenated word pairs\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, 1] - edge probabilities\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class DepthModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that predicts whether a word should be the root of the tree.\n",
    "\n",
    "    Architecture:\n",
    "    - Input: Single word embedding (768 dims)\n",
    "    - Hidden: 256 neurons with LeakyReLU and dropout\n",
    "    - Output: Single probability (sigmoid)\n",
    "\n",
    "    This model learns to identify which words are most likely to be sentence roots!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, dropout_rate=0.3):\n",
    "        super(DepthModel, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            # Input layer: single word embedding -> hidden layer\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # LeakyReLU activation\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            # Regularization dropout\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            # Output layer: hidden -> probability\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            # Sigmoid activation\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, 768] - word embeddings\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, 1] - root probabilities\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Let's create and test our models\n",
    "print(\"üèóÔ∏è Creating neural network models...\")\n",
    "\n",
    "# Initialize models\n",
    "distance_model = DistanceModel()\n",
    "depth_model = DepthModel()\n",
    "\n",
    "print(f\"‚úÖ Distance Model created!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in distance_model.parameters()):,}\")\n",
    "print(f\"   Input: {768*2} dims (two word embeddings)\")\n",
    "print(f\"   Output: 1 dim (edge probability)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Depth Model created!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in depth_model.parameters()):,}\")\n",
    "print(f\"   Input: {768} dims (single word embedding)\")\n",
    "print(f\"   Output: 1 dim (root probability)\")\n",
    "\n",
    "# Test with dummy data\n",
    "print(f\"\\nüß™ Testing models with dummy data...\")\n",
    "\n",
    "# Test distance model\n",
    "dummy_pair = torch.randn(4, 768 * 2)  # Batch of 4 word pairs\n",
    "distance_predictions = distance_model(dummy_pair)\n",
    "print(f\"Distance model output shape: {distance_predictions.shape}\")\n",
    "print(\n",
    "    f\"Distance predictions (should be [0,1]): {distance_predictions.squeeze().detach().numpy()}\"\n",
    ")\n",
    "\n",
    "# Test depth model\n",
    "dummy_words = torch.randn(5, 768)  # Batch of 5 words\n",
    "depth_predictions = depth_model(dummy_words)\n",
    "print(f\"Depth model output shape: {depth_predictions.shape}\")\n",
    "print(\n",
    "    f\"Depth predictions (should be [0,1]): {depth_predictions.squeeze().detach().numpy()}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Both models working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Training Strategy\n",
    "\n",
    "Now that we have our models, let's understand how to train them effectively for dependency parsing.\n",
    "\n",
    "### The Data Challenge üìä\n",
    "\n",
    "**Imbalanced Data Problem**:\n",
    "- Most word pairs are **NOT** connected (negative examples)\n",
    "- Only a few word pairs **ARE** connected (positive examples)\n",
    "- Ratio can be 1:20 or worse!\n",
    "\n",
    "**Root Detection Problem**:\n",
    "- Only **ONE** word per sentence is the root\n",
    "- All other words are **NOT** roots\n",
    "- Even more imbalanced than edges!\n",
    "\n",
    "### Our Training Solutions üí°\n",
    "\n",
    "#### 1. Balanced Sampling for Distance Model\n",
    "```python\n",
    "# Instead of using all negative examples:\n",
    "positive_pairs = pairs_with_edges      # Few examples\n",
    "negative_pairs = pairs_without_edges   # Many examples\n",
    "\n",
    "# Sample equal numbers:\n",
    "balanced_negatives = random.sample(negative_pairs, len(positive_pairs))\n",
    "training_data = positive_pairs + balanced_negatives\n",
    "```\n",
    "\n",
    "#### 2. Balanced Sampling for Depth Model\n",
    "```python\n",
    "# Instead of using all non-root words:\n",
    "root_words = [word for word in sentence if word.is_root]     # 1 example\n",
    "non_root_words = [word for word in sentence if not word.is_root]  # Many examples\n",
    "\n",
    "# Sample equal numbers:\n",
    "balanced_non_roots = random.sample(non_root_words, len(root_words))\n",
    "training_data = root_words + balanced_non_roots\n",
    "```\n",
    "\n",
    "### Why This Works üéØ\n",
    "\n",
    "‚úÖ **Prevents bias**: Model doesn't just predict \"no edge\" all the time  \n",
    "‚úÖ **Faster training**: Smaller, balanced datasets train quicker  \n",
    "‚úÖ **Better learning**: Model sees equal examples of both classes  \n",
    "‚úÖ **Improved metrics**: Better precision and recall on positive class\n",
    "\n",
    "### Training Loop Overview\n",
    "\n",
    "For each epoch:\n",
    "1. **Shuffle** and **balance** the data\n",
    "2. **Forward pass** through model\n",
    "3. **Compute loss** (Binary Cross Entropy)\n",
    "4. **Backward pass** (gradient computation)\n",
    "5. **Update weights** (optimizer step)\n",
    "6. **Evaluate** on validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üå≥ Tree Construction Algorithm\n",
    "\n",
    "Once our models are trained, we need to convert their predictions into a valid dependency tree. This is where the magic happens!\n",
    "\n",
    "### The Challenge ü§î\n",
    "\n",
    "Our models give us:\n",
    "- **Edge probabilities**: P(edge between word i and word j)\n",
    "- **Root probabilities**: P(word i is root)\n",
    "\n",
    "But we need:\n",
    "- **Valid tree**: Exactly n-1 edges, connected, acyclic\n",
    "- **Single root**: Exactly one root word\n",
    "- **Connected**: Every word reachable from root\n",
    "\n",
    "### Our Greedy Tree Construction Algorithm üõ†Ô∏è\n",
    "\n",
    "```python\n",
    "def construct_dependency_tree(edge_probabilities, root_probabilities):\n",
    "    # Step 1: Choose root (highest root probability)\n",
    "    root = argmax(root_probabilities)\n",
    "    \n",
    "    # Step 2: Initialize tree with root\n",
    "    nodes_in_tree = {root}\n",
    "    edges = []\n",
    "    \n",
    "    # Step 3: Greedily add edges\n",
    "    for _ in range(n - 1):  # Need exactly n-1 edges\n",
    "        best_edge = None\n",
    "        best_prob = -infinity\n",
    "        \n",
    "        # Find best edge from tree to outside\n",
    "        for node_in_tree in nodes_in_tree:\n",
    "            for candidate in nodes_not_in_tree:\n",
    "                prob = edge_probabilities[node_in_tree][candidate]\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_edge = (node_in_tree, candidate)\n",
    "        \n",
    "        # Add best edge to tree\n",
    "        edges.append(best_edge)\n",
    "        nodes_in_tree.add(best_edge[1])\n",
    "    \n",
    "    return edges, root\n",
    "```\n",
    "\n",
    "### Why This Algorithm Works ‚úÖ\n",
    "\n",
    "1. **Guarantees valid tree**: Always produces exactly n-1 edges\n",
    "2. **Maintains connectivity**: Always extends from existing tree\n",
    "3. **Prevents cycles**: Never connects within the tree\n",
    "4. **Uses model predictions**: Chooses highest probability edges and root\n",
    "5. **Efficient**: O(n¬≤) time complexity\n",
    "\n",
    "### Visual Example üëÅÔ∏è\n",
    "\n",
    "```\n",
    "Sentence: \"The cat sleeps peacefully\"\n",
    "Root probs: [0.1, 0.2, 0.9, 0.1]  ‚Üí Choose \"sleeps\" as root\n",
    "\n",
    "Step 1: Tree = {sleeps}\n",
    "Best edge: sleeps ‚Üí cat (prob=0.8)\n",
    "Tree = {sleeps, cat}\n",
    "\n",
    "Step 2: Tree = {sleeps, cat}  \n",
    "Best edge: cat ‚Üí The (prob=0.7)\n",
    "Tree = {sleeps, cat, The}\n",
    "\n",
    "Step 3: Tree = {sleeps, cat, The}\n",
    "Best edge: sleeps ‚Üí peacefully (prob=0.6)\n",
    "Tree = {sleeps, cat, The, peacefully}\n",
    "\n",
    "Final tree: sleeps ‚Üí cat ‚Üí The, sleeps ‚Üí peacefully\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_demo(sentence_words, distance_model, depth_model):\n",
    "    \"\"\"\n",
    "    Demonstration of the complete parsing pipeline.\n",
    "\n",
    "    This function shows how all the pieces fit together:\n",
    "    1. Get word embeddings from BERT\n",
    "    2. Use models to predict edge and root probabilities\n",
    "    3. Construct tree with greedy algorithm\n",
    "    \"\"\"\n",
    "    print(f\"üîç Parsing sentence: {' '.join(sentence_words)}\")\n",
    "\n",
    "    # Step 1: Get word embeddings (simplified - assumes perfect word alignment)\n",
    "    sentence_str = \" \".join(sentence_words)\n",
    "    embeddings = get_bert_embeddings([sentence_str])[0]\n",
    "\n",
    "    # For demo, let's assume we have word-level embeddings\n",
    "    # In practice, you'd need the subword aggregation from earlier\n",
    "    n_words = len(sentence_words)\n",
    "\n",
    "    # Create dummy embeddings if mismatch (for demo purposes)\n",
    "    if embeddings.shape[0] != n_words:\n",
    "        print(\n",
    "            f\"   üìù Note: Using dummy embeddings for demo ({embeddings.shape[0]} subwords ‚Üí {n_words} words)\"\n",
    "        )\n",
    "        embeddings = torch.randn(n_words, 768)\n",
    "\n",
    "    # Step 2: Get predictions from models\n",
    "    print(\"   ü§ñ Getting model predictions...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Root probabilities for each word\n",
    "        root_probs = depth_model(embeddings).squeeze()\n",
    "\n",
    "        # Edge probabilities for all word pairs\n",
    "        edge_probs = torch.zeros(n_words, n_words)\n",
    "        for i in range(n_words):\n",
    "            for j in range(n_words):\n",
    "                if i != j:  # Don't predict self-edges\n",
    "                    # Concatenate embeddings of word pair\n",
    "                    pair_embedding = torch.cat([embeddings[i], embeddings[j]])\n",
    "                    edge_probs[i][j] = distance_model(\n",
    "                        pair_embedding.unsqueeze(0)\n",
    "                    ).item()\n",
    "\n",
    "    # Step 3: Construct tree greedily\n",
    "    print(\"   üå≥ Constructing dependency tree...\")\n",
    "\n",
    "    # Choose root (word with highest root probability)\n",
    "    root_idx = torch.argmax(root_probs).item()\n",
    "    print(\n",
    "        f\"   üëë Root selected: '{sentence_words[root_idx]}' (prob: {root_probs[root_idx]:.3f})\"\n",
    "    )\n",
    "\n",
    "    # Greedy tree construction\n",
    "    nodes_in_tree = {root_idx}\n",
    "    edges = []\n",
    "\n",
    "    for step in range(n_words - 1):\n",
    "        best_edge = None\n",
    "        best_prob = -1\n",
    "\n",
    "        # Find best edge from tree to outside\n",
    "        for node_in_tree in nodes_in_tree:\n",
    "            for candidate in range(n_words):\n",
    "                if candidate not in nodes_in_tree:\n",
    "                    prob = edge_probs[node_in_tree][candidate].item()\n",
    "                    if prob > best_prob:\n",
    "                        best_prob = prob\n",
    "                        best_edge = (node_in_tree, candidate)\n",
    "\n",
    "        if best_edge:\n",
    "            edges.append(best_edge)\n",
    "            nodes_in_tree.add(best_edge[1])\n",
    "            head, dep = best_edge\n",
    "            print(\n",
    "                f\"   üîó Step {step+1}: {sentence_words[head]} ‚Üí {sentence_words[dep]} (prob: {best_prob:.3f})\"\n",
    "            )\n",
    "\n",
    "    # Create result\n",
    "    result = ParsedSentence(sentence_words, edges, root_idx)\n",
    "\n",
    "    print(f\"\\n   ‚úÖ Final tree:\")\n",
    "    result.pretty_print()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test our parsing pipeline\n",
    "test_words = [\"The\", \"cat\", \"sleeps\", \"peacefully\"]\n",
    "print(\"üöÄ Testing complete parsing pipeline:\")\n",
    "result = parse_sentence_demo(test_words, distance_model, depth_model)\n",
    "\n",
    "print(f\"\\nüìä Tree statistics:\")\n",
    "print(f\"   - Root: {result.words[result.root]}\")\n",
    "print(f\"   - Edges: {len(result.edges)}\")\n",
    "print(f\"   - Expected edges: {len(result.words) - 1}\")\n",
    "print(f\"   - Tree valid: {len(result.edges) == len(result.words) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üìä Evaluation Metrics\n",
    "\n",
    "Understanding how to measure parsing quality is crucial for developing good models.\n",
    "\n",
    "### Primary Metrics üéØ\n",
    "\n",
    "#### 1. UUAS (Unlabeled Undirected Attachment Score)\n",
    "- **What it measures**: Percentage of correctly identified edges\n",
    "- **Formula**: `UUAS = (correct_edges / total_edges) √ó 100`\n",
    "- **Range**: 0% to 100% (higher is better)\n",
    "\n",
    "```python\n",
    "def calculate_uuas(true_edges, predicted_edges):\n",
    "    true_set = set(true_edges)\n",
    "    pred_set = set(predicted_edges)\n",
    "    \n",
    "    # Convert directed edges to undirected for comparison\n",
    "    true_undirected = set()\n",
    "    for head, dep in true_set:\n",
    "        true_undirected.add((min(head, dep), max(head, dep)))\n",
    "    \n",
    "    pred_undirected = set()\n",
    "    for head, dep in pred_set:\n",
    "        pred_undirected.add((min(head, dep), max(head, dep)))\n",
    "    \n",
    "    correct = len(true_undirected & pred_undirected)\n",
    "    total = len(true_undirected)\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "```\n",
    "\n",
    "#### 2. Root Placement Accuracy\n",
    "- **What it measures**: Percentage of correctly identified sentence roots\n",
    "- **Formula**: `Root_Acc = (correct_roots / total_sentences) √ó 100`\n",
    "- **Range**: 0% to 100% (higher is better)\n",
    "\n",
    "```python\n",
    "def calculate_root_accuracy(true_roots, predicted_roots):\n",
    "    correct = sum(1 for true, pred in zip(true_roots, predicted_roots) if true == pred)\n",
    "    total = len(true_roots)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "```\n",
    "\n",
    "### Final Score Calculation üèÜ\n",
    "\n",
    "The problem uses a weighted combination:\n",
    "\n",
    "```python\n",
    "def calculate_final_score(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "    \n",
    "    return scale(root_placement) + scale(uuas)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Score range: 0.0 to 2.0\n",
    "- Each metric contributes 0-1 points\n",
    "- Performance below 50% gets 0 points\n",
    "- Performance above 85% gets full points\n",
    "- Linear scaling between 50% and 85%\n",
    "\n",
    "### Why These Metrics? ü§î\n",
    "\n",
    "‚úÖ **UUAS**: Measures structural understanding  \n",
    "‚úÖ **Root Placement**: Critical for tree validity  \n",
    "‚úÖ **Scaling**: Encourages meaningful performance  \n",
    "‚úÖ **Balance**: Both metrics matter equally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the evaluation metrics\n",
    "def calculate_uuas(\n",
    "    true_sentence: ParsedSentence, pred_sentence: ParsedSentence\n",
    ") -> float:\n",
    "    \"\"\"Calculate UUAS score for a single sentence\"\"\"\n",
    "    true_edges = set(true_sentence.get_sorted_edges())\n",
    "    pred_edges = set(pred_sentence.get_sorted_edges())\n",
    "\n",
    "    # Convert to undirected edges\n",
    "    true_undirected = set()\n",
    "    for head, dep in true_edges:\n",
    "        true_undirected.add((min(head, dep), max(head, dep)))\n",
    "\n",
    "    pred_undirected = set()\n",
    "    for head, dep in pred_edges:\n",
    "        pred_undirected.add((min(head, dep), max(head, dep)))\n",
    "\n",
    "    if len(true_undirected) == 0:\n",
    "        return 1.0 if len(pred_undirected) == 0 else 0.0\n",
    "\n",
    "    correct = len(true_undirected & pred_undirected)\n",
    "    return correct / len(true_undirected)\n",
    "\n",
    "\n",
    "def calculate_final_score(root_placement: float, uuas: float) -> float:\n",
    "    \"\"\"Calculate final score as defined in the problem\"\"\"\n",
    "\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "\n",
    "    return scale(root_placement) + scale(uuas)\n",
    "\n",
    "\n",
    "# Test evaluation on our sample sentences\n",
    "print(\"üìä Testing evaluation metrics:\")\n",
    "\n",
    "# Create a \"predicted\" version of our first sample sentence\n",
    "true_sentence = sample_sentences[0]  # \"Maria bought a car .\"\n",
    "# Let's create a slightly wrong prediction for testing\n",
    "pred_sentence = ParsedSentence(\n",
    "    words=[\"Maria\", \"bought\", \"a\", \"car\", \".\"],\n",
    "    edges=[\n",
    "        (1, 0),\n",
    "        (1, 3),\n",
    "        (1, 2),\n",
    "        (1, 4),\n",
    "    ],  # Different from true: (1,2) instead of (3,2)\n",
    "    root=1,  # Same root\n",
    ")\n",
    "\n",
    "print(f\"\\nTrue sentence: {true_sentence}\")\n",
    "true_sentence.pretty_print()\n",
    "\n",
    "print(f\"\\nPredicted sentence: {pred_sentence}\")\n",
    "pred_sentence.pretty_print()\n",
    "\n",
    "# Calculate metrics\n",
    "uuas_score = calculate_uuas(true_sentence, pred_sentence)\n",
    "root_correct = true_sentence.root == pred_sentence.root\n",
    "final_score = calculate_final_score(1.0 if root_correct else 0.0, uuas_score)\n",
    "\n",
    "print(f\"\\nüìà Evaluation Results:\")\n",
    "print(f\"   UUAS Score: {uuas_score:.3f} ({uuas_score*100:.1f}%)\")\n",
    "print(f\"   Root Correct: {root_correct} ({100 if root_correct else 0}%)\")\n",
    "print(f\"   Final Score: {final_score:.3f}/2.0\")\n",
    "\n",
    "# Show which edges were correct/incorrect\n",
    "true_edges = set(true_sentence.get_sorted_edges())\n",
    "pred_edges = set(pred_sentence.get_sorted_edges())\n",
    "\n",
    "print(f\"\\nüîç Edge Analysis:\")\n",
    "print(f\"   True edges: {true_edges}\")\n",
    "print(f\"   Pred edges: {pred_edges}\")\n",
    "print(f\"   Correct: {true_edges & pred_edges}\")\n",
    "print(f\"   Missing: {true_edges - pred_edges}\")\n",
    "print(f\"   Extra: {pred_edges - true_edges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéÆ Interactive Exercises\n",
    "\n",
    "Now it's your turn to experiment and learn! Try these challenges to deepen your understanding.\n",
    "\n",
    "### üéØ Exercise 1: Model Architecture Exploration\n",
    "\n",
    "Try modifying the neural network architectures and observe the effects:\n",
    "\n",
    "1. **Change layer sizes**: Try 128, 512, or 1024 hidden neurons\n",
    "2. **Add more layers**: Create deeper networks  \n",
    "3. **Try different activations**: ReLU, ELU, or Swish\n",
    "4. **Experiment with dropout**: 0.1, 0.5, or 0.7\n",
    "\n",
    "**Questions to consider**:\n",
    "- How does model size affect training speed?\n",
    "- Do deeper networks perform better?\n",
    "- What happens with very high dropout rates?\n",
    "\n",
    "### üéØ Exercise 2: Aggregation Strategy Comparison\n",
    "\n",
    "Compare different methods for aggregating subword embeddings:\n",
    "\n",
    "1. **Mean**: Average all subword embeddings ‚úÖ (current)\n",
    "2. **First**: Use only the first subword\n",
    "3. **Last**: Use only the last subword  \n",
    "4. **Max**: Element-wise maximum\n",
    "5. **Attention**: Learned weighted combination\n",
    "\n",
    "**Implementation hint**:\n",
    "```python\n",
    "# Try implementing attention-based aggregation\n",
    "class AttentionAggregation(nn.Module):\n",
    "    def __init__(self, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, subword_embeddings):\n",
    "        # subword_embeddings: [num_subwords, embed_dim]\n",
    "        weights = torch.softmax(self.attention(subword_embeddings), dim=0)\n",
    "        return torch.sum(weights * subword_embeddings, dim=0)\n",
    "```\n",
    "\n",
    "### üéØ Exercise 3: Tree Construction Algorithms\n",
    "\n",
    "Experiment with different tree construction strategies:\n",
    "\n",
    "1. **Greedy** (current): Always add highest probability edge\n",
    "2. **Beam search**: Keep top-k partial trees\n",
    "3. **Maximum Spanning Tree**: Use edge weights\n",
    "4. **Random sampling**: Sample edges based on probabilities\n",
    "\n",
    "### üéØ Exercise 4: Training Strategies\n",
    "\n",
    "Try different approaches to handle class imbalance:\n",
    "\n",
    "1. **Different sampling ratios**: 1:1, 1:2, 1:5 positive:negative\n",
    "2. **Weighted loss**: Give positive examples higher weight\n",
    "3. **Focal loss**: Focus learning on hard examples\n",
    "4. **Curriculum learning**: Start with easy examples\n",
    "\n",
    "### üéØ Exercise 5: Error Analysis\n",
    "\n",
    "Analyze what your models get wrong:\n",
    "\n",
    "1. **Edge type analysis**: Which syntactic relationships are hardest?\n",
    "2. **Sentence length**: Performance vs sentence length\n",
    "3. **Root word patterns**: What makes good/bad root predictions?\n",
    "4. **Common errors**: Most frequent mistake patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Experiment Playground - Try different model architectures!\n",
    "\n",
    "\n",
    "def create_modified_distance_model(\n",
    "    hidden_dim=256, num_layers=1, activation=\"leaky_relu\", dropout_rate=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a distance model with customizable architecture.\n",
    "\n",
    "    Args:\n",
    "        hidden_dim: Size of hidden layers\n",
    "        num_layers: Number of hidden layers (1-3)\n",
    "        activation: 'relu', 'leaky_relu', 'elu', or 'swish'\n",
    "        dropout_rate: Dropout probability\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "\n",
    "    # Input layer\n",
    "    layers.append(nn.Linear(768 * 2, hidden_dim))\n",
    "\n",
    "    # Activation function\n",
    "    if activation == \"relu\":\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == \"leaky_relu\":\n",
    "        layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "    elif activation == \"elu\":\n",
    "        layers.append(nn.ELU())\n",
    "    elif activation == \"swish\":\n",
    "        layers.append(nn.SiLU())  # SiLU is the same as Swish\n",
    "\n",
    "    layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        if activation == \"relu\":\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation == \"leaky_relu\":\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "        elif activation == \"elu\":\n",
    "            layers.append(nn.ELU())\n",
    "        elif activation == \"swish\":\n",
    "            layers.append(nn.SiLU())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(hidden_dim, 1))\n",
    "    layers.append(nn.Sigmoid())\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Test different architectures\n",
    "architectures_to_test = [\n",
    "    {\"name\": \"Small\", \"hidden_dim\": 128, \"num_layers\": 1},\n",
    "    {\"name\": \"Standard\", \"hidden_dim\": 256, \"num_layers\": 1},  # Current\n",
    "    {\"name\": \"Large\", \"hidden_dim\": 512, \"num_layers\": 1},\n",
    "    {\"name\": \"Deep\", \"hidden_dim\": 256, \"num_layers\": 3},\n",
    "]\n",
    "\n",
    "print(\"üî¨ Testing different model architectures:\")\n",
    "for arch in architectures_to_test:\n",
    "    model = create_modified_distance_model(\n",
    "        **{k: v for k, v in arch.items() if k != \"name\"}\n",
    "    )\n",
    "\n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Test forward pass\n",
    "    dummy_input = torch.randn(10, 768 * 2)\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "\n",
    "    print(\n",
    "        f\"   {arch['name']:>8}: {num_params:>6,} params, output shape: {output.shape}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nüí° Experiment ideas:\")\n",
    "print(f\"   - Which architecture would train fastest?\")\n",
    "print(f\"   - Which might generalize best?\")\n",
    "print(f\"   - How does parameter count relate to performance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üöÄ Advanced Techniques\n",
    "\n",
    "Ready to take your dependency parsing to the next level? Here are some advanced techniques used in state-of-the-art systems.\n",
    "\n",
    "### 1. Multi-Layer BERT Features üé≠\n",
    "\n",
    "Instead of using just layer 7, combine features from multiple layers:\n",
    "\n",
    "```python\n",
    "def get_multi_layer_embeddings(sentences, layers=[4, 7, 10]):\n",
    "    \"\"\"Extract and combine features from multiple BERT layers\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for layer in layers:\n",
    "        layer_embs = get_bert_embeddings(sentences, layer_idx=layer)\n",
    "        all_embeddings.append(layer_embs)\n",
    "    \n",
    "    # Concatenate or average across layers\n",
    "    combined = torch.cat(all_embeddings, dim=-1)  # Concatenation\n",
    "    # OR: combined = torch.mean(torch.stack(all_embeddings), dim=0)  # Average\n",
    "    \n",
    "    return combined\n",
    "```\n",
    "\n",
    "### 2. Biaffine Attention üéØ\n",
    "\n",
    "Advanced edge scoring using biaffine transformations:\n",
    "\n",
    "```python\n",
    "class BiaffineAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.head_mlp = nn.Linear(768, hidden_dim)\n",
    "        self.dep_mlp = nn.Linear(768, hidden_dim)\n",
    "        self.biaffine = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        head_repr = self.head_mlp(embeddings)  # [seq_len, hidden_dim]\n",
    "        dep_repr = self.dep_mlp(embeddings)    # [seq_len, hidden_dim]\n",
    "        \n",
    "        # Biaffine scoring: head^T * W * dep for all pairs\n",
    "        scores = torch.einsum('ih,hj,jk->ik', \n",
    "                             head_repr, \n",
    "                             self.biaffine.weight, \n",
    "                             dep_repr.transpose(0, 1))\n",
    "        return scores\n",
    "```\n",
    "\n",
    "### 3. Constrained Training üîí\n",
    "\n",
    "Ensure model predictions form valid trees during training:\n",
    "\n",
    "```python\n",
    "def tree_constraint_loss(edge_probs, true_edges):\n",
    "    \"\"\"Add penalty for violating tree constraints\"\"\"\n",
    "    # Penalty for disconnected components\n",
    "    connectivity_loss = compute_connectivity_penalty(edge_probs)\n",
    "    \n",
    "    # Penalty for cycles\n",
    "    cycle_loss = compute_cycle_penalty(edge_probs)\n",
    "    \n",
    "    # Standard BCE loss\n",
    "    bce_loss = F.binary_cross_entropy(edge_probs, true_edges)\n",
    "    \n",
    "    return bce_loss + 0.1 * connectivity_loss + 0.1 * cycle_loss\n",
    "```\n",
    "\n",
    "### 4. Ensemble Methods üé™\n",
    "\n",
    "Combine multiple models for better performance:\n",
    "\n",
    "```python\n",
    "class EnsembleParser:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        # Get predictions from all models\n",
    "        all_edge_probs = []\n",
    "        all_root_probs = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            edge_p, root_p = model.predict(sentence)\n",
    "            all_edge_probs.append(edge_p)\n",
    "            all_root_probs.append(root_p)\n",
    "        \n",
    "        # Average predictions\n",
    "        avg_edge_probs = torch.mean(torch.stack(all_edge_probs), dim=0)\n",
    "        avg_root_probs = torch.mean(torch.stack(all_root_probs), dim=0)\n",
    "        \n",
    "        return avg_edge_probs, avg_root_probs\n",
    "```\n",
    "\n",
    "### 5. Data Augmentation üìà\n",
    "\n",
    "Increase training data diversity:\n",
    "\n",
    "```python\n",
    "def augment_sentence(sentence):\n",
    "    \"\"\"Create variations of training sentences\"\"\"\n",
    "    augmentations = []\n",
    "    \n",
    "    # Word dropout: randomly remove words\n",
    "    if len(sentence) > 3:\n",
    "        indices = random.sample(range(1, len(sentence)-1), k=1)\n",
    "        new_words = [w for i, w in enumerate(sentence.words) if i not in indices]\n",
    "        augmentations.append(create_sentence_from_words(new_words))\n",
    "    \n",
    "    # Synonym replacement (if you have a synonym dictionary)\n",
    "    # Word reordering (for some languages)\n",
    "    \n",
    "    return augmentations\n",
    "```\n",
    "\n",
    "### üìö Research Directions\n",
    "\n",
    "- **Graph Neural Networks**: Use GNNs for iterative refinement\n",
    "- **Transformer Parsers**: End-to-end parsing with transformers\n",
    "- **Cross-lingual Transfer**: Train on multiple languages\n",
    "- **Few-shot Learning**: Adapt to new domains with little data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. üìñ Summary and Next Steps\n",
    "\n",
    "Congratulations! üéâ You've learned how to use deep learning for dependency parsing!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **üå≥ Dependency Parsing Fundamentals**:\n",
    "   - Understanding syntactic tree structures\n",
    "   - Head-dependent relationships and tree constraints\n",
    "   - Distance matrices and depth calculations\n",
    "\n",
    "2. **ü§ñ Neural Approach**:\n",
    "   - Using HerBERT for contextual word embeddings\n",
    "   - Two-model architecture: distance and depth prediction\n",
    "   - Handling subword tokenization and aggregation\n",
    "\n",
    "3. **üíª Implementation Skills**:\n",
    "   - PyTorch model architecture design\n",
    "   - Balanced training for imbalanced data\n",
    "   - Greedy tree construction algorithms\n",
    "\n",
    "4. **üìä Evaluation & Analysis**:\n",
    "   - UUAS and root placement metrics\n",
    "   - Error analysis and performance debugging\n",
    "   - Comparative evaluation strategies\n",
    "\n",
    "### For the Solution Implementation:\n",
    "\n",
    "You now have all the knowledge to implement the complete solution! The key components are:\n",
    "\n",
    "```python\n",
    "def your_dependency_parser(sentence, tokenizer, bert_model, distance_model, depth_model):\n",
    "    # 1. Extract word embeddings from HerBERT\n",
    "    embeddings = get_word_embeddings([sentence], tokenizer, bert_model)[0]\n",
    "    \n",
    "    # 2. Predict edge probabilities\n",
    "    edge_probs = predict_all_edges(embeddings, distance_model)\n",
    "    \n",
    "    # 3. Predict root probabilities  \n",
    "    root_probs = depth_model(embeddings)\n",
    "    \n",
    "    # 4. Construct tree greedily\n",
    "    tree = construct_dependency_tree(edge_probs, root_probs)\n",
    "    \n",
    "    return tree\n",
    "```\n",
    "\n",
    "### üöÄ Key Success Factors:\n",
    "\n",
    "- **Quality embeddings**: Proper subword aggregation is crucial\n",
    "- **Balanced training**: Handle class imbalance carefully\n",
    "- **Valid trees**: Ensure your algorithm always produces valid trees\n",
    "- **Hyperparameter tuning**: Learning rates, epochs, and model sizes matter\n",
    "- **Evaluation**: Monitor both UUAS and root placement\n",
    "\n",
    "### üéØ Expected Performance Targets:\n",
    "\n",
    "Based on the scoring function, aim for:\n",
    "- **UUAS**: 70-85% (unlabeled attachment score)\n",
    "- **Root Placement**: 75-90% (correct root identification)\n",
    "- **Final Score**: 1.0-2.0 points\n",
    "\n",
    "### üìö Useful Resources:\n",
    "\n",
    "- üõ†Ô∏è [HerBERT Documentation](https://huggingface.co/allegro/herbert-base-cased)\n",
    "- üìñ [Universal Dependencies](https://universaldependencies.org/) - Annotation guidelines\n",
    "- üìë [Dependency Parsing Survey](https://www.aclweb.org/anthology/J08-4003.pdf)\n",
    "- üéì [Stanford CS224N](http://web.stanford.edu/class/cs224n/) - NLP Course\n",
    "\n",
    "### üî• Advanced Topics to Explore:\n",
    "\n",
    "- **Graph-based parsing**: Maximum spanning tree algorithms\n",
    "- **Transition-based parsing**: Arc-standard and arc-eager systems\n",
    "- **Neural architectures**: BiLSTMs, attention mechanisms, transformers\n",
    "- **Cross-lingual parsing**: Universal Dependencies and multilingual models\n",
    "\n",
    "**Good luck with your implementation!** üåü\n",
    "\n",
    "Remember: The key insight is that neural networks can learn complex syntactic patterns from contextual embeddings, making them much more powerful than traditional feature-based approaches. Your models will learn to identify both local syntactic patterns and long-range dependencies automatically!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
