{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu7dm0kO2sbV"
   },
   "source": [
    "# Analiza zale≈ºno≈õciowa\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## Wstƒôp\n",
    "\n",
    "Jƒôzyk, kt√≥rym pos≈Çugujemy siƒô na co dzie≈Ñ, funkcjonuje na zasadzie kompozycyjno≈õci. Oznacza to, ≈ºe znaczenie z≈Ço≈ºonych wyra≈ºe≈Ñ jƒôzykowych mo≈ºna wywnioskowaƒá z ich czƒô≈õci sk≈Çadowych i z relacji miƒôdzy nimi. Ta w≈Ça≈õciwo≈õƒá pozwala u≈ºytkownikom jƒôzyka na daleko idƒÖcƒÖ kreatywno≈õƒá w sposobie konstruowania wypowiedzi, przy zachowaniu precyzji komunikacji. Spos√≥b w jaki s≈Çowa w zdaniu sƒÖ ze sobƒÖ zwiƒÖzane, tworzy strukturƒô ukorzenionego drzewa. Problemem, kt√≥ry rozwa≈ºamy w tym zadaniu, jest automatyczna konstrukcja takich drzew dla zda≈Ñ w jƒôzyku polskim. Problem nosi nazwƒô analizy sk≈Çadniowej zda≈Ñ, a konkretnie dokonywaƒá bƒôdziemy analizy zale≈ºno≈õciowej.\n",
    "\n",
    "Analiza sk≈Çadniowa jest w og√≥lno≈õci trudna. Na przyk≈Çad, mimo ≈ºe zdania `(1) Maria do jutra jest zajƒôta.` oraz `(2) Droga do domu jest zajƒôta.` zawierajƒÖ kolejno te same czƒô≈õci mowy, w dodatku o dok≈Çadnie tej samej formie gramatycznej, to w zdaniu (1) fraza \"do jutra\" modyfikuje czasownik \"jest zajƒôta\", natomiast w zdaniu (2) fraza \"do domu\" jest podrzƒôdnikiem rzeczownika \"droga\". W dodatku, czasami nawet natywni u≈ºytkownicy jƒôzyka mogƒÖ zinterpretowaƒá strukturƒô¬†zdania na dwa r√≥≈ºne sposoby: zdanie `Zauwa≈ºy≈Çem dzi≈õ samoch√≥d Adama, kt√≥rego dawno nie widzia≈Çem.` mo≈ºe byƒá interpretowane na dwa sposoby w zale≈ºno≈õci od tego, do czego odnosi siƒô \"kt√≥rego\": czy do \"samochodu Adama\", czy mo≈ºe do \"Adama\".\n",
    "\n",
    "Istnieje wiele r√≥≈ºnych algorytm√≥w rozwiƒÖzujƒÖcych problem analizy zale≈ºno≈õciowej. Klasyczne metody przetwarzajƒÖ zdanie s≈Çowo po s≈Çowie, od lewej do prawej i wstawiajƒÖ krawƒôdzie w oparciu albo o pewien ustalony zbi√≥r regu≈Ç lub o algorytm uczenia maszynowego. W tym zadaniu u≈ºyjemy innej metody. Twoim zadaniem bƒôdzie przewidzenie drzewa zale≈ºno≈õciowego w oparciu o wektory s≈Ç√≥w otrzymane modelem HerBERT.\n",
    "\n",
    "HerBERT to polska wersja BERT, kt√≥ry jest modelem jƒôzykowym i dzia≈Ça nastƒôpujƒÖco:\n",
    "1. BERT posiada modu≈Ç nazywany tokenizatorem (ang. tokenizer), kt√≥ry dzieli zdanie na pewne pods≈Çowa. Na przyk≈Çad zdanie `Dostajƒô klucz i biegnƒô do swojego pokoju.` dzieli na `'Dosta', 'jƒô', 'klucz', 'i', 'bieg', 'nƒô', 'do', 'swojego', 'pokoju', '.'`. Tokenizator jest wyposa≈ºony w s≈Çownik, kt√≥ry pods≈Çowom przypisuje unikalne liczby: w praktyce zatem otrzymujemy ma≈Ço zrozumia≈Çe dla cz≈Çowieka `18577, 2779, 22816, 1009, 4775, 2788, 2041, 5058, 7217, 1899`.\n",
    "1. Nastƒôpnie BERT posiada s≈Çownik, kt√≥ry zamienia te liczby na wektory o d≈Çugo≈õci 768. Otrzymujemy zatem macierz o rozmiarach `10 x 768`.\n",
    "1. BERT posiada 12 warstw, z kt√≥rych ka≈ºda bierze wynik poprzedniej i wykonuje na niej pewnƒÖ¬†transformacjƒô. Szczeg√≥≈Çy nie sƒÖ istotne w tym zadaniu! Wa≈ºne jest natomiast to, ≈ºe ca≈Çy model jest uczony automatycznie, przy u≈ºyciu du≈ºych korpus√≥w tekstu. Zinterpretowanie dzia≈Çania ka≈ºdej warstwy jest niemo≈ºliwe! Natomiast byƒá mo≈ºe w skomplikowanym algorytmie, kt√≥rego nauczy≈Ç siƒô¬†BERT r√≥≈ºne warstwy pe≈ÇniƒÖ r√≥≈ºne role.\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "Twoim zadaniem bƒôdzie automatyczna analiza sk≈Çadniowa zda≈Ñ w jƒôzyku polskim. Pominiemy dok≈Çadne obja≈õnienie sposobu konstruowania takich drzew, mo≈ºesz samemu popatrzeƒá¬†na przyk≈Çady! Dostaniesz zbi√≥r danych treningowych zawierajƒÖcy 1000 przyk≈Çad√≥w rozk≈Çad√≥w zda≈Ñ. W pliku `train.conll` znajdujƒÖ siƒô poetykietowane zdania, na przyk≈Çad:\n",
    "\n",
    "| # | Word      | - | - | - | - | Head | - | - | - |\n",
    "|---|-----------|---|---|---|---|--------|---|---|---|\n",
    "| 1 | Wyobra≈∫   | _ | _ | _ | _ | 0      | _ | _ | _ |\n",
    "| 2 | sobie     | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "| 3 | cz≈Çowieka | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "| 4 | znajdujƒÖcego | _ | _ | _ | _ | 3    | _ | _ | _ |\n",
    "| 5 | siƒô       | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
    "| 6 | na        | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
    "| 7 | ogromnej  | _ | _ | _ | _ | 8      | _ | _ | _ |\n",
    "| 8 | g√≥rze     | _ | _ | _ | _ | 6      | _ | _ | _ |\n",
    "| 9 | .         | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "\n",
    "Co jest sposobem na zakodowanie nastƒôpujƒÖcego drzewa sk≈Çadniowego zdania z≈Ço≈ºonego:\n",
    "```\n",
    "      Wyobra≈∫                          \n",
    "   ______|_____________                 \n",
    "  |      |         cz≈Çowieka           \n",
    "  |      |             |                \n",
    "  |      |        znajdujƒÖcego         \n",
    "  |      |      _______|__________      \n",
    "  |      |     |                  na   \n",
    "  |      |     |                  |     \n",
    "  |      |     |                g√≥rze  \n",
    "  |      |     |                  |     \n",
    "sobie    .    siƒô              ogromnej\n",
    "```\n",
    "Dostarczamy Ci funkcjƒô w Pythonie s≈Çu≈ºƒÖcƒÖ do wczytania przyk≈Çad√≥w z tego pliku i na ich wizualizacjƒô. Twoje rozwiƒÖzanie powinno:\n",
    "1. Dzieliƒá zdanie na pods≈Çowa.\n",
    "1. Dla ka≈ºdego pods≈Çowa przypisywaƒá wektor. Nale≈ºy u≈ºyƒá tutaj finalnych lub po≈õrednich wektor√≥w wyliczonych przez model HerBERT.\n",
    "1. Agregowaƒá wektory pods≈Ç√≥w tak aby otrzymaƒá wektory s≈Ç√≥w.\n",
    "1. Zaimplementowaƒá i wyuczyƒá prosty model przewidujƒÖcy odleg≈Ço≈õci w drzewie i g≈Çƒôboko≈õci w drzewie poszczeg√≥lnych s≈Ç√≥w w zdaniu.\n",
    "1. U≈ºyƒá modeli odleg≈Ço≈õci i g≈Çƒôboko≈õci do skonstruowania drzewa sk≈Çadniowego.\n",
    "\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje finalne rozwiƒÖzanie bƒôdzie testowane w ≈õrodowisku **bez** GPU.\n",
    "- Ewaluacja twojego rozwiƒÖzania (bez treningu) na 200 przyk≈Çadach testowych powinna trwaƒá¬†nie d≈Çu≈ºej ni≈º¬†5 minut na Google Colab bez GPU.\n",
    "- Do dyspozycji masz model typu BERT: `allegro/herbert-base-cased` oraz tokenizer `allegro/herbert-base-cased`. Nie wolno korzystaƒá z innych uprzednio wytrenowanych modeli oraz ze zbior√≥w danych innych ni≈º dostarczony.\n",
    "- Lista dopuszczalnych bibliotek: `transformers`, `nltk`, `torch`.\n",
    "\n",
    "## Uwagi i wskaz√≥wki\n",
    "- Liczne wskaz√≥wki znajdujƒÖ siƒô we wzorcach funkcji, kt√≥re powiniene≈õ zaimplementowaƒá.\n",
    "\n",
    "## Pliki zg≈Çoszeniowe\n",
    "RozwiƒÖzanie zadania stanowi plik archiwum zip zawierajƒÖce:\n",
    "1. Ten notebook\n",
    "2. Plik z wagami modelu odleg≈Ço≈õci: `distance_model.pth`\n",
    "3. Plik z wagami modelu g≈Çƒôboko≈õci: `depth_model.pth`\n",
    "\n",
    "Uruchomienie ca≈Çego notebooka z flagƒÖ `FINAL_EVALUATION_MODE` ustawionƒÖ na `False` powinno w maksymalnie 10 minut skutkowaƒá utworzeniem obu plik√≥w z wagami.\n",
    "\n",
    "## Ewaluacja\n",
    "Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`, a nastƒôpnie zostanie uruchomiony ca≈Çy notebook.\n",
    "Zaimplementowana przez Ciebie funkcja `parse_sentence`, kt√≥rej wzorzec znajdziesz na ko≈Ñcu tego notatnika, zostanie oceniona na 200 przyk≈Çadach testowych.\n",
    "Ewaluacja bƒôdzie podobna do tej zaimplementowanej w funkcji `evaluate_model`.\n",
    "Pamiƒôtaj jednak, ≈ºe ostateczna funkcja do ewaluacji sprawdza≈Ça bƒôdzie dodatkowo, czy zwracane przez twojƒÖ funkcjƒô `parse_sentence` drzewa sƒÖ poprawne!\n",
    "\n",
    "Ewaluacja nie mo≈ºe zajmowaƒá wiƒôcej ni≈º 3 minuty. Mo≈ºesz uruchomiƒá walidacjƒô swojego rozwiƒÖzania na dostarczonym zbiorze danych walidacyjnych na Google Colab, aby przekonaƒá siƒô czy nie przekraczasz czasu.\n",
    "Za pomocƒÖ skryptu `validation_script.py` bƒôdziesz m√≥g≈Ç upewniƒá siƒô, ≈ºe Twoje rozwiƒÖzanie zostanie prawid≈Çowo wykonane na naszych serwerach oceniajƒÖcych:\n",
    "\n",
    "```\n",
    "python3 validation_script.py --train\n",
    "python3 validation_script.py\n",
    "```\n",
    "\n",
    "Podczas sprawdzania zadania, u≈ºyjemy dw√≥ch metryk: UUAS oraz root placement.\n",
    "1. Root placemenet oznacza u≈Çamek przyk≈Çad√≥w na kt√≥rych poprawnie wska≈ºesz korze≈Ñ drzewa sk≈Çadniowego,\n",
    "2. UUAS dla konkretnego zdania to u≈Çamek poprawnie umieszczonych krawƒôdzi. UUAS dla zbioru to ≈õrednia wynik√≥w dla poszczeg√≥lnych zda≈Ñ.\n",
    "\n",
    "\n",
    "Za to zadanie mo≈ºesz zdobyƒá¬†pomiƒôdzy pomiƒôdzy 0 i 2 punkty. Tw√≥j wynik za to zadanie zostanie wyliczony za pomocƒÖ funkcji:\n",
    "```Python\n",
    "def points(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "    return (scale(root_placement) + scale(uuas))\n",
    "```\n",
    "Innymi s≈Çowy, tw√≥j wynik jest sumƒÖ wynik√≥w za root placement i UUAS. Wynik za danƒÖ metrykƒô jest 0 je≈õli warto≈õƒá danej metryki jest poni≈ºej 0.5 i 1 je≈õli jest powy≈ºej 0.85. Pomiƒôdzy tymi warto≈õciami, wynik ro≈õnie liniowo z warto≈õciƒÖ metryki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4loni7x2sba"
   },
   "source": [
    "# Kod startowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4x3my7r2sbb"
   },
   "outputs": [],
   "source": [
    "FINAL_EVALUATION_MODE = (\n",
    "    False  # W czasie sprawdzania twojego rozwiƒÖzania, zmienimy tƒÖ warto≈õƒá na True\n",
    ")\n",
    "DEPTH_MODEL_PATH = \"depth_model.pth\"  # Nie zmieniaj!\n",
    "DISTANCE_MODEL_PATH = \"distance_model.pth\"  # Nie zmieniaj!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgEWf7M82sbb"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer\n",
    "from utils import (\n",
    "    ListDataset,\n",
    "    ParsedSentence,\n",
    "    Sentence,\n",
    "    merge_subword_tokens,\n",
    "    read_conll,\n",
    "    uuas_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTGWj-tW2sbb"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KFhuQwt2sbc"
   },
   "outputs": [],
   "source": [
    "train_sentences = read_conll(\"train.conll\")  # 1000 zda≈Ñ\n",
    "val_sentences = read_conll(\"valid.conll\")  # 200 zda≈Ñ\n",
    "\n",
    "train_sentences[6].pretty_print()  # wy≈õwietl drzewo jednego zdania\n",
    "print(train_sentences[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQYUDgF2sbc"
   },
   "source": [
    "# Twoje rozwiƒÖzanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6g9oW4g2sbc"
   },
   "outputs": [],
   "source": [
    "def get_distances(sentence: ParsedSentence):\n",
    "    \"\"\"Znajd≈∫ odleg≈Ço≈õci miƒôdzy ka≈ºdƒÖ parƒÖ s≈Ç√≥w w zdaniu.\n",
    "    Zwraca macierz numpy o wymiarach (len(sentence), len(sentence)).\"\"\"\n",
    "    distances = np.zeros((len(sentence), len(sentence)))\n",
    "    neighbors = [[] for _ in range(len(sentence))]\n",
    "    for a, b in sentence.get_sorted_edges():\n",
    "        neighbors[a].append(b)\n",
    "        neighbors[b].append(a)\n",
    "\n",
    "    def dfs(v, vis, dist):\n",
    "        vis[v] = True\n",
    "        for neighbor in neighbors[v]:\n",
    "            if vis[neighbor]:\n",
    "                continue\n",
    "            dist[neighbor] = dist[v] + 1\n",
    "            dfs(neighbor, vis, dist)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        visited = [False for _ in range(len(sentence))]\n",
    "        visited[i] = True\n",
    "        distan = [0 for _ in range(len(sentence))]\n",
    "        dfs(i, visited, distan)\n",
    "        for idx, dist in enumerate(distan):\n",
    "            distances[i][idx] = dist\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "print(get_distances(train_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq--HUNP2sbd"
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(\n",
    "    sentences_s: List[str],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    progress_bar: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ekstraktuje embeddingi pods≈Ç√≥w z modelu HerBERT dla listy zda≈Ñ.\n",
    "\n",
    "    DLACZEGO U≈ªYWAMY MODELU BERT:\n",
    "    - BERT (HerBERT w przypadku polskiego) to kontekstowy model jƒôzykowy\n",
    "    - Ka≈ºde s≈Çowo otrzymuje reprezentacjƒô wektorowƒÖ kt√≥ra zale≈ºy od kontekstu\n",
    "    - Embeddingi BERT zawierajƒÖ bogatƒÖ informacjƒô semantycznƒÖ i sk≈ÇadniowƒÖ\n",
    "\n",
    "    PROCES TOKENIZACJI:\n",
    "    1. BERT dzieli zdania na pods≈Çowa (subwords) u≈ºywajƒÖc algorytmu WordPiece\n",
    "    2. Przyk≈Çad: \"Dostajƒô\" ‚Üí [\"Dosta\", \"jƒô\"]\n",
    "    3. Ka≈ºdy pods≈Ç√≥w otrzymuje unikalny ID ze s≈Çownika\n",
    "\n",
    "    ARCHITEKTURA BERT:\n",
    "    - 12 warstw transformer'a\n",
    "    - Ka≈ºda warstwa przetwarza reprezentacje z poprzedniej warstwy\n",
    "    - U≈ºywamy warstwy 7 (≈õrodkowej) jako kompromis miƒôdzy specjalizacjƒÖ a og√≥lno≈õciƒÖ\n",
    "\n",
    "    Args:\n",
    "        sentences_s: Lista zda≈Ñ jako stringi\n",
    "        tokenizer: Tokenizator HerBERT do podzia≈Çu na pods≈Çowa\n",
    "        model: Model HerBERT do generowania embedding√≥w\n",
    "        progress_bar: Czy pokazywaƒá pasek postƒôpu (opcjonalne)\n",
    "\n",
    "    Returns:\n",
    "        tuple zawierajƒÖca:\n",
    "        - tokens: Lista tensor√≥w z ID token√≥w dla ka≈ºdego zdania\n",
    "        - embeddings: Lista tensor√≥w z embeddingami o wymiarach (seq_len, 768)\n",
    "    \"\"\"\n",
    "\n",
    "    # Wskaz√≥wki:\n",
    "    #  1. Mo≈ºesz u≈ºyƒá funkcji:\n",
    "    #   encoded = tokenizer.batch_encode_plus(...)\n",
    "    #   with torch.no_grad():\n",
    "    #     model(**encoded, output_hidden_states=True)\n",
    "    #  2. Aby przyspieszyƒá obliczenia, pamiƒôtaj o zgrupowaniu (batching) zda≈Ñ, przed podaniem ich do modelu.\n",
    "    #  3. Pamiƒôtaj, ≈ºe ka≈ºde zdanie mo≈ºe mieƒá innƒÖ d≈Çugo≈õƒá, wiƒôc ≈ºeby wype≈Çniƒá dodatkowe miejsce w zwracanym\n",
    "    #   tensorze, HERBERT zastosuje padding. Pamiƒôtaj o usuniƒôciu paddingu z wynik√≥w.\n",
    "    #  4. Tokenizator i model u≈ºywa specjalnych token√≥w (np. poczƒÖtku i ko≈Ñca zdania), kt√≥re r√≥wnie≈º powinny\n",
    "    #   zostaƒá usuniƒôte.\n",
    "\n",
    "    # KROK 1: Tokenizacja i kodowanie wszystkich zda≈Ñ jednocze≈õnie (batching)\n",
    "    # batch_encode_plus automatycznie:\n",
    "    # - Dodaje specjalne tokeny [CLS] na poczƒÖtku i [SEP] na ko≈Ñcu\n",
    "    # - Aplikuje padding do najd≈Çu≈ºszego zdania w batchu\n",
    "    # - Tworzy attention_mask wskazujƒÖcƒÖ prawdziwe tokeny vs padding\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        sentences_s,\n",
    "        return_tensors=\"pt\",  # Zwr√≥ƒá jako PyTorch tensory\n",
    "        padding=True,  # Dodaj padding do jednakowej d≈Çugo≈õci\n",
    "        truncation=True,  # Obetnij za d≈Çugie zdania\n",
    "    )\n",
    "\n",
    "    # KROK 2: Przepuszczenie przez model BERT\n",
    "    # torch.no_grad() wy≈ÇƒÖcza obliczanie gradient√≥w (oszczƒôdno≈õƒá pamiƒôci)\n",
    "    # output_hidden_states=True zwraca ukryte stany ze wszystkich warstw\n",
    "    with torch.no_grad():\n",
    "        bert_output = model(**encoded, output_hidden_states=True)\n",
    "\n",
    "    # KROK 3: Ekstrakcja token√≥w i embedding√≥w dla ka≈ºdego zdania\n",
    "    tokens = []\n",
    "    embeddings = []\n",
    "\n",
    "    for sentence_idx in range(len(sentences_s)):\n",
    "        # Maska wskazujƒÖca kt√≥re pozycje to prawdziwe tokeny (nie padding)\n",
    "        attention_mask = encoded[\"attention_mask\"][sentence_idx].bool()\n",
    "\n",
    "        # Ekstraktujemy prawdziwe tokeny (bez padding)\n",
    "        sentence_tokens = encoded[\"input_ids\"][sentence_idx, attention_mask]\n",
    "\n",
    "        # Usuwamy specjalne tokeny [CLS] (pierwszy) i [SEP] (ostatni)\n",
    "        sentence_tokens_clean = sentence_tokens[1:-1]\n",
    "        tokens.append(sentence_tokens_clean)\n",
    "\n",
    "        # Ekstraktujemy embeddingi z warstwy 7 (indeksowane od 0)\n",
    "        # hidden_states[0] = embeddingi wej≈õciowe\n",
    "        # hidden_states[1-12] = warstwy transformer'a\n",
    "        # Wybieramy warstwƒô 7 jako dobry kompromis miƒôdzy og√≥lno≈õciƒÖ a specjalizacjƒÖ\n",
    "        layer_7_embeddings = bert_output.hidden_states[7][sentence_idx]\n",
    "\n",
    "        # Usuwamy embeddingi dla padding i specjalnych token√≥w\n",
    "        sentence_embeddings = layer_7_embeddings[attention_mask][1:-1]\n",
    "        embeddings.append(sentence_embeddings)\n",
    "\n",
    "    return tokens, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq76vhja2sbd"
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(\n",
    "    sentences: List[Sentence], tokenizer, model\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Konwertuje embeddingi pods≈Ç√≥w na embeddingi pe≈Çnych s≈Ç√≥w poprzez agregacjƒô.\n",
    "\n",
    "    PROBLEM DO ROZWIƒÑZANIA:\n",
    "    - BERT tokenizuje s≈Çowa na pods≈Çowa (np. \"programowanie\" ‚Üí [\"program\", \"owanie\"])\n",
    "    - Potrzebujemy jednego embeddingu na s≈Çowo, nie na pods≈Çowo\n",
    "    - RozwiƒÖzanie: agregujemy embeddingi pods≈Ç√≥w nale≈ºƒÖcych do tego samego s≈Çowa\n",
    "\n",
    "    STRATEGIA AGREGACJI:\n",
    "    - U≈ºywamy ≈õredniej arytmetycznej embedding√≥w pods≈Ç√≥w\n",
    "    - Alternatywy: suma, pierwsza pozycja, ostatnia pozycja\n",
    "    - ≈örednia zachowuje informacjƒô ze wszystkich pods≈Ç√≥w\n",
    "\n",
    "    PROCES:\n",
    "    1. Otrzymujemy embeddingi pods≈Ç√≥w z get_bert_embeddings\n",
    "    2. Grupujemy pods≈Çowa nale≈ºƒÖce do tego samego s≈Çowa\n",
    "    3. Obliczamy ≈õredniƒÖ embedding√≥w w ka≈ºdej grupie\n",
    "    4. Zwracamy embedding na poziomie s≈Ç√≥w\n",
    "\n",
    "    Args:\n",
    "        sentences: Lista obiekt√≥w Sentence zawierajƒÖcych s≈Çowa do przetworzenia\n",
    "        tokenizer: Tokenizator HerBERT\n",
    "        model: Model HerBERT\n",
    "\n",
    "    Returns:\n",
    "        Lista tensor√≥w, ka≈ºdy tensor zawiera embeddingi s≈Ç√≥w dla jednego zdania\n",
    "        Wymiary: [num_words, embedding_dim] dla ka≈ºdego zdania\n",
    "    \"\"\"\n",
    "\n",
    "    # KROK 1: Konwertujemy obiekty Sentence na stringi\n",
    "    # Potrzebne do przekazania do get_bert_embeddings\n",
    "    sentence_strings = [str(sentence) for sentence in sentences]\n",
    "\n",
    "    # KROK 2: Definiujemy funkcjƒô agregacji\n",
    "    # U≈ºywamy ≈õredniej arytmetycznej - najczƒô≈õciej stosowana metoda\n",
    "    def aggregation_function(subword_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Agreguje embeddingi pods≈Ç√≥w do jednego embeddingu s≈Çowa.\n",
    "\n",
    "        Args:\n",
    "            subword_embeddings: Tensor o wymiarach [num_subwords, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor o wymiarach [embedding_dim] - embedding s≈Çowa\n",
    "        \"\"\"\n",
    "        return torch.mean(subword_embeddings, dim=0)\n",
    "\n",
    "    # KROK 3: Otrzymujemy embeddingi pods≈Ç√≥w\n",
    "    subword_tokens, subword_embeddings = get_bert_embeddings(\n",
    "        sentence_strings, tokenizer, model\n",
    "    )\n",
    "\n",
    "    # KROK 4: Agregujemy pods≈Çowa do s≈Ç√≥w dla ka≈ºdego zdania\n",
    "    word_embeddings = []\n",
    "    for sentence, tokens, embeddings in zip(\n",
    "        sentences, subword_tokens, subword_embeddings\n",
    "    ):\n",
    "        # merge_subword_tokens to pomocnicza funkcja kt√≥ra:\n",
    "        # - Mapuje tokeny pods≈Ç√≥w z powrotem na oryginalne s≈Çowa\n",
    "        # - Stosuje funkcjƒô agregacji do pods≈Ç√≥w nale≈ºƒÖcych do tego samego s≈Çowa\n",
    "        sentence_word_embeddings = merge_subword_tokens(\n",
    "            words=sentence.words,  # Lista oryginalnych s≈Ç√≥w\n",
    "            subword_tokens=tokens,  # Lista token√≥w pods≈Ç√≥w\n",
    "            subword_embeddings=embeddings,  # Embeddingi pods≈Ç√≥w\n",
    "            tokenizer=tokenizer,  # Tokenizator do dekodowania\n",
    "            aggregation_fn=aggregation_function,  # Funkcja agregacji\n",
    "            verbose=False,  # Bez logowania szczeg√≥≈Ç√≥w\n",
    "        )\n",
    "        word_embeddings.append(sentence_word_embeddings)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF486oQ02sbd"
   },
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    sentences: List[ParsedSentence], tokenizer, model\n",
    ") -> tuple[ListDataset, ListDataset]:\n",
    "    \"\"\"\n",
    "    Przygotowuje zbiory danych do trenowania modeli odleg≈Ço≈õci i g≈Çƒôboko≈õci.\n",
    "\n",
    "    DLACZEGO POTRZEBUJEMY DW√ìCH MODELI:\n",
    "    1. Model odleg≈Ço≈õci - przewiduje czy dwa s≈Çowa sƒÖ po≈ÇƒÖczone krawƒôdziƒÖ\n",
    "    2. Model g≈Çƒôboko≈õci - przewiduje kt√≥re s≈Çowo jest korzeniem drzewa\n",
    "\n",
    "    ARCHITEKTURA TRENINGU:\n",
    "    - Oba modele u≈ºywajƒÖ tych samych embedding√≥w s≈Ç√≥w jako wej≈õcie\n",
    "    - Model odleg≈Ço≈õci: input = para embedding√≥w, output = prawdopodobie≈Ñstwo krawƒôdzi\n",
    "    - Model g≈Çƒôboko≈õci: input = embedding s≈Çowa, output = prawdopodobie≈Ñstwo korzenia\n",
    "\n",
    "    PRZYGOTOWANIE DANYCH:\n",
    "    1. Obliczamy embeddingi s≈Ç√≥w u≈ºywajƒÖc HerBERT\n",
    "    2. Obliczamy prawdziwe odleg≈Ço≈õci w drzewie zale≈ºno≈õciowym\n",
    "    3. Wyznaczamy g≈Çƒôboko≈õci (odleg≈Ço≈õƒá od korzenia)\n",
    "    4. Pakujemy dane w formƒô gotowƒÖ do treningu\n",
    "\n",
    "    Args:\n",
    "        sentences: Lista ParsedSentence z prawdziwymi drzewami zale≈ºno≈õciowymi\n",
    "        tokenizer: Tokenizator HerBERT\n",
    "        model: Model HerBERT\n",
    "\n",
    "    Returns:\n",
    "        tuple zawierajƒÖca:\n",
    "        - dataset_dist: Dataset do treningu modelu odleg≈Ço≈õci\n",
    "        - dataset_depth: Dataset do treningu modelu g≈Çƒôboko≈õci\n",
    "    \"\"\"\n",
    "\n",
    "    # KROK 1: Obliczamy embeddingi s≈Ç√≥w dla wszystkich zda≈Ñ\n",
    "    # To jest najbardziej czasoch≈Çonna operacja, wiƒôc robimy jƒÖ raz\n",
    "    print(\"Obliczam embeddingi s≈Ç√≥w...\")\n",
    "    word_embeddings = get_word_embeddings(sentences, tokenizer, model)\n",
    "\n",
    "    # KROK 2: Obliczamy macierze odleg≈Ço≈õci dla ka≈ºdego zdania\n",
    "    # Odleg≈Ço≈õƒá = liczba krawƒôdzi miƒôdzy dwoma s≈Çowami w drzewie\n",
    "    print(\"Obliczam macierze odleg≈Ço≈õci...\")\n",
    "    distance_matrices = []\n",
    "    for sentence in sentences:\n",
    "        distance_matrix = get_distances(sentence)\n",
    "        distance_matrices.append(distance_matrix)\n",
    "\n",
    "    # KROK 3: Obliczamy g≈Çƒôboko≈õci s≈Ç√≥w (odleg≈Ço≈õƒá od korzenia)\n",
    "    # G≈Çƒôboko≈õƒá = odleg≈Ço≈õƒá od korzenia drzewa do danego s≈Çowa\n",
    "    print(\"Obliczam g≈Çƒôboko≈õci s≈Ç√≥w...\")\n",
    "    depth_vectors = []\n",
    "    for distance_matrix, sentence in zip(distance_matrices, sentences):\n",
    "        # G≈Çƒôboko≈õƒá ka≈ºdego s≈Çowa = odleg≈Ço≈õƒá od korzenia\n",
    "        root_index = sentence.root\n",
    "        depths = distance_matrix[root_index]  # Wiersz odpowiadajƒÖcy korzeniowi\n",
    "\n",
    "        # Dodajemy wymiar [1] ≈ºeby otrzymaƒá tensor 2D: [num_words, 1]\n",
    "        depths = depths[..., None]  # Przekszta≈Çca [n] na [n, 1]\n",
    "        depth_vectors.append(depths)\n",
    "\n",
    "    # KROK 4: Tworzymy datasety\n",
    "    # Ka≈ºdy element datasetu to krotka: (embeddings, targets, sentence)\n",
    "\n",
    "    # Dataset dla modelu odleg≈Ço≈õci\n",
    "    # Targets = macierz odleg≈Ço≈õci miƒôdzy wszystkimi parami s≈Ç√≥w\n",
    "    distance_data = list(zip(word_embeddings, distance_matrices, sentences))\n",
    "    dataset_dist = ListDataset(distance_data)\n",
    "\n",
    "    # Dataset dla modelu g≈Çƒôboko≈õci\n",
    "    # Targets = wektor g≈Çƒôboko≈õci dla ka≈ºdego s≈Çowa\n",
    "    depth_data = list(zip(word_embeddings, depth_vectors, sentences))\n",
    "    dataset_depth = ListDataset(depth_data)\n",
    "\n",
    "    print(f\"Utworzono datasety: {len(dataset_dist)} przyk≈Çad√≥w\")\n",
    "    return dataset_dist, dataset_depth\n",
    "\n",
    "\n",
    "# Tworzenie zbior√≥w treningowych i walidacyjnych\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"Przygotowujƒô zbiory treningowe...\")\n",
    "    trainset_dist, trainset_depth = get_datasets(train_sentences, tokenizer, model)\n",
    "\n",
    "    print(\"Przygotowujƒô zbiory walidacyjne...\")\n",
    "    valset_dist, valset_depth = get_datasets(val_sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mH7K2BOo2sbe"
   },
   "outputs": [],
   "source": [
    "def pad_arrays(sequence: List[np.ndarray], pad_with: float = np.inf) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dope≈Çnia tablice o r√≥≈ºnych rozmiarach do jednakowych wymiar√≥w.\n",
    "\n",
    "    PROBLEM BATCH'OWANIA:\n",
    "    - Zdania majƒÖ r√≥≈ºne d≈Çugo≈õci (r√≥≈ºnƒÖ liczbƒô s≈Ç√≥w)\n",
    "    - PyTorch wymaga tensor√≥w o jednakowych wymiarach w batch'u\n",
    "    - RozwiƒÖzanie: padding - dope≈Çnianie kr√≥tszych sekwencji\n",
    "\n",
    "    ALGORYTM:\n",
    "    1. Znajd≈∫ maksymalne wymiary we wszystkich tablicach\n",
    "    2. Dope≈Çnij ka≈ºdƒÖ tablicƒô do maksymalnych wymiar√≥w\n",
    "    3. U≈ºyj specjalnej warto≈õci (pad_with) do oznaczenia dope≈Çnienia\n",
    "\n",
    "    DLACZEGO np.inf:\n",
    "    - Umo≈ºliwia ≈Çatwe tworzenie masek (prawdziwe dane != inf)\n",
    "    - Nie wp≈Çywa na obliczenia gdy u≈ºywamy mask\n",
    "\n",
    "    Args:\n",
    "        sequence: Lista tablic numpy o potencjalnie r√≥≈ºnych rozmiarach\n",
    "        pad_with: Warto≈õƒá u≈ºywana do dope≈Çnienia (domy≈õlnie np.inf)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor z dope≈Çnionymi danymi\n",
    "    \"\"\"\n",
    "    # KROK 1: Znajd≈∫ maksymalne wymiary\n",
    "    # Pobieramy kszta≈Çty wszystkich tablic w sekwencji\n",
    "    shapes = np.array([list(array.shape) for array in sequence])\n",
    "    max_dimensions = list(shapes.max(axis=0))  # Maksimum dla ka≈ºdego wymiaru\n",
    "\n",
    "    # KROK 2: Dope≈Çnij ka≈ºdƒÖ tablicƒô do maksymalnych wymiar√≥w\n",
    "    padded_arrays = []\n",
    "    for array in sequence:\n",
    "        # Oblicz ile dope≈Çnienia potrzeba dla ka≈ºdego wymiaru\n",
    "        padding_amounts = []\n",
    "        for dim_idx in range(array.ndim):\n",
    "            current_size = array.shape[dim_idx]\n",
    "            max_size = max_dimensions[dim_idx]\n",
    "            padding_needed = max_size - current_size\n",
    "            # np.pad przyjmuje padding jako (przed, po) dla ka≈ºdego wymiaru\n",
    "            padding_amounts.append((0, padding_needed))\n",
    "\n",
    "        # Zastosuj padding\n",
    "        padded_array = np.pad(\n",
    "            array, tuple(padding_amounts), mode=\"constant\", constant_values=pad_with\n",
    "        )\n",
    "        padded_arrays.append(padded_array)\n",
    "\n",
    "    # KROK 3: Konwertuj na tensor PyTorch\n",
    "    return torch.tensor(padded_arrays, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: List[tuple],\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple]:\n",
    "    \"\"\"\n",
    "    Funkcja kolizji (collate function) dla DataLoader'a.\n",
    "\n",
    "    ROLA FUNKCJI COLLATE:\n",
    "    - DataLoader pobiera pojedyncze przyk≈Çady z datasetu\n",
    "    - collate_fn ≈ÇƒÖczy je w batch (paczkƒô) do efektywnego przetwarzania\n",
    "    - Musi rozwiƒÖzaƒá problem r√≥≈ºnych rozmiar√≥w tensor√≥w\n",
    "\n",
    "    PROCES:\n",
    "    1. Rozpakuj batch na sk≈Çadniki (embeddingi, targety, zdania)\n",
    "    2. Zastosuj padding do embedding√≥w i target√≥w\n",
    "    3. Utw√≥rz maskƒô wskazujƒÖcƒÖ prawdziwe dane (vs padding)\n",
    "    4. Zwr√≥ƒá wszystko jako tensory gotowe do treningu\n",
    "\n",
    "    Args:\n",
    "        batch: Lista krotek (embeddings, targets, sentences) z datasetu\n",
    "\n",
    "    Returns:\n",
    "        tuple zawierajƒÖca:\n",
    "        - padded_embeddings: Tensor embedding√≥w z paddingiem\n",
    "        - padded_targets: Tensor target√≥w z paddingiem\n",
    "        - mask: Maska prawdziwych danych (True) vs padding (False)\n",
    "        - sentences: Oryginalne zdania (do debugowania)\n",
    "    \"\"\"\n",
    "    # KROK 1: Rozpakuj batch na sk≈Çadniki\n",
    "    embeddings, targets, sentences = zip(*batch)\n",
    "\n",
    "    # KROK 2: Zastosuj padding\n",
    "    # Embeddingi dope≈Çniamy zerami (neutralne dla sieci neuronowej)\n",
    "    padded_embeddings = pad_arrays(embeddings, pad_with=0.0)\n",
    "\n",
    "    # Targety dope≈Çniamy niesko≈Ñczono≈õciƒÖ (≈Çatwe do maskowania)\n",
    "    padded_targets = pad_arrays(targets, pad_with=np.inf)\n",
    "\n",
    "    # KROK 3: Utw√≥rz maskƒô prawdziwych danych\n",
    "    # mask[i,j] = True je≈õli pozycja (i,j) zawiera prawdziwe dane\n",
    "    # mask[i,j] = False je≈õli pozycja (i,j) to padding\n",
    "    mask = padded_targets != torch.inf\n",
    "\n",
    "    return padded_embeddings, padded_targets, mask, sentences\n",
    "\n",
    "\n",
    "# Tworzenie DataLoader'√≥w - obiekt√≥w odpowiedzialnych za batch'owanie danych\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    # KONFIGURACJA DATALOADER'√ìW:\n",
    "    # - batch_size=32: Kompromis miƒôdzy wydajno≈õciƒÖ a zu≈ºyciem pamiƒôci\n",
    "    # - shuffle=True dla treningu: Zapobiega overfittingowi, poprawia generalizacjƒô\n",
    "    # - shuffle=False dla walidacji: Deterministyczne wyniki, ≈Çatwiejsze debugowanie\n",
    "    # - collate_fn: Nasza funkcja ≈ÇƒÖczƒÖca przyk≈Çady w batche\n",
    "\n",
    "    # DataLoader'y dla modelu odleg≈Ço≈õci\n",
    "    dist_trainloader = DataLoader(\n",
    "        trainset_dist,\n",
    "        batch_size=32,\n",
    "        shuffle=True,  # Mieszanie dla lepszego treningu\n",
    "        collate_fn=collate_fn,  # Nasza funkcja paddingu\n",
    "    )\n",
    "    dist_valloader = DataLoader(\n",
    "        valset_dist,\n",
    "        batch_size=32,\n",
    "        shuffle=False,  # Bez mieszania dla walidacji\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    # DataLoader'y dla modelu g≈Çƒôboko≈õci\n",
    "    depth_trainloader = DataLoader(\n",
    "        trainset_depth, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    depth_valloader = DataLoader(\n",
    "        valset_depth, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "# DOKUMENTACJA FORMAT√ìW DANYCH:\n",
    "#\n",
    "# DataLoader'y zwracajƒÖ krotki w formacie:\n",
    "# (padded_embeddings, padded_targets, mask, sentences)\n",
    "#\n",
    "# Dla modelu odleg≈Ço≈õci (dist_*loader):\n",
    "# - embeddings.shape: (batch_size, max_seq_len, 768)  # Embeddingi s≈Ç√≥w\n",
    "# - distances.shape: (batch_size, max_seq_len, max_seq_len)  # Macierz odleg≈Ço≈õci\n",
    "# - mask.shape: (batch_size, max_seq_len, max_seq_len)  # Maska prawdziwych danych\n",
    "#\n",
    "# Dla modelu g≈Çƒôboko≈õci (depth_*loader):\n",
    "# - embeddings.shape: (batch_size, max_seq_len, 768)  # Embeddingi s≈Ç√≥w\n",
    "# - depths.shape: (batch_size, max_seq_len, 1)  # G≈Çƒôboko≈õci s≈Ç√≥w\n",
    "# - mask.shape: (batch_size, max_seq_len, 1)  # Maska prawdziwych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXMfvteE2sbe"
   },
   "outputs": [],
   "source": [
    "class DistanceModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model przewidujƒÖcy prawdopodobie≈Ñstwo krawƒôdzi miƒôdzy parami s≈Ç√≥w.\n",
    "\n",
    "    ARCHITEKTURA:\n",
    "    - Wej≈õcie: Konkatenacja embedding√≥w dw√≥ch s≈Ç√≥w (768*2 = 1536 wymiar√≥w)\n",
    "    - Warstwa ukryta: 256 neuron√≥w z aktywacjƒÖ LeakyReLU\n",
    "    - Dropout: 30% dla regularyzacji (zapobieganie overfittingowi)\n",
    "    - Wyj≈õcie: 1 neuron z sigmoidƒÖ ‚Üí prawdopodobie≈Ñstwo krawƒôdzi\n",
    "\n",
    "    DLACZEGO TAKA ARCHITEKTURA:\n",
    "    - LeakyReLU: Lepsze ni≈º ReLU dla gradient flow\n",
    "    - Dropout: Zapobiega overfittingowi na ma≈Çym zbiorze danych\n",
    "    - Sigmoid: Wyj≈õcie w zakresie [0,1] jako prawdopodobie≈Ñstwo\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ARCHITEKTURA SIECI:\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # Warstwa wej≈õciowa: 768*2 (dwa embeddingi) ‚Üí 256\n",
    "            torch.nn.Linear(768 * 2, 256),\n",
    "            # Aktywacja: LeakyReLU pozwala na ma≈Çe gradienty dla ujemnych warto≈õci\n",
    "            torch.nn.LeakyReLU(negative_slope=0.01),\n",
    "            # Regularyzacja: Dropout losowo wy≈ÇƒÖcza 30% neuron√≥w podczas treningu\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            # Warstwa wyj≈õciowa: 256 ‚Üí 1 (prawdopodobie≈Ñstwo krawƒôdzi)\n",
    "            torch.nn.Linear(256, 1),\n",
    "            # Sigmoid: mapuje dowolnƒÖ liczbƒô rzeczywistƒÖ na zakres [0,1]\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Propagacja w prz√≥d przez sieƒá.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor o wymiarach [batch_size, 768*2] - para embedding√≥w\n",
    "\n",
    "        Returns:\n",
    "            Tensor o wymiarach [batch_size, 1] - prawdopodobie≈Ñstwo krawƒôdzi\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DepthModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model przewidujƒÖcy prawdopodobie≈Ñstwo ≈ºe s≈Çowo jest korzeniem drzewa.\n",
    "\n",
    "    ARCHITEKTURA:\n",
    "    - Wej≈õcie: Embedding pojedynczego s≈Çowa (768 wymiar√≥w)\n",
    "    - Warstwa ukryta: 256 neuron√≥w z aktywacjƒÖ LeakyReLU\n",
    "    - Dropout: 30% dla regularyzacji\n",
    "    - Wyj≈õcie: 1 neuron z sigmoidƒÖ ‚Üí prawdopodobie≈Ñstwo bycia korzeniem\n",
    "\n",
    "    ZADANIE MODELU:\n",
    "    - G≈Çƒôboko≈õƒá 0 = korze≈Ñ drzewa\n",
    "    - G≈Çƒôboko≈õƒá > 0 = nie korze≈Ñ\n",
    "    - Model przewiduje P(g≈Çƒôboko≈õƒá = 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ARCHITEKTURA SIECI:\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # Warstwa wej≈õciowa: 768 (jeden embedding) ‚Üí 256\n",
    "            torch.nn.Linear(768, 256),\n",
    "            # Aktywacja: LeakyReLU\n",
    "            torch.nn.LeakyReLU(negative_slope=0.01),\n",
    "            # Regularyzacja: Dropout 30%\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            # Warstwa wyj≈õciowa: 256 ‚Üí 1 (prawdopodobie≈Ñstwo korzenia)\n",
    "            torch.nn.Linear(256, 1),\n",
    "            # Sigmoid: prawdopodobie≈Ñstwo w zakresie [0,1]\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Propagacja w prz√≥d przez sieƒá.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor o wymiarach [batch_size, 768] - embeddingi s≈Ç√≥w\n",
    "\n",
    "        Returns:\n",
    "            Tensor o wymiarach [batch_size, 1] - prawdopodobie≈Ñstwo korzenia\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFZKmWLm2sbe"
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask): ...\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, valloader, epochs, lr):\n",
    "    \"\"\"Pƒôtla uczƒÖca twoich modeli.\"\"\"\n",
    "    # TODO: implement me\n",
    "    l = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if isinstance(model, DepthModel):\n",
    "            loss_sum = 0\n",
    "            correct = 0\n",
    "            size = 0\n",
    "            for input, label, mask, sentences in dataloader:\n",
    "                zero = input[(mask & (label == 0)).squeeze()]\n",
    "                not_zero = input[(mask & (label != 0)).squeeze()]\n",
    "\n",
    "                selected = not_zero[torch.randint(not_zero.shape[0], (zero.shape[0],))]\n",
    "                batch = torch.cat((zero, selected))\n",
    "                labels = torch.cat(\n",
    "                    (torch.zeros(zero.shape[0]), torch.ones(selected.shape[0]))\n",
    "                )\n",
    "\n",
    "                # print(labels)\n",
    "                output = model(batch).squeeze()\n",
    "                loss = l(output, labels.float())\n",
    "                with torch.no_grad():\n",
    "                    correct += torch.sum(labels == torch.round(output))\n",
    "                    loss_sum += loss * batch.shape[0]\n",
    "                    size += batch.shape[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            val_correct = 0\n",
    "            val_sum = 0\n",
    "            with torch.no_grad():\n",
    "                for input, label, mask, sentences in valloader:\n",
    "                    val_sum += torch.sum(mask)\n",
    "                    val_correct += torch.sum(\n",
    "                        torch.round(model(input[mask.squeeze()]))\n",
    "                        == torch.where(label[mask.squeeze()] == 0, 0, 1)\n",
    "                    )\n",
    "            print(\n",
    "                f\"epoch: {epoch} loss:{loss_sum/size} accuracy:{correct/size} accuracy_val: {val_correct/val_sum}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            loss_sum = 0\n",
    "            correct = 0\n",
    "            size = 0\n",
    "            for inputs, label, mask, sentences in dataloader:\n",
    "                n, m, d = inputs.shape\n",
    "                # Repeat and permute the tensor to get all pairwise combinations\n",
    "                tensor_i = inputs.unsqueeze(2).expand(n, m, m, d)  # Shape: (n, m, m, d)\n",
    "                tensor_j = inputs.unsqueeze(1).expand(n, m, m, d)  # Shape: (n, m, m, d)\n",
    "\n",
    "                # Concatenate along the last dimension\n",
    "                pairwise_concat = torch.cat((tensor_i, tensor_j), dim=-1)\n",
    "                input_processed = pairwise_concat[mask]\n",
    "                targets = np.where(label[mask] == 1, 1, 0)\n",
    "\n",
    "                edges = input_processed[label[mask] == 1]\n",
    "                notedges = input_processed[label[mask] != 1]\n",
    "\n",
    "                notedges_in = notedges[\n",
    "                    torch.randint(notedges.shape[0], (edges.shape[0],))\n",
    "                ]\n",
    "                inputs_final = torch.cat((edges, notedges))\n",
    "                labels_final = torch.cat(\n",
    "                    (torch.ones(edges.shape[0]), torch.zeros(notedges.shape[0]))\n",
    "                )\n",
    "\n",
    "                output = model(inputs_final).squeeze()\n",
    "                loss = l(output, labels_final.float())\n",
    "                with torch.no_grad():\n",
    "                    correct += torch.sum(labels_final == torch.round(output))\n",
    "                    loss_sum += loss * inputs_final.shape[0]\n",
    "                    size += inputs_final.shape[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            print(f\"epoch: {epoch} loss:{loss_sum/size} accuracy:{correct/size}\")\n",
    "\n",
    "\n",
    "# W czasie ewaluacji, modele nie powinny byƒá ponownie trenowane.\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"Training depth model\")\n",
    "    depth_model = DepthModel()\n",
    "    # TODO: ustaw hiperparametry\n",
    "    train_model(depth_model, depth_trainloader, depth_valloader, lr=0.0001, epochs=15)\n",
    "    # zapisz wagi modelu do pliku\n",
    "    torch.save(depth_model.state_dict(), DEPTH_MODEL_PATH)\n",
    "\n",
    "    print(\"Training distance model\")\n",
    "    distance_model = DistanceModel()\n",
    "    # # TODO: ustaw hiperparametry\n",
    "    train_model(distance_model, dist_trainloader, dist_valloader, lr=0.001, epochs=15)\n",
    "    # # zapisz wagi modelu do pliku\n",
    "    torch.save(distance_model.state_dict(), DISTANCE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TCZC-Kw2sbe"
   },
   "outputs": [],
   "source": [
    "def parse_sentence(\n",
    "    sent: Sentence, distance_model, depth_model, tokenizer, model\n",
    ") -> ParsedSentence:\n",
    "    \"\"\"\n",
    "    Buduje drzewo sk≈Çadniowe dla pojedynczego zdania u≈ºywajƒÖc wytrenowanych modeli.\n",
    "\n",
    "    ALGORYTM PARSOWANIA:\n",
    "    1. Oblicz embeddingi s≈Ç√≥w u≈ºywajƒÖc HerBERT\n",
    "    2. Wybierz korze≈Ñ u≈ºywajƒÖc modelu g≈Çƒôboko≈õci\n",
    "    3. Oblicz prawdopodobie≈Ñstwa krawƒôdzi u≈ºywajƒÖc modelu odleg≈Ço≈õci\n",
    "    4. Zbuduj drzewo algorytmem zach≈Çannym (greedy tree construction)\n",
    "\n",
    "    STRATEGIA BUDOWY DRZEWA:\n",
    "    - Zaczynamy od korzenia (wybrane przez model g≈Çƒôboko≈õci)\n",
    "    - Iteracyjnie dodajemy najbardziej prawdopodobne krawƒôdzie\n",
    "    - Zapewniamy ≈ºe wynik to drzewo (dok≈Çadnie n-1 krawƒôdzi, po≈ÇƒÖczony)\n",
    "\n",
    "    DLACZEGO TEN ALGORYTM:\n",
    "    - Prosty i deterministyczny\n",
    "    - Gwarantuje poprawne drzewo\n",
    "    - Wykorzystuje przewidywania obu modeli\n",
    "\n",
    "    Args:\n",
    "        sent: Zdanie do sparsowania\n",
    "        distance_model: Wytrenowany model przewidujƒÖcy krawƒôdzie\n",
    "        depth_model: Wytrenowany model przewidujƒÖcy korze≈Ñ\n",
    "        tokenizer: Tokenizator HerBERT\n",
    "        model: Model HerBERT\n",
    "\n",
    "    Returns:\n",
    "        ParsedSentence: Zdanie z przewidzianym drzewem sk≈Çadniowym\n",
    "    \"\"\"\n",
    "\n",
    "    # KROK 1: Oblicz embeddingi s≈Ç√≥w dla zdania\n",
    "    # Funkcja get_word_embeddings zwraca listƒô, wiƒôc bierzemy pierwszy (i jedyny) element\n",
    "    word_embeddings = get_word_embeddings([sent], tokenizer, model)[0]\n",
    "\n",
    "    # Dodajemy wymiar batch (unsqueeze(0)) ≈ºeby dopasowaƒá format oczekiwany przez modele\n",
    "    # Wymiary: [1, num_words, 768]\n",
    "    embeddings_batch = word_embeddings.unsqueeze(0)\n",
    "\n",
    "    # KROK 2: Przygotuj dane dla modelu odleg≈Ço≈õci\n",
    "    # Tworzymy wszystkie mo≈ºliwe pary embedding√≥w s≈Ç√≥w\n",
    "    batch_size, num_words, embedding_dim = embeddings_batch.shape\n",
    "\n",
    "    # Rozszerzamy tensor ≈ºeby otrzymaƒá wszystkie pary (i,j)\n",
    "    # tensor_i[b,i,j,:] = embedding s≈Çowa i w zdaniu b\n",
    "    tensor_i = embeddings_batch.unsqueeze(2).expand(\n",
    "        batch_size, num_words, num_words, embedding_dim\n",
    "    )\n",
    "    # tensor_j[b,i,j,:] = embedding s≈Çowa j w zdaniu b\n",
    "    tensor_j = embeddings_batch.unsqueeze(1).expand(\n",
    "        batch_size, num_words, num_words, embedding_dim\n",
    "    )\n",
    "\n",
    "    # Konkatenujemy embeddingi par s≈Ç√≥w\n",
    "    # pairwise_concat[b,i,j,:] = [embedding_i, embedding_j]\n",
    "    pairwise_concat = torch.cat((tensor_i, tensor_j), dim=-1)\n",
    "\n",
    "    # KROK 3: Przewidywania modeli\n",
    "    with torch.no_grad():  # Wy≈ÇƒÖczamy gradient tracking dla inferencji\n",
    "        # Model odleg≈Ço≈õci: prawdopodobie≈Ñstwa krawƒôdzi dla wszystkich par\n",
    "        edge_probabilities = distance_model(pairwise_concat)[\n",
    "            0\n",
    "        ]  # [num_words, num_words]\n",
    "\n",
    "        # Model g≈Çƒôboko≈õci: prawdopodobie≈Ñstwa korzenia dla ka≈ºdego s≈Çowa\n",
    "        root_probabilities = depth_model(embeddings_batch)[0]  # [num_words, 1]\n",
    "\n",
    "    # KROK 4: Wybierz korze≈Ñ drzewa\n",
    "    # S≈Çowo z najwy≈ºszym prawdopodobie≈Ñstwem bycia korzeniem\n",
    "    root_index = torch.argmax(root_probabilities.squeeze()).item()\n",
    "\n",
    "    # KROK 5: Buduj drzewo algorytmem zach≈Çannym\n",
    "    # Zaczynamy z korzeniem w drzewie\n",
    "    nodes_in_tree = {root_index}\n",
    "    edges = []\n",
    "\n",
    "    # Dodajemy dok≈Çadnie n-1 krawƒôdzi (w≈Ça≈õciwo≈õƒá drzewa)\n",
    "    for _ in range(len(sent) - 1):\n",
    "        best_edge = None\n",
    "        best_probability = -np.inf\n",
    "\n",
    "        # Sprawdzamy wszystkie mo≈ºliwe krawƒôdzie z drzewa do wƒôz≈Ç√≥w spoza drzewa\n",
    "        for node_in_tree in nodes_in_tree:\n",
    "            for candidate_node in range(len(sent)):\n",
    "                # Pomijamy wƒôz≈Çy ju≈º w drzewie\n",
    "                if candidate_node in nodes_in_tree:\n",
    "                    continue\n",
    "\n",
    "                # Sprawdzamy prawdopodobie≈Ñstwo krawƒôdzi\n",
    "                edge_prob = edge_probabilities[node_in_tree][candidate_node].item()\n",
    "\n",
    "                # Aktualizujemy najlepszƒÖ krawƒôd≈∫\n",
    "                if edge_prob > best_probability:\n",
    "                    best_probability = edge_prob\n",
    "                    best_edge = (node_in_tree, candidate_node)\n",
    "\n",
    "        # Dodajemy najlepszƒÖ krawƒôd≈∫ do drzewa\n",
    "        if best_edge is not None:\n",
    "            parent, child = best_edge\n",
    "            edges.append((parent, child))\n",
    "            nodes_in_tree.add(child)\n",
    "\n",
    "    # KROK 6: Utw√≥rz obiekt ParsedSentence\n",
    "    # from_edges_and_root automatycznie waliduje czy wynik to poprawne drzewo\n",
    "    return ParsedSentence.from_edges_and_root(sent.words, edges, root_index)\n",
    "\n",
    "\n",
    "# TESTOWANIE ALGORYTMU PARSOWANIA\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTOWANIE ALGORYTMU NA PRZYK≈ÅADOWYM ZDANIU\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Wybieramy przyk≈Çadowe zdanie do analizy\n",
    "    example_sentence = train_sentences[30]\n",
    "    print(f\"Analizowane zdanie: {example_sentence}\")\n",
    "    print()\n",
    "\n",
    "    # Parsujemy zdanie naszym algorytmem\n",
    "    print(\"ü§ñ PRZEWIDZIANE DRZEWO (nasz algorytm):\")\n",
    "    predicted_tree = parse_sentence(\n",
    "        example_sentence, distance_model, depth_model, tokenizer, model\n",
    "    )\n",
    "    predicted_tree.pretty_print()\n",
    "    print()\n",
    "\n",
    "    # Pokazujemy prawdziwe drzewo z danych treningowych\n",
    "    print(\"‚úÖ PRAWDZIWE DRZEWO (dane treningowe):\")\n",
    "    example_sentence.pretty_print()\n",
    "    print()\n",
    "\n",
    "    # Por√≥wnanie wynik√≥w\n",
    "    print(\"üìä POR√ìWNANIE:\")\n",
    "    print(\n",
    "        f\"Przewidziany korze≈Ñ: {predicted_tree.root} (s≈Çowo: '{example_sentence.words[predicted_tree.root]}')\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Prawdziwy korze≈Ñ: {example_sentence.root} (s≈Çowo: '{example_sentence.words[example_sentence.root]}')\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Korze≈Ñ poprawny: {'‚úÖ' if predicted_tree.root == example_sentence.root else '‚ùå'}\"\n",
    "    )\n",
    "\n",
    "    # Obliczamy dok≈Çadno≈õƒá krawƒôdzi\n",
    "    predicted_edges = set(predicted_tree.get_sorted_edges())\n",
    "    true_edges = set(example_sentence.get_sorted_edges())\n",
    "    correct_edges = len(predicted_edges & true_edges)\n",
    "    total_edges = len(true_edges)\n",
    "    edge_accuracy = correct_edges / total_edges if total_edges > 0 else 0\n",
    "\n",
    "    print(f\"Poprawne krawƒôdzie: {correct_edges}/{total_edges} ({edge_accuracy:.1%})\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxixQq__2sbf"
   },
   "source": [
    "# Ewaluacja\n",
    "Kod bardzo podobny do poni≈ºszego bƒôdzie s≈Çu≈ºy≈Ç do ewaluacji rozwiƒÖzania na zdaniach testowych. Wywo≈ÇujƒÖc poni≈ºsze kom√≥rki mo≈ºesz dowiedzieƒá siƒô ile punkt√≥w zdoby≈Çoby twoje rozwiƒÖzanie, gdyby≈õmy ocenili je na danych walidacyjnych. Przed wys≈Çaniem rozwiƒÖzania upewnij siƒô, ≈ºe ca≈Çy notebook wykonuje siƒô od poczƒÖtku do ko≈Ñca bez b≈Çƒôd√≥w i bez ingerencji u≈ºytkownika po wykonaniu polecenia `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWVwj-7m2sbf"
   },
   "outputs": [],
   "source": [
    "def points(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "\n",
    "    return scale(root_placement) + scale(uuas)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    sentences: List[ParsedSentence], distance_model, depth_model, tokenizer, model\n",
    "):\n",
    "    sum_uuas = 0\n",
    "    root_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sent in sentences:\n",
    "            parsed = parse_sentence(sent, distance_model, depth_model, tokenizer, model)\n",
    "            root_correct += int(parsed.root == sent.root)\n",
    "            sum_uuas += uuas_score(sent, parsed)\n",
    "\n",
    "    root_placement = root_correct / len(sentences)\n",
    "    uuas = sum_uuas / len(sentences)\n",
    "\n",
    "    print(f\"UUAS: {uuas * 100:.3}%\")\n",
    "    print(f\"Root placement: {root_placement * 100:.3}%\")\n",
    "    print(f\"Your score: {points(root_placement, uuas):.1}/2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjSzxdNY2sbf"
   },
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    distance_model_loaded = DistanceModel()\n",
    "    distance_model_loaded.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n",
    "\n",
    "    depth_model_loaded = DepthModel()\n",
    "    depth_model_loaded.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n",
    "\n",
    "    evaluate_model(\n",
    "        val_sentences, distance_model_loaded, depth_model_loaded, tokenizer, model\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SelfStudy OAI",
   "language": "python",
   "name": "selfstudy-oai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
