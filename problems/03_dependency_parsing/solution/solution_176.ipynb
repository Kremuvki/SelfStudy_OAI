{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu7dm0kO2sbV"
   },
   "source": [
    "# Analiza zależnościowa\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## Wstęp\n",
    "\n",
    "Język, którym posługujemy się na co dzień, funkcjonuje na zasadzie kompozycyjności. Oznacza to, że znaczenie złożonych wyrażeń językowych można wywnioskować z ich części składowych i z relacji między nimi. Ta właściwość pozwala użytkownikom języka na daleko idącą kreatywność w sposobie konstruowania wypowiedzi, przy zachowaniu precyzji komunikacji. Sposób w jaki słowa w zdaniu są ze sobą związane, tworzy strukturę ukorzenionego drzewa. Problemem, który rozważamy w tym zadaniu, jest automatyczna konstrukcja takich drzew dla zdań w języku polskim. Problem nosi nazwę analizy składniowej zdań, a konkretnie dokonywać będziemy analizy zależnościowej.\n",
    "\n",
    "Analiza składniowa jest w ogólności trudna. Na przykład, mimo że zdania `(1) Maria do jutra jest zajęta.` oraz `(2) Droga do domu jest zajęta.` zawierają kolejno te same części mowy, w dodatku o dokładnie tej samej formie gramatycznej, to w zdaniu (1) fraza \"do jutra\" modyfikuje czasownik \"jest zajęta\", natomiast w zdaniu (2) fraza \"do domu\" jest podrzędnikiem rzeczownika \"droga\". W dodatku, czasami nawet natywni użytkownicy języka mogą zinterpretować strukturę zdania na dwa różne sposoby: zdanie `Zauważyłem dziś samochód Adama, którego dawno nie widziałem.` może być interpretowane na dwa sposoby w zależności od tego, do czego odnosi się \"którego\": czy do \"samochodu Adama\", czy może do \"Adama\".\n",
    "\n",
    "Istnieje wiele różnych algorytmów rozwiązujących problem analizy zależnościowej. Klasyczne metody przetwarzają zdanie słowo po słowie, od lewej do prawej i wstawiają krawędzie w oparciu albo o pewien ustalony zbiór reguł lub o algorytm uczenia maszynowego. W tym zadaniu użyjemy innej metody. Twoim zadaniem będzie przewidzenie drzewa zależnościowego w oparciu o wektory słów otrzymane modelem HerBERT.\n",
    "\n",
    "HerBERT to polska wersja BERT, który jest modelem językowym i działa następująco:\n",
    "1. BERT posiada moduł nazywany tokenizatorem (ang. tokenizer), który dzieli zdanie na pewne podsłowa. Na przykład zdanie `Dostaję klucz i biegnę do swojego pokoju.` dzieli na `'Dosta', 'ję', 'klucz', 'i', 'bieg', 'nę', 'do', 'swojego', 'pokoju', '.'`. Tokenizator jest wyposażony w słownik, który podsłowom przypisuje unikalne liczby: w praktyce zatem otrzymujemy mało zrozumiałe dla człowieka `18577, 2779, 22816, 1009, 4775, 2788, 2041, 5058, 7217, 1899`.\n",
    "1. Następnie BERT posiada słownik, który zamienia te liczby na wektory o długości 768. Otrzymujemy zatem macierz o rozmiarach `10 x 768`.\n",
    "1. BERT posiada 12 warstw, z których każda bierze wynik poprzedniej i wykonuje na niej pewną transformację. Szczegóły nie są istotne w tym zadaniu! Ważne jest natomiast to, że cały model jest uczony automatycznie, przy użyciu dużych korpusów tekstu. Zinterpretowanie działania każdej warstwy jest niemożliwe! Natomiast być może w skomplikowanym algorytmie, którego nauczył się BERT różne warstwy pełnią różne role.\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "Twoim zadaniem będzie automatyczna analiza składniowa zdań w języku polskim. Pominiemy dokładne objaśnienie sposobu konstruowania takich drzew, możesz samemu popatrzeć na przykłady! Dostaniesz zbiór danych treningowych zawierający 1000 przykładów rozkładów zdań. W pliku `train.conll` znajdują się poetykietowane zdania, na przykład:\n",
    "\n",
    "| # | Word      | - | - | - | - | Head | - | - | - |\n",
    "|---|-----------|---|---|---|---|--------|---|---|---|\n",
    "| 1 | Wyobraź   | _ | _ | _ | _ | 0      | _ | _ | _ |\n",
    "| 2 | sobie     | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "| 3 | człowieka | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "| 4 | znajdującego | _ | _ | _ | _ | 3    | _ | _ | _ |\n",
    "| 5 | się       | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
    "| 6 | na        | _ | _ | _ | _ | 4      | _ | _ | _ |\n",
    "| 7 | ogromnej  | _ | _ | _ | _ | 8      | _ | _ | _ |\n",
    "| 8 | górze     | _ | _ | _ | _ | 6      | _ | _ | _ |\n",
    "| 9 | .         | _ | _ | _ | _ | 1      | _ | _ | _ |\n",
    "\n",
    "Co jest sposobem na zakodowanie następującego drzewa składniowego zdania złożonego:\n",
    "```\n",
    "      Wyobraź                          \n",
    "   ______|_____________                 \n",
    "  |      |         człowieka           \n",
    "  |      |             |                \n",
    "  |      |        znajdującego         \n",
    "  |      |      _______|__________      \n",
    "  |      |     |                  na   \n",
    "  |      |     |                  |     \n",
    "  |      |     |                górze  \n",
    "  |      |     |                  |     \n",
    "sobie    .    się              ogromnej\n",
    "```\n",
    "Dostarczamy Ci funkcję w Pythonie służącą do wczytania przykładów z tego pliku i na ich wizualizację. Twoje rozwiązanie powinno:\n",
    "1. Dzielić zdanie na podsłowa.\n",
    "1. Dla każdego podsłowa przypisywać wektor. Należy użyć tutaj finalnych lub pośrednich wektorów wyliczonych przez model HerBERT.\n",
    "1. Agregować wektory podsłów tak aby otrzymać wektory słów.\n",
    "1. Zaimplementować i wyuczyć prosty model przewidujący odległości w drzewie i głębokości w drzewie poszczególnych słów w zdaniu.\n",
    "1. Użyć modeli odległości i głębokości do skonstruowania drzewa składniowego.\n",
    "\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje finalne rozwiązanie będzie testowane w środowisku **bez** GPU.\n",
    "- Ewaluacja twojego rozwiązania (bez treningu) na 200 przykładach testowych powinna trwać nie dłużej niż 5 minut na Google Colab bez GPU.\n",
    "- Do dyspozycji masz model typu BERT: `allegro/herbert-base-cased` oraz tokenizer `allegro/herbert-base-cased`. Nie wolno korzystać z innych uprzednio wytrenowanych modeli oraz ze zbiorów danych innych niż dostarczony.\n",
    "- Lista dopuszczalnych bibliotek: `transformers`, `nltk`, `torch`.\n",
    "\n",
    "## Uwagi i wskazówki\n",
    "- Liczne wskazówki znajdują się we wzorcach funkcji, które powinieneś zaimplementować.\n",
    "\n",
    "## Pliki zgłoszeniowe\n",
    "Rozwiązanie zadania stanowi plik archiwum zip zawierające:\n",
    "1. Ten notebook\n",
    "2. Plik z wagami modelu odległości: `distance_model.pth`\n",
    "3. Plik z wagami modelu głębokości: `depth_model.pth`\n",
    "\n",
    "Uruchomienie całego notebooka z flagą `FINAL_EVALUATION_MODE` ustawioną na `False` powinno w maksymalnie 10 minut skutkować utworzeniem obu plików z wagami.\n",
    "\n",
    "## Ewaluacja\n",
    "Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`, a następnie zostanie uruchomiony cały notebook.\n",
    "Zaimplementowana przez Ciebie funkcja `parse_sentence`, której wzorzec znajdziesz na końcu tego notatnika, zostanie oceniona na 200 przykładach testowych.\n",
    "Ewaluacja będzie podobna do tej zaimplementowanej w funkcji `evaluate_model`.\n",
    "Pamiętaj jednak, że ostateczna funkcja do ewaluacji sprawdzała będzie dodatkowo, czy zwracane przez twoją funkcję `parse_sentence` drzewa są poprawne!\n",
    "\n",
    "Ewaluacja nie może zajmować więcej niż 3 minuty. Możesz uruchomić walidację swojego rozwiązania na dostarczonym zbiorze danych walidacyjnych na Google Colab, aby przekonać się czy nie przekraczasz czasu.\n",
    "Za pomocą skryptu `validation_script.py` będziesz mógł upewnić się, że Twoje rozwiązanie zostanie prawidłowo wykonane na naszych serwerach oceniających:\n",
    "\n",
    "```\n",
    "python3 validation_script.py --train\n",
    "python3 validation_script.py\n",
    "```\n",
    "\n",
    "Podczas sprawdzania zadania, użyjemy dwóch metryk: UUAS oraz root placement.\n",
    "1. Root placemenet oznacza ułamek przykładów na których poprawnie wskażesz korzeń drzewa składniowego,\n",
    "2. UUAS dla konkretnego zdania to ułamek poprawnie umieszczonych krawędzi. UUAS dla zbioru to średnia wyników dla poszczególnych zdań.\n",
    "\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy pomiędzy 0 i 2 punkty. Twój wynik za to zadanie zostanie wyliczony za pomocą funkcji:\n",
    "```Python\n",
    "def points(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "    return (scale(root_placement) + scale(uuas))\n",
    "```\n",
    "Innymi słowy, twój wynik jest sumą wyników za root placement i UUAS. Wynik za daną metrykę jest 0 jeśli wartość danej metryki jest poniżej 0.5 i 1 jeśli jest powyżej 0.85. Pomiędzy tymi wartościami, wynik rośnie liniowo z wartością metryki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4loni7x2sba"
   },
   "source": [
    "# Kod startowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4x3my7r2sbb"
   },
   "outputs": [],
   "source": [
    "FINAL_EVALUATION_MODE = (\n",
    "    False  # W czasie sprawdzania twojego rozwiązania, zmienimy tą wartość na True\n",
    ")\n",
    "DEPTH_MODEL_PATH = \"depth_model.pth\"  # Nie zmieniaj!\n",
    "DISTANCE_MODEL_PATH = \"distance_model.pth\"  # Nie zmieniaj!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgEWf7M82sbb"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer\n",
    "from utils import (\n",
    "    ListDataset,\n",
    "    ParsedSentence,\n",
    "    Sentence,\n",
    "    merge_subword_tokens,\n",
    "    read_conll,\n",
    "    uuas_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTGWj-tW2sbb"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KFhuQwt2sbc"
   },
   "outputs": [],
   "source": [
    "train_sentences = read_conll(\"train.conll\")  # 1000 zdań\n",
    "val_sentences = read_conll(\"valid.conll\")  # 200 zdań\n",
    "\n",
    "train_sentences[6].pretty_print()  # wyświetl drzewo jednego zdania\n",
    "print(train_sentences[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQYUDgF2sbc"
   },
   "source": [
    "# Twoje rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6g9oW4g2sbc"
   },
   "outputs": [],
   "source": [
    "def get_distances(sentence: ParsedSentence):\n",
    "    \"\"\"Znajdź odległości między każdą parą słów w zdaniu.\n",
    "    Zwraca macierz numpy o wymiarach (len(sentence), len(sentence)).\"\"\"\n",
    "    distances = np.zeros((len(sentence), len(sentence)))\n",
    "    neighbors = [[] for _ in range(len(sentence))]\n",
    "    for a, b in sentence.get_sorted_edges():\n",
    "        neighbors[a].append(b)\n",
    "        neighbors[b].append(a)\n",
    "\n",
    "    def dfs(v, vis, dist):\n",
    "        vis[v] = True\n",
    "        for neighbor in neighbors[v]:\n",
    "            if vis[neighbor]:\n",
    "                continue\n",
    "            dist[neighbor] = dist[v] + 1\n",
    "            dfs(neighbor, vis, dist)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        visited = [False for _ in range(len(sentence))]\n",
    "        visited[i] = True\n",
    "        distan = [0 for _ in range(len(sentence))]\n",
    "        dfs(i, visited, distan)\n",
    "        for idx, dist in enumerate(distan):\n",
    "            distances[i][idx] = dist\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "print(get_distances(train_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq--HUNP2sbd"
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(\n",
    "    sentences_s: List[str],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "    progress_bar: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ekstraktuje embeddingi podsłów z modelu HerBERT dla listy zdań.\n",
    "\n",
    "    DLACZEGO UŻYWAMY MODELU BERT:\n",
    "    - BERT (HerBERT w przypadku polskiego) to kontekstowy model językowy\n",
    "    - Każde słowo otrzymuje reprezentację wektorową która zależy od kontekstu\n",
    "    - Embeddingi BERT zawierają bogatą informację semantyczną i składniową\n",
    "\n",
    "    PROCES TOKENIZACJI:\n",
    "    1. BERT dzieli zdania na podsłowa (subwords) używając algorytmu WordPiece\n",
    "    2. Przykład: \"Dostaję\" → [\"Dosta\", \"ję\"]\n",
    "    3. Każdy podsłów otrzymuje unikalny ID ze słownika\n",
    "\n",
    "    ARCHITEKTURA BERT:\n",
    "    - 12 warstw transformer'a\n",
    "    - Każda warstwa przetwarza reprezentacje z poprzedniej warstwy\n",
    "    - Używamy warstwy 7 (środkowej) jako kompromis między specjalizacją a ogólnością\n",
    "\n",
    "    Args:\n",
    "        sentences_s: Lista zdań jako stringi\n",
    "        tokenizer: Tokenizator HerBERT do podziału na podsłowa\n",
    "        model: Model HerBERT do generowania embeddingów\n",
    "        progress_bar: Czy pokazywać pasek postępu (opcjonalne)\n",
    "\n",
    "    Returns:\n",
    "        tuple zawierająca:\n",
    "        - tokens: Lista tensorów z ID tokenów dla każdego zdania\n",
    "        - embeddings: Lista tensorów z embeddingami o wymiarach (seq_len, 768)\n",
    "    \"\"\"\n",
    "\n",
    "    # Wskazówki:\n",
    "    #  1. Możesz użyć funkcji:\n",
    "    #   encoded = tokenizer.batch_encode_plus(...)\n",
    "    #   with torch.no_grad():\n",
    "    #     model(**encoded, output_hidden_states=True)\n",
    "    #  2. Aby przyspieszyć obliczenia, pamiętaj o zgrupowaniu (batching) zdań, przed podaniem ich do modelu.\n",
    "    #  3. Pamiętaj, że każde zdanie może mieć inną długość, więc żeby wypełnić dodatkowe miejsce w zwracanym\n",
    "    #   tensorze, HERBERT zastosuje padding. Pamiętaj o usunięciu paddingu z wyników.\n",
    "    #  4. Tokenizator i model używa specjalnych tokenów (np. początku i końca zdania), które również powinny\n",
    "    #   zostać usunięte.\n",
    "\n",
    "    # KROK 1: Tokenizacja i kodowanie wszystkich zdań jednocześnie (batching)\n",
    "    # batch_encode_plus automatycznie:\n",
    "    # - Dodaje specjalne tokeny [CLS] na początku i [SEP] na końcu\n",
    "    # - Aplikuje padding do najdłuższego zdania w batchu\n",
    "    # - Tworzy attention_mask wskazującą prawdziwe tokeny vs padding\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        sentences_s,\n",
    "        return_tensors=\"pt\",  # Zwróć jako PyTorch tensory\n",
    "        padding=True,  # Dodaj padding do jednakowej długości\n",
    "        truncation=True,  # Obetnij za długie zdania\n",
    "    )\n",
    "\n",
    "    # KROK 2: Przepuszczenie przez model BERT\n",
    "    # torch.no_grad() wyłącza obliczanie gradientów (oszczędność pamięci)\n",
    "    # output_hidden_states=True zwraca ukryte stany ze wszystkich warstw\n",
    "    with torch.no_grad():\n",
    "        bert_output = model(**encoded, output_hidden_states=True)\n",
    "\n",
    "    # KROK 3: Ekstrakcja tokenów i embeddingów dla każdego zdania\n",
    "    tokens = []\n",
    "    embeddings = []\n",
    "\n",
    "    for sentence_idx in range(len(sentences_s)):\n",
    "        # Maska wskazująca które pozycje to prawdziwe tokeny (nie padding)\n",
    "        attention_mask = encoded[\"attention_mask\"][sentence_idx].bool()\n",
    "\n",
    "        # Ekstraktujemy prawdziwe tokeny (bez padding)\n",
    "        sentence_tokens = encoded[\"input_ids\"][sentence_idx, attention_mask]\n",
    "\n",
    "        # Usuwamy specjalne tokeny [CLS] (pierwszy) i [SEP] (ostatni)\n",
    "        sentence_tokens_clean = sentence_tokens[1:-1]\n",
    "        tokens.append(sentence_tokens_clean)\n",
    "\n",
    "        # Ekstraktujemy embeddingi z warstwy 7 (indeksowane od 0)\n",
    "        # hidden_states[0] = embeddingi wejściowe\n",
    "        # hidden_states[1-12] = warstwy transformer'a\n",
    "        # Wybieramy warstwę 7 jako dobry kompromis między ogólnością a specjalizacją\n",
    "        layer_7_embeddings = bert_output.hidden_states[7][sentence_idx]\n",
    "\n",
    "        # Usuwamy embeddingi dla padding i specjalnych tokenów\n",
    "        sentence_embeddings = layer_7_embeddings[attention_mask][1:-1]\n",
    "        embeddings.append(sentence_embeddings)\n",
    "\n",
    "    return tokens, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq76vhja2sbd"
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(\n",
    "    sentences: List[Sentence], tokenizer, model\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Konwertuje embeddingi podsłów na embeddingi pełnych słów poprzez agregację.\n",
    "\n",
    "    PROBLEM DO ROZWIĄZANIA:\n",
    "    - BERT tokenizuje słowa na podsłowa (np. \"programowanie\" → [\"program\", \"owanie\"])\n",
    "    - Potrzebujemy jednego embeddingu na słowo, nie na podsłowo\n",
    "    - Rozwiązanie: agregujemy embeddingi podsłów należących do tego samego słowa\n",
    "\n",
    "    STRATEGIA AGREGACJI:\n",
    "    - Używamy średniej arytmetycznej embeddingów podsłów\n",
    "    - Alternatywy: suma, pierwsza pozycja, ostatnia pozycja\n",
    "    - Średnia zachowuje informację ze wszystkich podsłów\n",
    "\n",
    "    PROCES:\n",
    "    1. Otrzymujemy embeddingi podsłów z get_bert_embeddings\n",
    "    2. Grupujemy podsłowa należące do tego samego słowa\n",
    "    3. Obliczamy średnią embeddingów w każdej grupie\n",
    "    4. Zwracamy embedding na poziomie słów\n",
    "\n",
    "    Args:\n",
    "        sentences: Lista obiektów Sentence zawierających słowa do przetworzenia\n",
    "        tokenizer: Tokenizator HerBERT\n",
    "        model: Model HerBERT\n",
    "\n",
    "    Returns:\n",
    "        Lista tensorów, każdy tensor zawiera embeddingi słów dla jednego zdania\n",
    "        Wymiary: [num_words, embedding_dim] dla każdego zdania\n",
    "    \"\"\"\n",
    "\n",
    "    # KROK 1: Konwertujemy obiekty Sentence na stringi\n",
    "    # Potrzebne do przekazania do get_bert_embeddings\n",
    "    sentence_strings = [str(sentence) for sentence in sentences]\n",
    "\n",
    "    # KROK 2: Definiujemy funkcję agregacji\n",
    "    # Używamy średniej arytmetycznej - najczęściej stosowana metoda\n",
    "    def aggregation_function(subword_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Agreguje embeddingi podsłów do jednego embeddingu słowa.\n",
    "\n",
    "        Args:\n",
    "            subword_embeddings: Tensor o wymiarach [num_subwords, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor o wymiarach [embedding_dim] - embedding słowa\n",
    "        \"\"\"\n",
    "        return torch.mean(subword_embeddings, dim=0)\n",
    "\n",
    "    # KROK 3: Otrzymujemy embeddingi podsłów\n",
    "    subword_tokens, subword_embeddings = get_bert_embeddings(\n",
    "        sentence_strings, tokenizer, model\n",
    "    )\n",
    "\n",
    "    # KROK 4: Agregujemy podsłowa do słów dla każdego zdania\n",
    "    word_embeddings = []\n",
    "    for sentence, tokens, embeddings in zip(\n",
    "        sentences, subword_tokens, subword_embeddings\n",
    "    ):\n",
    "        # merge_subword_tokens to pomocnicza funkcja która:\n",
    "        # - Mapuje tokeny podsłów z powrotem na oryginalne słowa\n",
    "        # - Stosuje funkcję agregacji do podsłów należących do tego samego słowa\n",
    "        sentence_word_embeddings = merge_subword_tokens(\n",
    "            words=sentence.words,  # Lista oryginalnych słów\n",
    "            subword_tokens=tokens,  # Lista tokenów podsłów\n",
    "            subword_embeddings=embeddings,  # Embeddingi podsłów\n",
    "            tokenizer=tokenizer,  # Tokenizator do dekodowania\n",
    "            aggregation_fn=aggregation_function,  # Funkcja agregacji\n",
    "            verbose=False,  # Bez logowania szczegółów\n",
    "        )\n",
    "        word_embeddings.append(sentence_word_embeddings)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF486oQ02sbd"
   },
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    sentences: List[ParsedSentence], tokenizer, model\n",
    ") -> tuple[ListDataset, ListDataset]:\n",
    "    \"\"\"\n",
    "    Przygotowuje zbiory danych do trenowania modeli odległości i głębokości.\n",
    "\n",
    "    DLACZEGO POTRZEBUJEMY DWÓCH MODELI:\n",
    "    1. Model odległości - przewiduje czy dwa słowa są połączone krawędzią\n",
    "    2. Model głębokości - przewiduje które słowo jest korzeniem drzewa\n",
    "\n",
    "    ARCHITEKTURA TRENINGU:\n",
    "    - Oba modele używają tych samych embeddingów słów jako wejście\n",
    "    - Model odległości: input = para embeddingów, output = prawdopodobieństwo krawędzi\n",
    "    - Model głębokości: input = embedding słowa, output = prawdopodobieństwo korzenia\n",
    "\n",
    "    PRZYGOTOWANIE DANYCH:\n",
    "    1. Obliczamy embeddingi słów używając HerBERT\n",
    "    2. Obliczamy prawdziwe odległości w drzewie zależnościowym\n",
    "    3. Wyznaczamy głębokości (odległość od korzenia)\n",
    "    4. Pakujemy dane w formę gotową do treningu\n",
    "\n",
    "    Args:\n",
    "        sentences: Lista ParsedSentence z prawdziwymi drzewami zależnościowymi\n",
    "        tokenizer: Tokenizator HerBERT\n",
    "        model: Model HerBERT\n",
    "\n",
    "    Returns:\n",
    "        tuple zawierająca:\n",
    "        - dataset_dist: Dataset do treningu modelu odległości\n",
    "        - dataset_depth: Dataset do treningu modelu głębokości\n",
    "    \"\"\"\n",
    "\n",
    "    # KROK 1: Obliczamy embeddingi słów dla wszystkich zdań\n",
    "    # To jest najbardziej czasochłonna operacja, więc robimy ją raz\n",
    "    print(\"Obliczam embeddingi słów...\")\n",
    "    word_embeddings = get_word_embeddings(sentences, tokenizer, model)\n",
    "\n",
    "    # KROK 2: Obliczamy macierze odległości dla każdego zdania\n",
    "    # Odległość = liczba krawędzi między dwoma słowami w drzewie\n",
    "    print(\"Obliczam macierze odległości...\")\n",
    "    distance_matrices = []\n",
    "    for sentence in sentences:\n",
    "        distance_matrix = get_distances(sentence)\n",
    "        distance_matrices.append(distance_matrix)\n",
    "\n",
    "    # KROK 3: Obliczamy głębokości słów (odległość od korzenia)\n",
    "    # Głębokość = odległość od korzenia drzewa do danego słowa\n",
    "    print(\"Obliczam głębokości słów...\")\n",
    "    depth_vectors = []\n",
    "    for distance_matrix, sentence in zip(distance_matrices, sentences):\n",
    "        # Głębokość każdego słowa = odległość od korzenia\n",
    "        root_index = sentence.root\n",
    "        depths = distance_matrix[root_index]  # Wiersz odpowiadający korzeniowi\n",
    "\n",
    "        # Dodajemy wymiar [1] żeby otrzymać tensor 2D: [num_words, 1]\n",
    "        depths = depths[..., None]  # Przekształca [n] na [n, 1]\n",
    "        depth_vectors.append(depths)\n",
    "\n",
    "    # KROK 4: Tworzymy datasety\n",
    "    # Każdy element datasetu to krotka: (embeddings, targets, sentence)\n",
    "\n",
    "    # Dataset dla modelu odległości\n",
    "    # Targets = macierz odległości między wszystkimi parami słów\n",
    "    distance_data = list(zip(word_embeddings, distance_matrices, sentences))\n",
    "    dataset_dist = ListDataset(distance_data)\n",
    "\n",
    "    # Dataset dla modelu głębokości\n",
    "    # Targets = wektor głębokości dla każdego słowa\n",
    "    depth_data = list(zip(word_embeddings, depth_vectors, sentences))\n",
    "    dataset_depth = ListDataset(depth_data)\n",
    "\n",
    "    print(f\"Utworzono datasety: {len(dataset_dist)} przykładów\")\n",
    "    return dataset_dist, dataset_depth\n",
    "\n",
    "\n",
    "# Tworzenie zbiorów treningowych i walidacyjnych\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"Przygotowuję zbiory treningowe...\")\n",
    "    trainset_dist, trainset_depth = get_datasets(train_sentences, tokenizer, model)\n",
    "\n",
    "    print(\"Przygotowuję zbiory walidacyjne...\")\n",
    "    valset_dist, valset_depth = get_datasets(val_sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mH7K2BOo2sbe"
   },
   "outputs": [],
   "source": [
    "def pad_arrays(sequence: List[np.ndarray], pad_with: float = np.inf) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dopełnia tablice o różnych rozmiarach do jednakowych wymiarów.\n",
    "\n",
    "    PROBLEM BATCH'OWANIA:\n",
    "    - Zdania mają różne długości (różną liczbę słów)\n",
    "    - PyTorch wymaga tensorów o jednakowych wymiarach w batch'u\n",
    "    - Rozwiązanie: padding - dopełnianie krótszych sekwencji\n",
    "\n",
    "    ALGORYTM:\n",
    "    1. Znajdź maksymalne wymiary we wszystkich tablicach\n",
    "    2. Dopełnij każdą tablicę do maksymalnych wymiarów\n",
    "    3. Użyj specjalnej wartości (pad_with) do oznaczenia dopełnienia\n",
    "\n",
    "    DLACZEGO np.inf:\n",
    "    - Umożliwia łatwe tworzenie masek (prawdziwe dane != inf)\n",
    "    - Nie wpływa na obliczenia gdy używamy mask\n",
    "\n",
    "    Args:\n",
    "        sequence: Lista tablic numpy o potencjalnie różnych rozmiarach\n",
    "        pad_with: Wartość używana do dopełnienia (domyślnie np.inf)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor z dopełnionymi danymi\n",
    "    \"\"\"\n",
    "    # KROK 1: Znajdź maksymalne wymiary\n",
    "    # Pobieramy kształty wszystkich tablic w sekwencji\n",
    "    shapes = np.array([list(array.shape) for array in sequence])\n",
    "    max_dimensions = list(shapes.max(axis=0))  # Maksimum dla każdego wymiaru\n",
    "\n",
    "    # KROK 2: Dopełnij każdą tablicę do maksymalnych wymiarów\n",
    "    padded_arrays = []\n",
    "    for array in sequence:\n",
    "        # Oblicz ile dopełnienia potrzeba dla każdego wymiaru\n",
    "        padding_amounts = []\n",
    "        for dim_idx in range(array.ndim):\n",
    "            current_size = array.shape[dim_idx]\n",
    "            max_size = max_dimensions[dim_idx]\n",
    "            padding_needed = max_size - current_size\n",
    "            # np.pad przyjmuje padding jako (przed, po) dla każdego wymiaru\n",
    "            padding_amounts.append((0, padding_needed))\n",
    "\n",
    "        # Zastosuj padding\n",
    "        padded_array = np.pad(\n",
    "            array, tuple(padding_amounts), mode=\"constant\", constant_values=pad_with\n",
    "        )\n",
    "        padded_arrays.append(padded_array)\n",
    "\n",
    "    # KROK 3: Konwertuj na tensor PyTorch\n",
    "    return torch.tensor(padded_arrays, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: List[tuple],\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple]:\n",
    "    \"\"\"\n",
    "    Funkcja kolizji (collate function) dla DataLoader'a.\n",
    "\n",
    "    ROLA FUNKCJI COLLATE:\n",
    "    - DataLoader pobiera pojedyncze przykłady z datasetu\n",
    "    - collate_fn łączy je w batch (paczkę) do efektywnego przetwarzania\n",
    "    - Musi rozwiązać problem różnych rozmiarów tensorów\n",
    "\n",
    "    PROCES:\n",
    "    1. Rozpakuj batch na składniki (embeddingi, targety, zdania)\n",
    "    2. Zastosuj padding do embeddingów i targetów\n",
    "    3. Utwórz maskę wskazującą prawdziwe dane (vs padding)\n",
    "    4. Zwróć wszystko jako tensory gotowe do treningu\n",
    "\n",
    "    Args:\n",
    "        batch: Lista krotek (embeddings, targets, sentences) z datasetu\n",
    "\n",
    "    Returns:\n",
    "        tuple zawierająca:\n",
    "        - padded_embeddings: Tensor embeddingów z paddingiem\n",
    "        - padded_targets: Tensor targetów z paddingiem\n",
    "        - mask: Maska prawdziwych danych (True) vs padding (False)\n",
    "        - sentences: Oryginalne zdania (do debugowania)\n",
    "    \"\"\"\n",
    "    # KROK 1: Rozpakuj batch na składniki\n",
    "    embeddings, targets, sentences = zip(*batch)\n",
    "\n",
    "    # KROK 2: Zastosuj padding\n",
    "    # Embeddingi dopełniamy zerami (neutralne dla sieci neuronowej)\n",
    "    padded_embeddings = pad_arrays(embeddings, pad_with=0.0)\n",
    "\n",
    "    # Targety dopełniamy nieskończonością (łatwe do maskowania)\n",
    "    padded_targets = pad_arrays(targets, pad_with=np.inf)\n",
    "\n",
    "    # KROK 3: Utwórz maskę prawdziwych danych\n",
    "    # mask[i,j] = True jeśli pozycja (i,j) zawiera prawdziwe dane\n",
    "    # mask[i,j] = False jeśli pozycja (i,j) to padding\n",
    "    mask = padded_targets != torch.inf\n",
    "\n",
    "    return padded_embeddings, padded_targets, mask, sentences\n",
    "\n",
    "\n",
    "# Tworzenie DataLoader'ów - obiektów odpowiedzialnych za batch'owanie danych\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    # KONFIGURACJA DATALOADER'ÓW:\n",
    "    # - batch_size=32: Kompromis między wydajnością a zużyciem pamięci\n",
    "    # - shuffle=True dla treningu: Zapobiega overfittingowi, poprawia generalizację\n",
    "    # - shuffle=False dla walidacji: Deterministyczne wyniki, łatwiejsze debugowanie\n",
    "    # - collate_fn: Nasza funkcja łącząca przykłady w batche\n",
    "\n",
    "    # DataLoader'y dla modelu odległości\n",
    "    dist_trainloader = DataLoader(\n",
    "        trainset_dist,\n",
    "        batch_size=32,\n",
    "        shuffle=True,  # Mieszanie dla lepszego treningu\n",
    "        collate_fn=collate_fn,  # Nasza funkcja paddingu\n",
    "    )\n",
    "    dist_valloader = DataLoader(\n",
    "        valset_dist,\n",
    "        batch_size=32,\n",
    "        shuffle=False,  # Bez mieszania dla walidacji\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    # DataLoader'y dla modelu głębokości\n",
    "    depth_trainloader = DataLoader(\n",
    "        trainset_depth, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    depth_valloader = DataLoader(\n",
    "        valset_depth, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "# DOKUMENTACJA FORMATÓW DANYCH:\n",
    "#\n",
    "# DataLoader'y zwracają krotki w formacie:\n",
    "# (padded_embeddings, padded_targets, mask, sentences)\n",
    "#\n",
    "# Dla modelu odległości (dist_*loader):\n",
    "# - embeddings.shape: (batch_size, max_seq_len, 768)  # Embeddingi słów\n",
    "# - distances.shape: (batch_size, max_seq_len, max_seq_len)  # Macierz odległości\n",
    "# - mask.shape: (batch_size, max_seq_len, max_seq_len)  # Maska prawdziwych danych\n",
    "#\n",
    "# Dla modelu głębokości (depth_*loader):\n",
    "# - embeddings.shape: (batch_size, max_seq_len, 768)  # Embeddingi słów\n",
    "# - depths.shape: (batch_size, max_seq_len, 1)  # Głębokości słów\n",
    "# - mask.shape: (batch_size, max_seq_len, 1)  # Maska prawdziwych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXMfvteE2sbe"
   },
   "outputs": [],
   "source": [
    "class DistanceModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model przewidujący prawdopodobieństwo krawędzi między parami słów.\n",
    "\n",
    "    ARCHITEKTURA:\n",
    "    - Wejście: Konkatenacja embeddingów dwóch słów (768*2 = 1536 wymiarów)\n",
    "    - Warstwa ukryta: 256 neuronów z aktywacją LeakyReLU\n",
    "    - Dropout: 30% dla regularyzacji (zapobieganie overfittingowi)\n",
    "    - Wyjście: 1 neuron z sigmoidą → prawdopodobieństwo krawędzi\n",
    "\n",
    "    DLACZEGO TAKA ARCHITEKTURA:\n",
    "    - LeakyReLU: Lepsze niż ReLU dla gradient flow\n",
    "    - Dropout: Zapobiega overfittingowi na małym zbiorze danych\n",
    "    - Sigmoid: Wyjście w zakresie [0,1] jako prawdopodobieństwo\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ARCHITEKTURA SIECI:\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # Warstwa wejściowa: 768*2 (dwa embeddingi) → 256\n",
    "            torch.nn.Linear(768 * 2, 256),\n",
    "            # Aktywacja: LeakyReLU pozwala na małe gradienty dla ujemnych wartości\n",
    "            torch.nn.LeakyReLU(negative_slope=0.01),\n",
    "            # Regularyzacja: Dropout losowo wyłącza 30% neuronów podczas treningu\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            # Warstwa wyjściowa: 256 → 1 (prawdopodobieństwo krawędzi)\n",
    "            torch.nn.Linear(256, 1),\n",
    "            # Sigmoid: mapuje dowolną liczbę rzeczywistą na zakres [0,1]\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Propagacja w przód przez sieć.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor o wymiarach [batch_size, 768*2] - para embeddingów\n",
    "\n",
    "        Returns:\n",
    "            Tensor o wymiarach [batch_size, 1] - prawdopodobieństwo krawędzi\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DepthModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model przewidujący prawdopodobieństwo że słowo jest korzeniem drzewa.\n",
    "\n",
    "    ARCHITEKTURA:\n",
    "    - Wejście: Embedding pojedynczego słowa (768 wymiarów)\n",
    "    - Warstwa ukryta: 256 neuronów z aktywacją LeakyReLU\n",
    "    - Dropout: 30% dla regularyzacji\n",
    "    - Wyjście: 1 neuron z sigmoidą → prawdopodobieństwo bycia korzeniem\n",
    "\n",
    "    ZADANIE MODELU:\n",
    "    - Głębokość 0 = korzeń drzewa\n",
    "    - Głębokość > 0 = nie korzeń\n",
    "    - Model przewiduje P(głębokość = 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ARCHITEKTURA SIECI:\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # Warstwa wejściowa: 768 (jeden embedding) → 256\n",
    "            torch.nn.Linear(768, 256),\n",
    "            # Aktywacja: LeakyReLU\n",
    "            torch.nn.LeakyReLU(negative_slope=0.01),\n",
    "            # Regularyzacja: Dropout 30%\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            # Warstwa wyjściowa: 256 → 1 (prawdopodobieństwo korzenia)\n",
    "            torch.nn.Linear(256, 1),\n",
    "            # Sigmoid: prawdopodobieństwo w zakresie [0,1]\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Propagacja w przód przez sieć.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor o wymiarach [batch_size, 768] - embeddingi słów\n",
    "\n",
    "        Returns:\n",
    "            Tensor o wymiarach [batch_size, 1] - prawdopodobieństwo korzenia\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFZKmWLm2sbe"
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask): ...\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, valloader, epochs, lr):\n",
    "    \"\"\"Pętla ucząca twoich modeli.\"\"\"\n",
    "    # TODO: implement me\n",
    "    l = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if isinstance(model, DepthModel):\n",
    "            loss_sum = 0\n",
    "            correct = 0\n",
    "            size = 0\n",
    "            for input, label, mask, sentences in dataloader:\n",
    "                zero = input[(mask & (label == 0)).squeeze()]\n",
    "                not_zero = input[(mask & (label != 0)).squeeze()]\n",
    "\n",
    "                selected = not_zero[torch.randint(not_zero.shape[0], (zero.shape[0],))]\n",
    "                batch = torch.cat((zero, selected))\n",
    "                labels = torch.cat(\n",
    "                    (torch.zeros(zero.shape[0]), torch.ones(selected.shape[0]))\n",
    "                )\n",
    "\n",
    "                # print(labels)\n",
    "                output = model(batch).squeeze()\n",
    "                loss = l(output, labels.float())\n",
    "                with torch.no_grad():\n",
    "                    correct += torch.sum(labels == torch.round(output))\n",
    "                    loss_sum += loss * batch.shape[0]\n",
    "                    size += batch.shape[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            val_correct = 0\n",
    "            val_sum = 0\n",
    "            with torch.no_grad():\n",
    "                for input, label, mask, sentences in valloader:\n",
    "                    val_sum += torch.sum(mask)\n",
    "                    val_correct += torch.sum(\n",
    "                        torch.round(model(input[mask.squeeze()]))\n",
    "                        == torch.where(label[mask.squeeze()] == 0, 0, 1)\n",
    "                    )\n",
    "            print(\n",
    "                f\"epoch: {epoch} loss:{loss_sum/size} accuracy:{correct/size} accuracy_val: {val_correct/val_sum}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            loss_sum = 0\n",
    "            correct = 0\n",
    "            size = 0\n",
    "            for inputs, label, mask, sentences in dataloader:\n",
    "                n, m, d = inputs.shape\n",
    "                # Repeat and permute the tensor to get all pairwise combinations\n",
    "                tensor_i = inputs.unsqueeze(2).expand(n, m, m, d)  # Shape: (n, m, m, d)\n",
    "                tensor_j = inputs.unsqueeze(1).expand(n, m, m, d)  # Shape: (n, m, m, d)\n",
    "\n",
    "                # Concatenate along the last dimension\n",
    "                pairwise_concat = torch.cat((tensor_i, tensor_j), dim=-1)\n",
    "                input_processed = pairwise_concat[mask]\n",
    "                targets = np.where(label[mask] == 1, 1, 0)\n",
    "\n",
    "                edges = input_processed[label[mask] == 1]\n",
    "                notedges = input_processed[label[mask] != 1]\n",
    "\n",
    "                notedges_in = notedges[\n",
    "                    torch.randint(notedges.shape[0], (edges.shape[0],))\n",
    "                ]\n",
    "                inputs_final = torch.cat((edges, notedges))\n",
    "                labels_final = torch.cat(\n",
    "                    (torch.ones(edges.shape[0]), torch.zeros(notedges.shape[0]))\n",
    "                )\n",
    "\n",
    "                output = model(inputs_final).squeeze()\n",
    "                loss = l(output, labels_final.float())\n",
    "                with torch.no_grad():\n",
    "                    correct += torch.sum(labels_final == torch.round(output))\n",
    "                    loss_sum += loss * inputs_final.shape[0]\n",
    "                    size += inputs_final.shape[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            print(f\"epoch: {epoch} loss:{loss_sum/size} accuracy:{correct/size}\")\n",
    "\n",
    "\n",
    "# W czasie ewaluacji, modele nie powinny być ponownie trenowane.\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"Training depth model\")\n",
    "    depth_model = DepthModel()\n",
    "    # TODO: ustaw hiperparametry\n",
    "    train_model(depth_model, depth_trainloader, depth_valloader, lr=0.0001, epochs=15)\n",
    "    # zapisz wagi modelu do pliku\n",
    "    torch.save(depth_model.state_dict(), DEPTH_MODEL_PATH)\n",
    "\n",
    "    print(\"Training distance model\")\n",
    "    distance_model = DistanceModel()\n",
    "    # # TODO: ustaw hiperparametry\n",
    "    train_model(distance_model, dist_trainloader, dist_valloader, lr=0.001, epochs=15)\n",
    "    # # zapisz wagi modelu do pliku\n",
    "    torch.save(distance_model.state_dict(), DISTANCE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TCZC-Kw2sbe"
   },
   "outputs": [],
   "source": [
    "def parse_sentence(\n",
    "    sent: Sentence, distance_model, depth_model, tokenizer, model\n",
    ") -> ParsedSentence:\n",
    "    \"\"\"\n",
    "    Buduje drzewo składniowe dla pojedynczego zdania używając wytrenowanych modeli.\n",
    "\n",
    "    ALGORYTM PARSOWANIA:\n",
    "    1. Oblicz embeddingi słów używając HerBERT\n",
    "    2. Wybierz korzeń używając modelu głębokości\n",
    "    3. Oblicz prawdopodobieństwa krawędzi używając modelu odległości\n",
    "    4. Zbuduj drzewo algorytmem zachłannym (greedy tree construction)\n",
    "\n",
    "    STRATEGIA BUDOWY DRZEWA:\n",
    "    - Zaczynamy od korzenia (wybrane przez model głębokości)\n",
    "    - Iteracyjnie dodajemy najbardziej prawdopodobne krawędzie\n",
    "    - Zapewniamy że wynik to drzewo (dokładnie n-1 krawędzi, połączony)\n",
    "\n",
    "    DLACZEGO TEN ALGORYTM:\n",
    "    - Prosty i deterministyczny\n",
    "    - Gwarantuje poprawne drzewo\n",
    "    - Wykorzystuje przewidywania obu modeli\n",
    "\n",
    "    Args:\n",
    "        sent: Zdanie do sparsowania\n",
    "        distance_model: Wytrenowany model przewidujący krawędzie\n",
    "        depth_model: Wytrenowany model przewidujący korzeń\n",
    "        tokenizer: Tokenizator HerBERT\n",
    "        model: Model HerBERT\n",
    "\n",
    "    Returns:\n",
    "        ParsedSentence: Zdanie z przewidzianym drzewem składniowym\n",
    "    \"\"\"\n",
    "\n",
    "    # KROK 1: Oblicz embeddingi słów dla zdania\n",
    "    # Funkcja get_word_embeddings zwraca listę, więc bierzemy pierwszy (i jedyny) element\n",
    "    word_embeddings = get_word_embeddings([sent], tokenizer, model)[0]\n",
    "\n",
    "    # Dodajemy wymiar batch (unsqueeze(0)) żeby dopasować format oczekiwany przez modele\n",
    "    # Wymiary: [1, num_words, 768]\n",
    "    embeddings_batch = word_embeddings.unsqueeze(0)\n",
    "\n",
    "    # KROK 2: Przygotuj dane dla modelu odległości\n",
    "    # Tworzymy wszystkie możliwe pary embeddingów słów\n",
    "    batch_size, num_words, embedding_dim = embeddings_batch.shape\n",
    "\n",
    "    # Rozszerzamy tensor żeby otrzymać wszystkie pary (i,j)\n",
    "    # tensor_i[b,i,j,:] = embedding słowa i w zdaniu b\n",
    "    tensor_i = embeddings_batch.unsqueeze(2).expand(\n",
    "        batch_size, num_words, num_words, embedding_dim\n",
    "    )\n",
    "    # tensor_j[b,i,j,:] = embedding słowa j w zdaniu b\n",
    "    tensor_j = embeddings_batch.unsqueeze(1).expand(\n",
    "        batch_size, num_words, num_words, embedding_dim\n",
    "    )\n",
    "\n",
    "    # Konkatenujemy embeddingi par słów\n",
    "    # pairwise_concat[b,i,j,:] = [embedding_i, embedding_j]\n",
    "    pairwise_concat = torch.cat((tensor_i, tensor_j), dim=-1)\n",
    "\n",
    "    # KROK 3: Przewidywania modeli\n",
    "    with torch.no_grad():  # Wyłączamy gradient tracking dla inferencji\n",
    "        # Model odległości: prawdopodobieństwa krawędzi dla wszystkich par\n",
    "        edge_probabilities = distance_model(pairwise_concat)[\n",
    "            0\n",
    "        ]  # [num_words, num_words]\n",
    "\n",
    "        # Model głębokości: prawdopodobieństwa korzenia dla każdego słowa\n",
    "        root_probabilities = depth_model(embeddings_batch)[0]  # [num_words, 1]\n",
    "\n",
    "    # KROK 4: Wybierz korzeń drzewa\n",
    "    # Słowo z najwyższym prawdopodobieństwem bycia korzeniem\n",
    "    root_index = torch.argmax(root_probabilities.squeeze()).item()\n",
    "\n",
    "    # KROK 5: Buduj drzewo algorytmem zachłannym\n",
    "    # Zaczynamy z korzeniem w drzewie\n",
    "    nodes_in_tree = {root_index}\n",
    "    edges = []\n",
    "\n",
    "    # Dodajemy dokładnie n-1 krawędzi (właściwość drzewa)\n",
    "    for _ in range(len(sent) - 1):\n",
    "        best_edge = None\n",
    "        best_probability = -np.inf\n",
    "\n",
    "        # Sprawdzamy wszystkie możliwe krawędzie z drzewa do węzłów spoza drzewa\n",
    "        for node_in_tree in nodes_in_tree:\n",
    "            for candidate_node in range(len(sent)):\n",
    "                # Pomijamy węzły już w drzewie\n",
    "                if candidate_node in nodes_in_tree:\n",
    "                    continue\n",
    "\n",
    "                # Sprawdzamy prawdopodobieństwo krawędzi\n",
    "                edge_prob = edge_probabilities[node_in_tree][candidate_node].item()\n",
    "\n",
    "                # Aktualizujemy najlepszą krawędź\n",
    "                if edge_prob > best_probability:\n",
    "                    best_probability = edge_prob\n",
    "                    best_edge = (node_in_tree, candidate_node)\n",
    "\n",
    "        # Dodajemy najlepszą krawędź do drzewa\n",
    "        if best_edge is not None:\n",
    "            parent, child = best_edge\n",
    "            edges.append((parent, child))\n",
    "            nodes_in_tree.add(child)\n",
    "\n",
    "    # KROK 6: Utwórz obiekt ParsedSentence\n",
    "    # from_edges_and_root automatycznie waliduje czy wynik to poprawne drzewo\n",
    "    return ParsedSentence.from_edges_and_root(sent.words, edges, root_index)\n",
    "\n",
    "\n",
    "# TESTOWANIE ALGORYTMU PARSOWANIA\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTOWANIE ALGORYTMU NA PRZYKŁADOWYM ZDANIU\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Wybieramy przykładowe zdanie do analizy\n",
    "    example_sentence = train_sentences[30]\n",
    "    print(f\"Analizowane zdanie: {example_sentence}\")\n",
    "    print()\n",
    "\n",
    "    # Parsujemy zdanie naszym algorytmem\n",
    "    print(\"🤖 PRZEWIDZIANE DRZEWO (nasz algorytm):\")\n",
    "    predicted_tree = parse_sentence(\n",
    "        example_sentence, distance_model, depth_model, tokenizer, model\n",
    "    )\n",
    "    predicted_tree.pretty_print()\n",
    "    print()\n",
    "\n",
    "    # Pokazujemy prawdziwe drzewo z danych treningowych\n",
    "    print(\"✅ PRAWDZIWE DRZEWO (dane treningowe):\")\n",
    "    example_sentence.pretty_print()\n",
    "    print()\n",
    "\n",
    "    # Porównanie wyników\n",
    "    print(\"📊 PORÓWNANIE:\")\n",
    "    print(\n",
    "        f\"Przewidziany korzeń: {predicted_tree.root} (słowo: '{example_sentence.words[predicted_tree.root]}')\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Prawdziwy korzeń: {example_sentence.root} (słowo: '{example_sentence.words[example_sentence.root]}')\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Korzeń poprawny: {'✅' if predicted_tree.root == example_sentence.root else '❌'}\"\n",
    "    )\n",
    "\n",
    "    # Obliczamy dokładność krawędzi\n",
    "    predicted_edges = set(predicted_tree.get_sorted_edges())\n",
    "    true_edges = set(example_sentence.get_sorted_edges())\n",
    "    correct_edges = len(predicted_edges & true_edges)\n",
    "    total_edges = len(true_edges)\n",
    "    edge_accuracy = correct_edges / total_edges if total_edges > 0 else 0\n",
    "\n",
    "    print(f\"Poprawne krawędzie: {correct_edges}/{total_edges} ({edge_accuracy:.1%})\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxixQq__2sbf"
   },
   "source": [
    "# Ewaluacja\n",
    "Kod bardzo podobny do poniższego będzie służył do ewaluacji rozwiązania na zdaniach testowych. Wywołując poniższe komórki możesz dowiedzieć się ile punktów zdobyłoby twoje rozwiązanie, gdybyśmy ocenili je na danych walidacyjnych. Przed wysłaniem rozwiązania upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez ingerencji użytkownika po wykonaniu polecenia `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWVwj-7m2sbf"
   },
   "outputs": [],
   "source": [
    "def points(root_placement, uuas):\n",
    "    def scale(x, lower=0.5, upper=0.85):\n",
    "        scaled = min(max(x, lower), upper)\n",
    "        return (scaled - lower) / (upper - lower)\n",
    "\n",
    "    return scale(root_placement) + scale(uuas)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    sentences: List[ParsedSentence], distance_model, depth_model, tokenizer, model\n",
    "):\n",
    "    sum_uuas = 0\n",
    "    root_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sent in sentences:\n",
    "            parsed = parse_sentence(sent, distance_model, depth_model, tokenizer, model)\n",
    "            root_correct += int(parsed.root == sent.root)\n",
    "            sum_uuas += uuas_score(sent, parsed)\n",
    "\n",
    "    root_placement = root_correct / len(sentences)\n",
    "    uuas = sum_uuas / len(sentences)\n",
    "\n",
    "    print(f\"UUAS: {uuas * 100:.3}%\")\n",
    "    print(f\"Root placement: {root_placement * 100:.3}%\")\n",
    "    print(f\"Your score: {points(root_placement, uuas):.1}/2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjSzxdNY2sbf"
   },
   "outputs": [],
   "source": [
    "if not FINAL_EVALUATION_MODE:\n",
    "    distance_model_loaded = DistanceModel()\n",
    "    distance_model_loaded.load_state_dict(torch.load(DISTANCE_MODEL_PATH))\n",
    "\n",
    "    depth_model_loaded = DepthModel()\n",
    "    depth_model_loaded.load_state_dict(torch.load(DEPTH_MODEL_PATH))\n",
    "\n",
    "    evaluate_model(\n",
    "        val_sentences, distance_model_loaded, depth_model_loaded, tokenizer, model\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SelfStudy OAI",
   "language": "python",
   "name": "selfstudy-oai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
