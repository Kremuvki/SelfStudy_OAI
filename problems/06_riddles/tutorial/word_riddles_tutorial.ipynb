{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Tutorial: Solving Word Riddles with Semantic Similarity\n",
    "\n",
    "![Word Riddles](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Question_mark_%28black%29.svg/200px-Question_mark_%28black%29.svg.png)\n",
    "\n",
    "## Welcome to the Fascinating World of Word Riddles! üîç\n",
    "\n",
    "In this comprehensive tutorial, you'll learn:\n",
    "- üß© What are word riddles and how humans solve them\n",
    "- üìä Word embeddings and semantic similarity\n",
    "- üéØ Clustering algorithms for grouping similar words\n",
    "- üî¢ TF-IDF and inverse document frequency\n",
    "- ü§ñ Building an intelligent riddle-solving system\n",
    "- üíª Hands-on implementation with real Polish riddles\n",
    "- üé® Optimization techniques for better performance\n",
    "- üß™ Interactive exercises to build your skills\n",
    "\n",
    "By the end, you'll be ready to implement a sophisticated word riddle solver using semantic similarity and clustering!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1. [üéì Understanding Word Riddles](#1--understanding-word-riddles)\n",
    "2. [üìä Word Embeddings and Semantic Similarity](#2--word-embeddings-and-semantic-similarity)\n",
    "3. [üîß Setting Up the Environment](#3--setting-up-the-environment)\n",
    "4. [üóÇÔ∏è Working with Polish Language Data](#4--working-with-polish-language-data)\n",
    "5. [üéØ Clustering Words by Meaning](#5--clustering-words-by-meaning)\n",
    "6. [üìà TF-IDF and Word Importance](#6--tf-idf-and-word-importance)\n",
    "7. [üßÆ Cosine Similarity and Vector Operations](#7--cosine-similarity-and-vector-operations)\n",
    "8. [üèóÔ∏è Building the Riddle Solver Architecture](#8--building-the-riddle-solver-architecture)\n",
    "9. [‚ö° Optimization Techniques](#9--optimization-techniques)\n",
    "10. [üéÆ Interactive Exercises](#10--interactive-exercises)\n",
    "11. [üöÄ Complete Solution Walkthrough](#11--complete-solution-walkthrough)\n",
    "12. [üìñ Summary and Next Steps](#12--summary-and-next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üéì Understanding Word Riddles\n",
    "\n",
    "### What is a Word Riddle?\n",
    "\n",
    "A word riddle presents a description or definition, and you need to guess the word being described. Think of it as reverse dictionary lookup! üîÑ\n",
    "\n",
    "**Examples:**\n",
    "- **Riddle:** \"kobieta podr√≥≈ºujƒÖca ≈õrodkiem transportu, np. samolotem, pociƒÖgiem, statkiem\"\n",
    "- **Answer:** \"pasa≈ºerka\" (female passenger)\n",
    "\n",
    "- **Riddle:** \"emocjonalne uczucie ≈ÇƒÖczƒÖce dwie osoby, oparte na zaufaniu, szacunku, trosce i oddaniu\"\n",
    "- **Answer:** \"mi≈Ço≈õƒá\" (love)\n",
    "\n",
    "### How Do Humans Solve Riddles?\n",
    "\n",
    "1. **Parse the description** - identify key concepts and relationships\n",
    "2. **Activate semantic knowledge** - think of related words and concepts\n",
    "3. **Find intersections** - look for words that match multiple clues\n",
    "4. **Eliminate impossibilities** - rule out words that don't fit\n",
    "5. **Select best match** - choose the word that best fits all clues\n",
    "\n",
    "### The Computational Challenge\n",
    "\n",
    "To solve riddles computationally, we need to:\n",
    "- **Understand meaning** - represent words as vectors in semantic space\n",
    "- **Measure similarity** - quantify how similar word meanings are\n",
    "- **Handle ambiguity** - deal with multiple meanings and synonyms\n",
    "- **Scale efficiently** - search through thousands of possible answers\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a riddle $R = \\{w_1, w_2, ..., w_n\\}$ (set of words in the description) and a dictionary $D$ of possible answers, find:\n",
    "\n",
    "$$\\text{answer} = \\arg\\max_{d \\in D} \\text{similarity}(R, \\text{definitions}(d))$$\n",
    "\n",
    "The challenge is defining `similarity` in a way that captures semantic relationships! üß†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìä Word Embeddings and Semantic Similarity\n",
    "\n",
    "### What are Word Embeddings?\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic relationships. Instead of treating words as discrete symbols, we represent them as points in a high-dimensional space where **similar words are close together**.\n",
    "\n",
    "### Key Intuition\n",
    "\n",
    "**\"You shall know a word by the company it keeps\"** - J.R. Firth\n",
    "\n",
    "Words that appear in similar contexts tend to have similar meanings. Word2Vec and similar models learn these patterns from large text corpora.\n",
    "\n",
    "### Word2Vec Architecture\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the mat\"\n",
    "Context window size = 2\n",
    "\n",
    "Training pairs:\n",
    "(\"cat\", \"The\"), (\"cat\", \"sat\")  # cat appears near these words\n",
    "(\"sat\", \"cat\"), (\"sat\", \"on\")   # sat appears near these words\n",
    "...\n",
    "```\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Word2Vec learns two matrices:\n",
    "- **Input matrix** $W_{in} \\in \\mathbb{R}^{V \\times d}$ (vocabulary size √ó embedding dimension)\n",
    "- **Output matrix** $W_{out} \\in \\mathbb{R}^{d \\times V}$\n",
    "\n",
    "The probability that word $w_o$ appears in the context of word $w_i$ is:\n",
    "\n",
    "$$P(w_o|w_i) = \\frac{\\exp(v_{w_o}^T v_{w_i})}{\\sum_{w=1}^{V} \\exp(v_w^T v_{w_i})}$$\n",
    "\n",
    "### Properties of Word Embeddings\n",
    "\n",
    "1. **Semantic similarity**: Similar words have similar vectors\n",
    "2. **Arithmetic relationships**: king - man + woman ‚âà queen\n",
    "3. **Clustering**: Related words cluster together in vector space\n",
    "4. **Compositionality**: Phrases can be represented as vector combinations\n",
    "\n",
    "### Why This Matters for Riddles\n",
    "\n",
    "Word embeddings allow us to:\n",
    "- **Measure semantic similarity** between riddle words and definitions\n",
    "- **Handle synonyms** - words with similar meanings have similar vectors\n",
    "- **Capture context** - polysemous words get context-dependent representations\n",
    "- **Enable fuzzy matching** - find approximate rather than exact matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Setting Up the Environment\n",
    "\n",
    "Let's start by importing all the necessary libraries and setting up our environment for working with Polish word riddles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for word riddle solving\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict as dd\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Natural language processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "\n",
    "# Word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Linear algebra operations\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üìä NumPy version:\", np.__version__)\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    print(\"‚úÖ NLTK punkt tokenizer ready!\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  NLTK download may be needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üóÇÔ∏è Working with Polish Language Data\n",
    "\n",
    "### Understanding the Data Structure\n",
    "\n",
    "In this problem, we work with several key data sources:\n",
    "\n",
    "1. **Dictionary definitions** (`plwiktionary_definitions_clean.txt`) - definitions of Polish words\n",
    "2. **Word base forms** (`superbazy_clean.txt`) - mapping from inflected forms to base forms\n",
    "3. **Word embeddings** (`w2v_polish_lemmas.model`) - pre-trained Word2Vec model\n",
    "4. **Sample riddles** (`zagadki_do_testow_clean.txt`) - examples for testing\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Base Forms (Lemmatization)**: Polish is a highly inflected language. The word \"kot\" (cat) can appear as \"kota\", \"kotem\", \"kot√≥w\", etc. We need to map all forms to their base form.\n",
    "\n",
    "**Inverse Document Frequency (IDF)**: Measures how rare/important a word is. Rare words are more informative than common words like \"the\", \"and\", etc.\n",
    "\n",
    "Let's simulate loading this data structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the data structures we'll work with\n",
    "# In the real problem, these would be loaded from files\n",
    "\n",
    "# Dictionary: word -> list of definitions (each definition is a set of words)\n",
    "all_word_definitions = dd(list)\n",
    "\n",
    "# Dictionary: inflected form -> base form\n",
    "bases = {}\n",
    "\n",
    "# Dictionary: base form -> IDF score\n",
    "base_idf = dd(float)\n",
    "\n",
    "# Let's create some example data to understand the structure\n",
    "# Example 1: \"kot\" (cat)\n",
    "all_word_definitions[\"kot\"] = [\n",
    "    {\"zwierzƒô\", \"domowe\", \"ssak\", \"futro\", \"pazury\"},\n",
    "    {\"ma≈Çy\", \"drapie≈ºnik\", \"miauczy\", \"≈Çapie\", \"myszy\"},\n",
    "]\n",
    "\n",
    "# Example 2: \"mi≈Ço≈õƒá\" (love)\n",
    "all_word_definitions[\"mi≈Ço≈õƒá\"] = [\n",
    "    {\"uczucie\", \"emocja\", \"przywiƒÖzanie\", \"serce\", \"kochaƒá\"},\n",
    "    {\"relacja\", \"zwiƒÖzek\", \"partnerstwo\", \"zaufanie\", \"oddanie\"},\n",
    "]\n",
    "\n",
    "# Example base form mappings\n",
    "bases[\"koty\"] = \"kot\"  # cats -> cat\n",
    "bases[\"kota\"] = \"kot\"  # cat (genitive) -> cat\n",
    "bases[\"kotem\"] = \"kot\"  # with cat (instrumental) -> cat\n",
    "bases[\"kocham\"] = \"kochaƒá\"  # I love -> to love\n",
    "bases[\"mi≈Ço≈õci\"] = \"mi≈Ço≈õƒá\"  # love (genitive) -> love\n",
    "\n",
    "# Example IDF scores (higher = rarer word)\n",
    "base_idf[\"kot\"] = 4.2\n",
    "base_idf[\"mi≈Ço≈õƒá\"] = 5.1\n",
    "base_idf[\"zwierzƒô\"] = 3.8\n",
    "base_idf[\"uczucie\"] = 4.6\n",
    "base_idf[\"i\"] = 1.2  # very common word, low IDF\n",
    "base_idf[\"jest\"] = 1.5  # very common word, low IDF\n",
    "\n",
    "print(\"üìö Sample data structure created!\")\n",
    "print(f\"üê± Definitions for 'kot': {len(all_word_definitions['kot'])} definitions\")\n",
    "print(f\"‚ù§Ô∏è  Definitions for 'mi≈Ço≈õƒá': {len(all_word_definitions['mi≈Ço≈õƒá'])} definitions\")\n",
    "print(f\"üî§ Base form mappings: {len(bases)} examples\")\n",
    "print(f\"üìä IDF scores: {len(base_idf)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get base form of a word\n",
    "def get_word_base(word):\n",
    "    \"\"\"Get the base (lemmatized) form of a word\"\"\"\n",
    "    word = word.lower()\n",
    "    return bases.get(word, word)  # return base form if exists, else return word itself\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_words = [\"koty\", \"kocham\", \"mi≈Ço≈õci\", \"nowe_s≈Çowo\"]\n",
    "print(\"üîç Testing base form lookup:\")\n",
    "for word in test_words:\n",
    "    base = get_word_base(word)\n",
    "    print(f\"  {word} ‚Üí {base}\")\n",
    "\n",
    "# Demonstrate why base forms matter\n",
    "print(\"\\nüí° Why base forms matter:\")\n",
    "print(\n",
    "    \"The riddle might use 'koty' (cats) but the dictionary definition is under 'kot' (cat)\"\n",
    ")\n",
    "print(\"Without lemmatization, we'd miss the connection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üéØ Clustering Words by Meaning\n",
    "\n",
    "### The Clustering Challenge\n",
    "\n",
    "When we look at word definitions, we often see many related words. For example, a definition of \"kot\" might include: `{\"zwierzƒô\", \"domowe\", \"ssak\", \"futro\", \"pazury\", \"ma≈Çy\", \"drapie≈ºnik\", \"miauczy\", \"≈Çapie\", \"myszy\"}`\n",
    "\n",
    "**Problem**: Not all these words are equally important! Some are:\n",
    "- **Core concepts** (zwierzƒô, ssak) - central to the meaning\n",
    "- **Descriptive details** (futro, pazury) - important but secondary  \n",
    "- **Common words** (ma≈Çy) - less discriminative\n",
    "\n",
    "### Clustering Solution\n",
    "\n",
    "We can group related words into **clusters** where each cluster represents a coherent concept. This helps us:\n",
    "1. **Reduce noise** - group similar words together\n",
    "2. **Weight importance** - give more weight to important clusters\n",
    "3. **Improve matching** - compare clusters instead of individual words\n",
    "\n",
    "### Mathematical Approach\n",
    "\n",
    "For a set of words $W = \\{w_1, w_2, ..., w_n\\}$ in a definition:\n",
    "\n",
    "1. **Convert to vectors**: $V = \\{v_1, v_2, ..., v_n\\}$ using Word2Vec\n",
    "2. **Apply clustering algorithm**: Group similar vectors\n",
    "3. **Compute cluster centroids**: $c_k = \\frac{\\sum_{v_i \\in C_k} w_i \\cdot v_i}{\\sum_{v_i \\in C_k} w_i}$\n",
    "4. **Weight by IDF**: Use inverse document frequency as weights $w_i$\n",
    "\n",
    "### Clustering Algorithm\n",
    "\n",
    "We'll use a **greedy clustering approach**:\n",
    "- Start with empty clusters\n",
    "- For each word vector:\n",
    "  - Find most similar existing cluster (cosine similarity)\n",
    "  - If similarity > threshold: add to cluster and update centroid\n",
    "  - Else: create new cluster (if word is important enough)\n",
    "\n",
    "This is more flexible than K-means because we don't need to specify the number of clusters in advance!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìà TF-IDF and Word Importance\n",
    "\n",
    "### What is TF-IDF?\n",
    "\n",
    "**TF-IDF** stands for Term Frequency - Inverse Document Frequency. It's a way to measure how important a word is in a document relative to a collection of documents.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{TF-IDF}(w, d, D) = \\text{TF}(w, d) \\times \\text{IDF}(w, D)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{TF}(w, d)$ = frequency of word $w$ in document $d$\n",
    "- $\\text{IDF}(w, D) = \\log\\frac{|D|}{|\\{d \\in D : w \\in d\\}|}$\n",
    "\n",
    "### Simplified IDF for Our Problem\n",
    "\n",
    "In our riddle solver, we use a simplified version focusing only on **IDF**:\n",
    "- Words that appear in many definitions ‚Üí **low IDF** ‚Üí less important\n",
    "- Words that appear in few definitions ‚Üí **high IDF** ‚Üí more important\n",
    "\n",
    "### Weight Function\n",
    "\n",
    "We transform raw IDF scores using a weight function:\n",
    "\n",
    "$$\\text{weight}(w) = \\max(0, (\\text{IDF}(w) - M)^D)$$\n",
    "\n",
    "Where:\n",
    "- $M$ = offset parameter (shifts the function)\n",
    "- $D$ = shape parameter (controls how steeply weights increase)\n",
    "\n",
    "This gives us several benefits:\n",
    "1. **Filter common words**: Very common words get weight ‚âà 0\n",
    "2. **Boost rare words**: Rare, specific words get high weights  \n",
    "3. **Smooth scaling**: Gradual transition, not binary cutoff\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the weight function\n",
    "def get_weight(idf_score, weight_m=-1.0, weight_d=1.35):\n",
    "    \"\"\"\n",
    "    Transform raw IDF score into a weight using power function.\n",
    "\n",
    "    Args:\n",
    "        idf_score: Raw IDF value\n",
    "        weight_m: Offset parameter (shifts function left/right)\n",
    "        weight_d: Shape parameter (controls steepness)\n",
    "\n",
    "    Returns:\n",
    "        float: Transformed weight (always >= 0)\n",
    "    \"\"\"\n",
    "    x = idf_score - weight_m\n",
    "    if x < 0:\n",
    "        return 0.0\n",
    "    return x**weight_d\n",
    "\n",
    "\n",
    "# Visualize the weight function\n",
    "idf_values = np.linspace(0, 6, 100)\n",
    "weights = [get_weight(idf) for idf in idf_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(idf_values, weights, \"b-\", linewidth=2, label=\"Weight Function\")\n",
    "plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "plt.xlabel(\"IDF Score\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"üìä IDF to Weight Transformation\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Add some example points\n",
    "example_words = [\"i\", \"jest\", \"zwierzƒô\", \"kot\", \"mi≈Ço≈õƒá\"]\n",
    "example_idfs = [1.2, 1.5, 3.8, 4.2, 5.1]\n",
    "example_weights = [get_weight(idf) for idf in example_idfs]\n",
    "\n",
    "for word, idf, weight in zip(example_words, example_idfs, example_weights):\n",
    "    plt.plot(idf, weight, \"ro\", markersize=8)\n",
    "    plt.annotate(\n",
    "        f\"{word}\\n({weight:.2f})\",\n",
    "        xy=(idf, weight),\n",
    "        xytext=(10, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key insights:\")\n",
    "print(f\"  ‚Ä¢ Common words like 'i' get weight ‚âà 0\")\n",
    "print(f\"  ‚Ä¢ Specific words like 'mi≈Ço≈õƒá' get high weight\")\n",
    "print(f\"  ‚Ä¢ The function smoothly transitions between extremes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üßÆ Cosine Similarity and Vector Operations\n",
    "\n",
    "### Understanding Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the **angle** between two vectors, not their magnitude. This is perfect for comparing word meanings because:\n",
    "\n",
    "- **Direction matters more than magnitude** - \"cat\" and \"kitten\" should be similar regardless of vector lengths\n",
    "- **Range is [-1, 1]** - easy to interpret (1 = identical, 0 = orthogonal, -1 = opposite)\n",
    "- **Robust to scaling** - adding the same concept multiple times doesn't change similarity\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For vectors $\\mathbf{a}$ and $\\mathbf{b}$:\n",
    "\n",
    "$$\\text{cosine similarity} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||} = \\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2} \\sqrt{\\sum_{i=1}^n b_i^2}}$$\n",
    "\n",
    "### Why Cosine Similarity for Words?\n",
    "\n",
    "1. **Semantic relationships**: Words with similar meanings have similar vector directions\n",
    "2. **Scale invariance**: A word mentioned once vs. multiple times has the same semantic content\n",
    "3. **Efficient computation**: Can be computed quickly using matrix operations\n",
    "\n",
    "### Vector Normalization\n",
    "\n",
    "Normalizing vectors to unit length simplifies cosine similarity:\n",
    "\n",
    "$$\\text{normalize}(\\mathbf{v}) = \\frac{\\mathbf{v}}{||\\mathbf{v}||} \\quad \\text{so that} \\quad ||\\text{normalize}(\\mathbf{v})|| = 1$$\n",
    "\n",
    "For normalized vectors: $\\text{cosine similarity} = \\mathbf{a} \\cdot \\mathbf{b}$ (just dot product!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement vector operations for word embeddings\n",
    "def normalize(vec):\n",
    "    \"\"\"Normalize vector to unit length\"\"\"\n",
    "    return vec / np.sqrt((vec**2).sum())\n",
    "\n",
    "\n",
    "def vector_len(vec):\n",
    "    \"\"\"Calculate vector length (Euclidean norm)\"\"\"\n",
    "    return np.sqrt((vec**2).sum())\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2) / (vector_len(vec1) * vector_len(vec2))\n",
    "\n",
    "\n",
    "# Create example vectors to demonstrate\n",
    "# Simulate word embeddings (in practice, these come from Word2Vec)\n",
    "cat_vector = np.array([0.2, 0.8, 0.1, 0.9, 0.3])\n",
    "dog_vector = np.array([0.3, 0.7, 0.2, 0.8, 0.4])  # Similar to cat\n",
    "car_vector = np.array([0.9, 0.1, 0.8, 0.2, 0.7])  # Different from cat\n",
    "\n",
    "print(\"üê± Example: Comparing word vectors\")\n",
    "print(f\"Cat vector: {cat_vector}\")\n",
    "print(f\"Dog vector: {dog_vector}\")\n",
    "print(f\"Car vector: {car_vector}\")\n",
    "print()\n",
    "\n",
    "# Calculate similarities\n",
    "cat_dog_sim = cosine_similarity(cat_vector, dog_vector)\n",
    "cat_car_sim = cosine_similarity(cat_vector, car_vector)\n",
    "dog_car_sim = cosine_similarity(dog_vector, car_vector)\n",
    "\n",
    "print(\"üîç Cosine similarities:\")\n",
    "print(f\"Cat ‚Üî Dog: {cat_dog_sim:.3f}\")\n",
    "print(f\"Cat ‚Üî Car: {cat_car_sim:.3f}\")\n",
    "print(f\"Dog ‚Üî Car: {dog_car_sim:.3f}\")\n",
    "print()\n",
    "\n",
    "# Demonstrate normalization\n",
    "print(\"üìè Vector normalization:\")\n",
    "cat_norm = normalize(cat_vector)\n",
    "print(f\"Original cat vector length: {vector_len(cat_vector):.3f}\")\n",
    "print(f\"Normalized cat vector length: {vector_len(cat_norm):.3f}\")\n",
    "print(f\"Normalized cat vector: {cat_norm}\")\n",
    "\n",
    "# Show that cosine similarity with normalized vectors is just dot product\n",
    "print(f\"\\nüßÆ Cosine similarity: {cosine_similarity(cat_vector, dog_vector):.3f}\")\n",
    "print(\n",
    "    f\"Dot product of normalized: {np.dot(normalize(cat_vector), normalize(dog_vector)):.3f}\"\n",
    ")\n",
    "print(\"‚úÖ They're the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üèóÔ∏è Building the Riddle Solver Architecture\n",
    "\n",
    "### Overall Strategy\n",
    "\n",
    "Our riddle solver uses a **semantic similarity approach**:\n",
    "\n",
    "1. **Preprocess** - cluster all dictionary definitions  \n",
    "2. **Query** - cluster the riddle description\n",
    "3. **Compare** - find dictionary words with most similar definition clusters\n",
    "4. **Rank** - return top K most similar words\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Clustering Engine** - groups related words in definitions\n",
    "2. **Similarity Calculator** - compares cluster sets using cosine similarity  \n",
    "3. **Scoring Function** - combines multiple similarity signals\n",
    "4. **Optimization Layer** - speeds up search with masking\n",
    "\n",
    "### Clustering Algorithm Details\n",
    "\n",
    "```python\n",
    "def set_to_clusters(word_set):\n",
    "    clusters = []\n",
    "    for word in word_set:\n",
    "        # Get word vector and IDF weight\n",
    "        vector = get_word_embedding(word)\n",
    "        weight = get_weight(word_idf[word])\n",
    "        \n",
    "        # Find most similar existing cluster\n",
    "        best_similarity = -1\n",
    "        best_cluster = -1\n",
    "        \n",
    "        for i, (cluster_centroid, cluster_weight) in enumerate(clusters):\n",
    "            similarity = cosine_similarity(vector, cluster_centroid)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_cluster = i\n",
    "        \n",
    "        # Decide: add to existing cluster or create new one?\n",
    "        if best_similarity > SIMILARITY_THRESHOLD:\n",
    "            # Update existing cluster centroid (weighted average)\n",
    "            old_centroid, old_weight = clusters[best_cluster]\n",
    "            new_centroid = (old_centroid * old_weight + vector * weight) / (old_weight + weight)\n",
    "            clusters[best_cluster] = [new_centroid, old_weight + weight]\n",
    "        elif weight > IMPORTANCE_THRESHOLD:\n",
    "            # Create new cluster for important words\n",
    "            clusters.append([vector, weight])\n",
    "    \n",
    "    return clusters\n",
    "```\n",
    "\n",
    "### Scoring Strategy\n",
    "\n",
    "For each dictionary word, we compare its definition clusters with riddle clusters:\n",
    "\n",
    "$$\\text{score} = \\frac{\\text{max similarities riddle‚Üídef} + \\text{max similarities def‚Üíriddle}}{2}$$\n",
    "\n",
    "This **bidirectional scoring** ensures that both the riddle matches the definition AND the definition matches the riddle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simplified clustering algorithm\n",
    "def simple_clustering_demo(\n",
    "    word_set, similarity_threshold=0.3, importance_threshold=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Demonstrate clustering algorithm with simplified word vectors.\n",
    "    In practice, this would use real Word2Vec embeddings.\n",
    "    \"\"\"\n",
    "    # Simulate word embeddings (normally from Word2Vec)\n",
    "    simulated_embeddings = {\n",
    "        \"zwierzƒô\": np.array([0.8, 0.2, 0.1, 0.9, 0.3]),\n",
    "        \"ssak\": np.array([0.7, 0.3, 0.2, 0.8, 0.4]),\n",
    "        \"kot\": np.array([0.6, 0.4, 0.3, 0.7, 0.5]),\n",
    "        \"futro\": np.array([0.5, 0.5, 0.4, 0.6, 0.6]),\n",
    "        \"miauczy\": np.array([0.4, 0.6, 0.5, 0.5, 0.7]),\n",
    "        \"≈Çapie\": np.array([0.2, 0.8, 0.7, 0.3, 0.9]),\n",
    "        \"myszy\": np.array([0.3, 0.7, 0.6, 0.4, 0.8]),\n",
    "        \"ma≈Çy\": np.array([0.1, 0.1, 0.1, 0.1, 0.1]),  # generic word\n",
    "    }\n",
    "\n",
    "    clusters = []\n",
    "\n",
    "    print(\"üéØ Clustering process:\")\n",
    "    print(f\"Similarity threshold: {similarity_threshold}\")\n",
    "    print(f\"Importance threshold: {importance_threshold}\")\n",
    "    print()\n",
    "\n",
    "    for word in word_set:\n",
    "        if word not in simulated_embeddings:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {word} (no embedding)\")\n",
    "            continue\n",
    "\n",
    "        vector = normalize(simulated_embeddings[word])\n",
    "        weight = get_weight(base_idf.get(word, 2.0))\n",
    "\n",
    "        print(f\"Processing '{word}' (weight: {weight:.2f})\")\n",
    "\n",
    "        if not clusters:\n",
    "            # First cluster\n",
    "            clusters.append([vector, weight])\n",
    "            print(f\"  ‚Üí Created first cluster\")\n",
    "            continue\n",
    "\n",
    "        # Find most similar cluster\n",
    "        best_sim = -1\n",
    "        best_idx = -1\n",
    "\n",
    "        for i, (centroid, cluster_weight) in enumerate(clusters):\n",
    "            sim = cosine_similarity(vector, centroid)\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_idx = i\n",
    "\n",
    "        print(f\"  ‚Üí Best similarity: {best_sim:.3f} with cluster {best_idx}\")\n",
    "\n",
    "        if best_sim > similarity_threshold:\n",
    "            # Add to existing cluster\n",
    "            old_centroid, old_weight = clusters[best_idx]\n",
    "            new_centroid = (old_centroid * old_weight + vector * weight) / (\n",
    "                old_weight + weight\n",
    "            )\n",
    "            clusters[best_idx] = [normalize(new_centroid), old_weight + weight]\n",
    "            print(f\"  ‚Üí Added to cluster {best_idx}\")\n",
    "        elif weight > importance_threshold:\n",
    "            # Create new cluster\n",
    "            clusters.append([vector, weight])\n",
    "            print(f\"  ‚Üí Created new cluster {len(clusters)-1}\")\n",
    "        else:\n",
    "            print(f\"  ‚Üí Ignored (low importance)\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(f\"üèÅ Final result: {len(clusters)} clusters\")\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Test with cat definition\n",
    "cat_definition = {\n",
    "    \"zwierzƒô\",\n",
    "    \"ssak\",\n",
    "    \"kot\",\n",
    "    \"futro\",\n",
    "    \"miauczy\",\n",
    "    \"≈Çapie\",\n",
    "    \"myszy\",\n",
    "    \"ma≈Çy\",\n",
    "}\n",
    "clusters = simple_clustering_demo(cat_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ‚ö° Optimization Techniques\n",
    "\n",
    "### Performance Challenges\n",
    "\n",
    "Solving riddles requires comparing thousands of dictionary words against each riddle. Key bottlenecks:\n",
    "\n",
    "1. **Vector operations** - many cosine similarity calculations\n",
    "2. **Redundant comparisons** - similar words get similar scores\n",
    "3. **Memory usage** - storing all embeddings and clusters\n",
    "\n",
    "### Optimization Strategy 1: Masking Similar Words\n",
    "\n",
    "**Idea**: If word A is very similar to word B, and word A doesn't match the riddle well, then word B probably won't either.\n",
    "\n",
    "**Implementation**:\n",
    "- Precompute similarity matrix between all dictionary words\n",
    "- Create boolean mask: `mask[i,j] = True` if words i and j are similar\n",
    "- During search, if word i gets low score, mark similar words as \"masked\"\n",
    "- Skip masked words in future comparisons\n",
    "\n",
    "```python\n",
    "# Precompute similarity matrix\n",
    "similarity_matrix = compute_word_similarities(all_words)\n",
    "mask = similarity_matrix > MASK_THRESHOLD\n",
    "\n",
    "# During riddle solving\n",
    "for i, word in enumerate(all_words):\n",
    "    if already_processed[i]:\n",
    "        continue\n",
    "    \n",
    "    score = compute_score(riddle, word)\n",
    "    if score < LOW_SCORE_THRESHOLD:\n",
    "        # Mark similar words as processed\n",
    "        already_processed |= mask[i]\n",
    "```\n",
    "\n",
    "### Optimization Strategy 2: Matrix Operations\n",
    "\n",
    "**Idea**: Use numpy's optimized matrix operations instead of loops.\n",
    "\n",
    "**Before**: Compare clusters one by one\n",
    "```python\n",
    "for riddle_cluster in riddle_clusters:\n",
    "    for def_cluster in definition_clusters:\n",
    "        similarity = cosine_similarity(riddle_cluster, def_cluster)\n",
    "```\n",
    "\n",
    "**After**: Compute all similarities at once\n",
    "```python\n",
    "# Shape: (num_riddle_clusters, num_def_clusters)\n",
    "similarity_matrix = riddle_clusters @ definition_clusters.T\n",
    "```\n",
    "\n",
    "### Optimization Strategy 3: Dimension Reduction\n",
    "\n",
    "**Idea**: Use every Nth element of word vectors to reduce computation.\n",
    "\n",
    "```python\n",
    "# Instead of full 300-dimensional vector\n",
    "full_vector = model.wv['word']\n",
    "\n",
    "# Use every 2nd element ‚Üí 150 dimensions\n",
    "reduced_vector = model.wv['word'][::2]\n",
    "```\n",
    "\n",
    "This trades some accuracy for significant speed improvement!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate matrix operations optimization\n",
    "def compare_performance():\n",
    "    \"\"\"Compare loop-based vs matrix-based similarity computation\"\"\"\n",
    "\n",
    "    # Create example cluster matrices\n",
    "    riddle_clusters = np.random.randn(3, 10)  # 3 clusters, 10 dimensions each\n",
    "    def_clusters = np.random.randn(5, 10)  # 5 clusters, 10 dimensions each\n",
    "\n",
    "    # Normalize for fair comparison\n",
    "    riddle_clusters = np.array([normalize(c) for c in riddle_clusters])\n",
    "    def_clusters = np.array([normalize(c) for c in def_clusters])\n",
    "\n",
    "    print(\"üîÑ Method 1: Loop-based computation\")\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "    similarities_loop = np.zeros((3, 5))\n",
    "    for i in range(3):\n",
    "        for j in range(5):\n",
    "            similarities_loop[i, j] = np.dot(riddle_clusters[i], def_clusters[j])\n",
    "    loop_time = time.time() - start_time\n",
    "\n",
    "    print(\"‚ö° Method 2: Matrix-based computation\")\n",
    "    start_time = time.time()\n",
    "    similarities_matrix = riddle_clusters @ def_clusters.T\n",
    "    matrix_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nResults comparison:\")\n",
    "    print(f\"Loop method time: {loop_time*1000:.2f} ms\")\n",
    "    print(f\"Matrix method time: {matrix_time*1000:.2f} ms\")\n",
    "    print(f\"Speedup: {loop_time/matrix_time:.1f}x\")\n",
    "    print(f\"Results identical: {np.allclose(similarities_loop, similarities_matrix)}\")\n",
    "\n",
    "    return similarities_matrix\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "similarity_matrix = compare_performance()\n",
    "print(f\"\\nüìä Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(\"Matrix values:\")\n",
    "print(similarity_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéÆ Interactive Exercises\n",
    "\n",
    "Now let's test your understanding with some hands-on exercises! Work through these to solidify the concepts.\n",
    "\n",
    "### Exercise 1: Word Similarity Exploration üîç\n",
    "\n",
    "**Your Task**: Use the functions we've built to explore relationships between Polish words.\n",
    "\n",
    "**Instructions**: \n",
    "1. Choose three Polish words that you think should be semantically similar\n",
    "2. Choose one word that should be different from the others\n",
    "3. Calculate pairwise cosine similarities\n",
    "4. Explain the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Complete this code\n",
    "print(\"üéØ Exercise 1: Word Similarity Exploration\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# TODO: Replace these with your chosen words\n",
    "similar_words = [\"kot\", \"pies\", \"kr√≥lik\"]  # Three similar words\n",
    "different_word = \"samoch√≥d\"  # One different word\n",
    "\n",
    "all_exercise_words = similar_words + [different_word]\n",
    "\n",
    "# Create simulated embeddings for exercise\n",
    "exercise_embeddings = {\n",
    "    \"kot\": np.array([0.8, 0.2, 0.1, 0.9, 0.3]),\n",
    "    \"pies\": np.array([0.7, 0.3, 0.2, 0.8, 0.4]),\n",
    "    \"kr√≥lik\": np.array([0.6, 0.4, 0.3, 0.7, 0.5]),\n",
    "    \"samoch√≥d\": np.array([0.1, 0.9, 0.8, 0.2, 0.6]),\n",
    "}\n",
    "\n",
    "print(\"Your words:\", all_exercise_words)\n",
    "print(\"\\nüìä Pairwise similarities:\")\n",
    "\n",
    "# TODO: Calculate all pairwise similarities\n",
    "for i, word1 in enumerate(all_exercise_words):\n",
    "    for j, word2 in enumerate(all_exercise_words):\n",
    "        if i < j:  # Avoid duplicates\n",
    "            vec1 = normalize(exercise_embeddings[word1])\n",
    "            vec2 = normalize(exercise_embeddings[word2])\n",
    "            similarity = cosine_similarity(vec1, vec2)\n",
    "            print(f\"{word1} ‚Üî {word2}: {similarity:.3f}\")\n",
    "\n",
    "print(\"\\n‚ùì Questions to think about:\")\n",
    "print(\"1. Which pairs have highest similarity?\")\n",
    "print(\"2. Which pairs have lowest similarity?\")\n",
    "print(\"3. Do the results match your intuition?\")\n",
    "print(\"4. What might cause unexpected results?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Mini Riddle Solver üß©\n",
    "\n",
    "**Your Task**: Create a simple riddle solver that can handle basic cases.\n",
    "\n",
    "**Scenario**: You're given a riddle and three possible answers. Find the best match!\n",
    "\n",
    "**Instructions**:\n",
    "1. Implement a function to compare riddle words with definition words\n",
    "2. Score each possible answer\n",
    "3. Return the best match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Build a Mini Riddle Solver\n",
    "print(\"üß© Exercise 2: Mini Riddle Solver\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sample riddle and possible answers\n",
    "riddle_words = {\"du≈ºe\", \"zwierzƒô\", \"trƒÖba\", \"szary\", \"afryka\"}\n",
    "candidate_answers = {\n",
    "    \"s≈Ço≈Ñ\": {\"zwierzƒô\", \"du≈ºe\", \"ssak\", \"trƒÖba\", \"afryka\", \"szary\"},\n",
    "    \"kot\": {\"zwierzƒô\", \"ma≈Çy\", \"futro\", \"miauczy\", \"pazury\"},\n",
    "    \"samoch√≥d\": {\"pojazd\", \"ko≈Ça\", \"silnik\", \"transport\", \"benzyna\"},\n",
    "}\n",
    "\n",
    "print(f\"üéØ Riddle words: {riddle_words}\")\n",
    "print(f\"ü§î Possible answers: {list(candidate_answers.keys())}\")\n",
    "print()\n",
    "\n",
    "\n",
    "def mini_riddle_solver(riddle_words, candidate_answers):\n",
    "    \"\"\"\n",
    "    Simple riddle solver using word overlap and IDF weighting.\n",
    "\n",
    "    Args:\n",
    "        riddle_words: set of words in the riddle\n",
    "        candidate_answers: dict mapping answer -> definition words\n",
    "\n",
    "    Returns:\n",
    "        list of (answer, score) pairs, sorted by score descending\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for answer, definition in candidate_answers.items():\n",
    "        print(f\"Evaluating '{answer}'...\")\n",
    "\n",
    "        # Find word overlaps\n",
    "        common_words = riddle_words.intersection(definition)\n",
    "        print(f\"  Common words: {common_words}\")\n",
    "\n",
    "        # Calculate weighted score using IDF\n",
    "        score = 0\n",
    "        for word in common_words:\n",
    "            weight = get_weight(base_idf.get(word, 3.0))  # Default IDF if not found\n",
    "            score += weight\n",
    "            print(f\"    '{word}': IDF weight = {weight:.2f}\")\n",
    "\n",
    "        # Add bonus for coverage\n",
    "        coverage = len(common_words) / len(riddle_words)\n",
    "        score += coverage * 2  # Bonus multiplier\n",
    "        print(f\"  Coverage bonus: {coverage:.2f} * 2 = {coverage * 2:.2f}\")\n",
    "        print(f\"  Total score: {score:.2f}\")\n",
    "        print()\n",
    "\n",
    "        scores.append((answer, score))\n",
    "\n",
    "    # Sort by score descending\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Solve the riddle\n",
    "results = mini_riddle_solver(riddle_words, candidate_answers)\n",
    "\n",
    "print(\"üèÜ Final ranking:\")\n",
    "for rank, (answer, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. {answer}: {score:.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best answer: {results[0][0]}\")\n",
    "print(\"Does this make sense? Why or why not?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üöÄ Complete Solution Walkthrough\n",
    "\n",
    "Now let's walk through the key components of the complete solution, connecting all the concepts we've learned.\n",
    "\n",
    "### Solution Architecture Overview\n",
    "\n",
    "The complete solution has these main components:\n",
    "\n",
    "1. **Configuration Parameters** - tunable hyperparameters\n",
    "2. **Helper Functions** - vector operations and utilities  \n",
    "3. **Clustering Engine** - groups words by semantic similarity\n",
    "4. **Masking System** - optimization for similar words\n",
    "5. **Precomputation** - processes all data upfront\n",
    "6. **Main Solver** - answers riddles using similarity scoring\n",
    "\n",
    "### Key Parameters from the Solution\n",
    "\n",
    "```python\n",
    "# Clustering parameters\n",
    "CLUSTERIZATION_A = 0.3      # Similarity threshold for joining clusters\n",
    "CLUSTERIZATION_W = 0.7      # Minimum IDF weight for new clusters\n",
    "VECTOR_R = 2                # Vector dimension reduction factor\n",
    "NORM_F = True               # Whether to normalize vectors\n",
    "\n",
    "# Masking parameters  \n",
    "MASK_T1 = 0.75              # High similarity threshold for masking\n",
    "MASK_T2 = 0.45              # Low similarity threshold for masking\n",
    "\n",
    "# IDF weight function parameters\n",
    "WEIGHT_M = -1.0             # Offset parameter\n",
    "WEIGHT_D = 1.35             # Shape parameter\n",
    "\n",
    "# Scoring parameters\n",
    "SCR_SNDW_1 = 0.3            # Weight for second direction similarity\n",
    "SCR_SNDW_2 = 0.3            # Weight for second direction normalization\n",
    "```\n",
    "\n",
    "### Algorithm Flow\n",
    "\n",
    "1. **Precomputation Phase**:\n",
    "   - Load word definitions and embeddings\n",
    "   - Cluster all dictionary definitions\n",
    "   - Compute similarity masks between words\n",
    "   - Optimize word ordering\n",
    "\n",
    "2. **Query Phase**:\n",
    "   - Cluster the riddle words\n",
    "   - Compare riddle clusters with each dictionary word's definition clusters\n",
    "   - Apply masking to skip similar words\n",
    "   - Score using bidirectional similarity\n",
    "   - Return top K answers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the core scoring logic from the solution\n",
    "def demonstrate_bidirectional_scoring():\n",
    "    \"\"\"Show how the solution scores riddle-definition similarity\"\"\"\n",
    "\n",
    "    print(\"üéØ Bidirectional Scoring Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Simulate riddle clusters (3 clusters, 5 dimensions each)\n",
    "    riddle_clusters = np.array(\n",
    "        [\n",
    "            [0.8, 0.2, 0.1, 0.9, 0.3],  # Animal concept\n",
    "            [0.6, 0.4, 0.3, 0.7, 0.5],  # Size concept\n",
    "            [0.4, 0.6, 0.5, 0.5, 0.7],  # Geography concept\n",
    "        ]\n",
    "    )\n",
    "    riddle_weights = np.array([3.0, 2.0, 2.5])  # IDF weights\n",
    "\n",
    "    # Simulate definition clusters (4 clusters, 5 dimensions each)\n",
    "    definition_clusters = np.array(\n",
    "        [\n",
    "            [0.7, 0.3, 0.2, 0.8, 0.4],  # Animal concept (similar)\n",
    "            [0.5, 0.5, 0.4, 0.6, 0.6],  # Size concept (similar)\n",
    "            [0.1, 0.9, 0.8, 0.2, 0.6],  # Transport concept (different)\n",
    "            [0.3, 0.7, 0.6, 0.4, 0.8],  # Action concept (different)\n",
    "        ]\n",
    "    )\n",
    "    definition_weights = np.array([2.8, 1.8, 1.2, 1.5])  # IDF weights\n",
    "\n",
    "    print(\"Riddle clusters shape:\", riddle_clusters.shape)\n",
    "    print(\"Definition clusters shape:\", definition_clusters.shape)\n",
    "    print()\n",
    "\n",
    "    # Compute similarity matrix (cosine similarities between all cluster pairs)\n",
    "    # This is the key matrix operation from the solution\n",
    "    similarity_matrix = riddle_clusters @ definition_clusters.T\n",
    "\n",
    "    print(\"üìä Cluster Similarity Matrix:\")\n",
    "    print(\"   \", \" \".join(f\"Def{i}\" for i in range(4)))\n",
    "    for i, row in enumerate(similarity_matrix):\n",
    "        print(f\"R{i} \", \" \".join(f\"{val:.2f}\" for val in row))\n",
    "    print()\n",
    "\n",
    "    # Direction 1: Riddle ‚Üí Definition (for each riddle cluster, find best definition match)\n",
    "    riddle_to_def = similarity_matrix.max(axis=1)  # Max along definition axis\n",
    "    score_1 = np.dot(riddle_to_def, riddle_weights) / riddle_weights.sum()\n",
    "\n",
    "    print(\"Direction 1 (Riddle ‚Üí Definition):\")\n",
    "    print(\"  Max similarities per riddle cluster:\", riddle_to_def)\n",
    "    print(\"  Weighted score:\", score_1)\n",
    "\n",
    "    # Direction 2: Definition ‚Üí Riddle (for each definition cluster, find best riddle match)\n",
    "    def_to_riddle = similarity_matrix.max(axis=0)  # Max along riddle axis\n",
    "    score_2 = np.dot(def_to_riddle, definition_weights) / definition_weights.sum()\n",
    "\n",
    "    print(\"\\nDirection 2 (Definition ‚Üí Riddle):\")\n",
    "    print(\"  Max similarities per definition cluster:\", def_to_riddle)\n",
    "    print(\"  Weighted score:\", score_2)\n",
    "\n",
    "    # Final combined score (like in the solution)\n",
    "    final_score = (score_1 + score_2 * 0.3) / (1 + 0.3)  # SCR_SNDW_1 = 0.3\n",
    "\n",
    "    print(f\"\\nüèÜ Final combined score: {final_score:.3f}\")\n",
    "    print(\"\\nüí° Key insights:\")\n",
    "    print(\"  ‚Ä¢ Higher scores = better semantic match\")\n",
    "    print(\"  ‚Ä¢ Bidirectional ensures both riddle and definition are well-covered\")\n",
    "    print(\"  ‚Ä¢ IDF weighting emphasizes important/rare concepts\")\n",
    "\n",
    "    return similarity_matrix, final_score\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "similarity_matrix, score = demonstrate_bidirectional_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. üìñ Summary and Next Steps\n",
    "\n",
    "### üéØ What You've Learned\n",
    "\n",
    "Congratulations! You've now mastered the key concepts needed to build a sophisticated word riddle solver:\n",
    "\n",
    "1. **Word Embeddings** - representing words as vectors that capture semantic relationships\n",
    "2. **Cosine Similarity** - measuring how similar word meanings are\n",
    "3. **Clustering** - grouping related words to reduce noise and improve matching\n",
    "4. **TF-IDF Weighting** - emphasizing rare, informative words over common ones\n",
    "5. **Matrix Operations** - optimizing computations for speed\n",
    "6. **Bidirectional Scoring** - ensuring both riddle and definitions are well-matched\n",
    "7. **Masking Optimization** - skipping similar words to improve performance\n",
    "\n",
    "### üß† Key Insights\n",
    "\n",
    "- **Semantic similarity is key** - word embeddings let us capture meaning relationships\n",
    "- **Clustering reduces noise** - grouping similar words improves signal-to-noise ratio\n",
    "- **IDF weighting matters** - rare words are more informative than common ones\n",
    "- **Bidirectional comparison is crucial** - both directions must match well\n",
    "- **Optimization is essential** - techniques like masking make real-time performance possible\n",
    "\n",
    "### üöÄ Building Your Solution\n",
    "\n",
    "Now you're ready to implement your own riddle solver! Key components:\n",
    "\n",
    "1. **Load and preprocess data** - word definitions, embeddings, IDF scores\n",
    "2. **Implement clustering** - group words by semantic similarity\n",
    "3. **Create similarity functions** - compare riddle clusters with definition clusters\n",
    "4. **Add optimization** - masking, matrix operations, dimension reduction\n",
    "5. **Tune parameters** - clustering thresholds, scoring weights, etc.\n",
    "\n",
    "### üîó Useful Resources\n",
    "\n",
    "- **Word2Vec Paper**: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "- **Cosine Similarity**: [Understanding the math behind similarity measures](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "- **TF-IDF**: [Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- **K-means Clustering**: [Understanding clustering algorithms](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- **NumPy Documentation**: [Efficient numerical operations](https://numpy.org/doc/)\n",
    "\n",
    "### üéÆ Additional Challenges\n",
    "\n",
    "Ready for more? Try these extensions:\n",
    "- **Experiment with different clustering algorithms** (K-means, DBSCAN, hierarchical)\n",
    "- **Try other similarity measures** (Euclidean distance, Manhattan distance)\n",
    "- **Implement attention mechanisms** for better cluster comparison\n",
    "- **Add word sense disambiguation** for polysemous words\n",
    "- **Create a web interface** for interactive riddle solving\n",
    "\n",
    "Good luck building your riddle solver! üß©‚ú®\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
