{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Tutorial: Deep Learning for Color Quantization\n",
    "\n",
    "![Color Quantization](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Dithering_example_dithered_color_palette.png/320px-Dithering_example_dithered_color_palette.png)\n",
    "\n",
    "## Welcome to the Fascinating World of Color Quantization! ðŸŒˆ\n",
    "\n",
    "In this comprehensive tutorial, you'll learn:\n",
    "- ðŸŽ¨ What is color quantization and why it matters\n",
    "- ðŸ§® The mathematics behind color reduction algorithms\n",
    "- ðŸ¤– How to use deep learning for intelligent color selection\n",
    "- ðŸ’» Hands-on implementation with PyTorch\n",
    "- ðŸŽ¯ Visualization of quantization effects\n",
    "- ðŸ§ª Interactive exercises to build your skills\n",
    "\n",
    "By the end, you'll be ready to implement sophisticated color quantization using neural networks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Table of Contents\n",
    "\n",
    "1. [ðŸŽ“ Understanding Color Quantization](#1--understanding-color-quantization)\n",
    "2. [ðŸ§® Mathematical Foundation](#2--mathematical-foundation)\n",
    "3. [ðŸ”§ Setting Up the Environment](#3--setting-up-the-environment)\n",
    "4. [ðŸ—ï¸ Classical Approaches](#4--classical-approaches)\n",
    "5. [ðŸ¤– Deep Learning Approach](#5--deep-learning-approach)\n",
    "6. [ðŸ§  Building the Neural Network](#6--building-the-neural-network)\n",
    "7. [ðŸŽ¯ Custom Loss Function](#7--custom-loss-function)\n",
    "8. [ðŸ’¼ Complete Solution](#8--complete-solution)\n",
    "9. [ðŸŽ¨ Visualizing Results](#9--visualizing-results)\n",
    "10. [ðŸŽ® Interactive Exercises](#10--interactive-exercises)\n",
    "11. [ðŸš€ Advanced Techniques](#11--advanced-techniques)\n",
    "12. [ðŸ“– Summary and Next Steps](#12--summary-and-next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸŽ“ Understanding Color Quantization\n",
    "\n",
    "### What is Color Quantization?\n",
    "\n",
    "Imagine you have a beautiful photograph with millions of colors ðŸŒˆ. Color quantization is the process of reducing this rich palette to a much smaller set of colors while preserving the visual quality as much as possible.\n",
    "\n",
    "Think of it like this:\n",
    "- **Original image**: 16.7 million possible colors (24-bit RGB)\n",
    "- **Quantized image**: Only 37 carefully chosen colors\n",
    "- **Goal**: Make the 37-color version look as close to the original as possible\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Color Palette**: The limited set of colors we can use (37 in our case)\n",
    "- **Color Assignment**: Mapping each original pixel to the closest palette color\n",
    "- **Quality Metrics**: How we measure how \"good\" our quantization is\n",
    "- **Color Space**: The mathematical representation of colors (RGB)\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "Given an image $I$ with pixels $p_{i,j} \\in \\mathbb{R}^3$ and a palette $P = \\{c_1, c_2, ..., c_k\\}$ where $k=37$:\n",
    "\n",
    "$$Q(I) = \\{q_{i,j} | q_{i,j} = \\arg\\min_{c \\in P} ||p_{i,j} - c||_2\\}$$\n",
    "\n",
    "Where $Q(I)$ is our quantized image and $||\\cdot||_2$ is the Euclidean distance.\n",
    "\n",
    "### Why Should We Care?\n",
    "\n",
    "ðŸ–¼ï¸ **Image Compression**: Reduce file sizes while maintaining quality  \n",
    "ðŸŽ® **Game Graphics**: Limited color palettes for retro aesthetics  \n",
    "ðŸ–¨ï¸ **Printing**: Optimize colors for specific printing processes  \n",
    "ðŸ“± **Mobile Displays**: Adapt images for devices with limited color ranges  \n",
    "ðŸŽ¨ **Artistic Effects**: Create stylized, poster-like images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ§® Mathematical Foundation\n",
    "\n",
    "Before diving into implementation, let's understand the math behind color quantization.\n",
    "\n",
    "### The Optimization Problem\n",
    "\n",
    "Color quantization is fundamentally an optimization problem. We want to find the best palette $P$ and assignment function $f$ such that:\n",
    "\n",
    "$$\\min_{P,f} \\sum_{i,j} ||p_{i,j} - f(p_{i,j}, P)||_2^2$$\n",
    "\n",
    "This is the **Mean Squared Error (MSE)** - our primary quality metric.\n",
    "\n",
    "### Color Cost Function\n",
    "\n",
    "In our specific problem, we have an additional constraint - the **color cost**. Some colors are \"more expensive\" than others:\n",
    "\n",
    "$$\\text{Color Cost}(c) = \\min_{v \\in V} ||c - v||_2$$\n",
    "\n",
    "Where $V$ are the vertices of the RGB cube: $\\{(0,0,0), (0,0,255), (0,255,0), ..., (255,255,255)\\}$\n",
    "\n",
    "### Complete Objective Function\n",
    "\n",
    "Our final objective combines both image quality and color preference:\n",
    "\n",
    "$$L = 2 \\cdot MSE + 21 \\cdot \\max(\\text{color costs}) + 42 \\cdot \\text{mean}(\\text{color costs})$$\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "This is a **non-convex, combinatorial optimization problem**. Traditional approaches:\n",
    "- K-means clustering ðŸ“Š\n",
    "- Median cut algorithm âœ‚ï¸\n",
    "- Octree quantization ðŸŒ³\n",
    "\n",
    "**Our Innovation**: Use deep learning to directly optimize this complex objective! ðŸ¤–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Key Insight\n",
    "\n",
    "**Why is this problem so hard?**\n",
    "\n",
    "Traditional clustering algorithms like K-means optimize for minimal within-cluster variance, but our objective function is much more complex! We need to balance:\n",
    "\n",
    "1. **Image Quality**: How similar the quantized image looks to the original\n",
    "2. **Color Preferences**: Some colors are \"cheaper\" than others (RGB cube vertices)\n",
    "3. **Exact Count**: We must use exactly 37 colors, no more, no less\n",
    "\n",
    "Think of it like choosing a color palette for a painting - you want colors that represent your image well, but you also prefer certain \"primary\" colors that are easier to work with!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ”§ Setting Up the Environment\n",
    "\n",
    "Let's start by importing all the necessary libraries and setting up our environment. We'll be working with PyTorch for neural networks and various other libraries for image processing and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for our color quantization tutorial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# For image processing and visualization\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up device - GPU greatly speeds up neural network training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Environment setup complete!\")\n",
    "print(f\"ðŸ“¦ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ–¥ï¸  CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a sample image for our experiments\n",
    "def create_sample_image(size=64):\n",
    "    \"\"\"Create a colorful sample image for experimentation\"\"\"\n",
    "    img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "\n",
    "    # Create different colored regions\n",
    "    # Red region\n",
    "    img[: size // 2, : size // 2] = [255, 100, 100]\n",
    "\n",
    "    # Green region\n",
    "    img[: size // 2, size // 2 :] = [100, 255, 100]\n",
    "\n",
    "    # Blue region\n",
    "    img[size // 2 :, : size // 2] = [100, 100, 255]\n",
    "\n",
    "    # Mixed region with gradient\n",
    "    for i in range(size // 2, size):\n",
    "        for j in range(size // 2, size):\n",
    "            img[i, j] = [\n",
    "                int(255 * (i - size // 2) / (size // 2)),\n",
    "                int(255 * (j - size // 2) / (size // 2)),\n",
    "                int(255 * ((i + j - size) / size)),\n",
    "            ]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Create and visualize our sample image\n",
    "sample_img = create_sample_image(128)\n",
    "print(f\"ðŸ“¸ Created sample image with shape: {sample_img.shape}\")\n",
    "print(f\"ðŸŽ¨ Color range: [{sample_img.min()}, {sample_img.max()}]\")\n",
    "print(\n",
    "    f\"ðŸ”¢ Unique colors in sample: {len(np.unique(sample_img.reshape(-1, 3), axis=0))}\"\n",
    ")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_img)\n",
    "plt.title(\"ðŸŽ¨ Sample Image for Color Quantization\", fontsize=14, fontweight=\"bold\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸ—ï¸ Classical Approaches\n",
    "\n",
    "Before we dive into the deep learning solution, let's understand how traditional methods work. This will help us appreciate why neural networks are so powerful for this problem.\n",
    "\n",
    "### K-Means Clustering: The Classic Approach\n",
    "\n",
    "K-means is the most common method for color quantization. The idea is simple:\n",
    "1. Treat each pixel as a point in 3D RGB space\n",
    "2. Find k=37 cluster centers that minimize within-cluster variance\n",
    "3. Assign each pixel to its nearest cluster center\n",
    "\n",
    "Let's implement this and see how it performs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(img1, img2):\n",
    "    \"\"\"Calculate Mean Squared Error between two images\"\"\"\n",
    "    return np.mean((img1.astype(float) - img2.astype(float)) ** 2)\n",
    "\n",
    "\n",
    "def color_cost(img):\n",
    "    \"\"\"\n",
    "    Calculate color cost according to the problem definition.\n",
    "\n",
    "    Color cost is the distance to the nearest RGB cube vertex.\n",
    "    RGB cube vertices are combinations of 0 and 255 for each channel.\n",
    "    \"\"\"\n",
    "    # The 8 vertices of the RGB cube\n",
    "    vertices = np.array(\n",
    "        [\n",
    "            [0, 0, 0],  # black\n",
    "            [0, 0, 255],  # blue\n",
    "            [0, 255, 0],  # green\n",
    "            [0, 255, 255],  # cyan\n",
    "            [255, 0, 0],  # red\n",
    "            [255, 0, 255],  # magenta\n",
    "            [255, 255, 0],  # yellow\n",
    "            [255, 255, 255],  # white\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Flatten image to list of pixels\n",
    "    pixels = img.reshape(-1, 3)\n",
    "\n",
    "    # Calculate distance from each pixel to each vertex\n",
    "    distances = np.sqrt(\n",
    "        np.sum((pixels[:, None, :] - vertices[None, :, :]) ** 2, axis=2)\n",
    "    )\n",
    "\n",
    "    # Cost of each pixel is distance to nearest vertex\n",
    "    costs = np.min(distances, axis=1)\n",
    "\n",
    "    return np.mean(costs), np.max(costs)\n",
    "\n",
    "\n",
    "def quantization_score(original, quantized):\n",
    "    \"\"\"\n",
    "    Calculate the complete quantization score according to the problem.\n",
    "\n",
    "    Score = 2*MSE + 21*max_color_cost + 42*mean_color_cost\n",
    "    \"\"\"\n",
    "    mse_value = mse(original, quantized)\n",
    "    mean_cost, max_cost = color_cost(quantized)\n",
    "\n",
    "    return 2 * mse_value + 21 * max_cost + 42 * mean_cost\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Scoring functions implemented!\")\n",
    "print(\"   - mse(): Mean Squared Error\")\n",
    "print(\"   - color_cost(): Distance to RGB cube vertices\")\n",
    "print(\"   - quantization_score(): Complete objective function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_quantization(image, n_colors=37):\n",
    "    \"\"\"\n",
    "    Quantize image colors using K-means clustering.\n",
    "\n",
    "    This is the traditional approach - let's see how it performs!\n",
    "    \"\"\"\n",
    "    # Reshape image to be a list of pixels\n",
    "    pixels = image.reshape(-1, 3)\n",
    "\n",
    "    # Apply K-means clustering to find color centers\n",
    "    kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)\n",
    "    kmeans.fit(pixels)\n",
    "\n",
    "    # Get the color palette (cluster centers)\n",
    "    palette = kmeans.cluster_centers_.astype(np.uint8)\n",
    "\n",
    "    # Assign each pixel to nearest cluster center\n",
    "    labels = kmeans.labels_\n",
    "    quantized_pixels = palette[labels]\n",
    "\n",
    "    # Reshape back to image format\n",
    "    quantized_image = quantized_pixels.reshape(image.shape)\n",
    "\n",
    "    return quantized_image, palette\n",
    "\n",
    "\n",
    "# Test K-means quantization on our sample image\n",
    "print(\"ðŸ”¬ Testing K-means quantization...\")\n",
    "kmeans_result, kmeans_palette = kmeans_quantization(sample_img, n_colors=37)\n",
    "\n",
    "# Calculate the score\n",
    "score = quantization_score(sample_img, kmeans_result)\n",
    "mse_val = mse(sample_img, kmeans_result)\n",
    "mean_cost, max_cost = color_cost(kmeans_result)\n",
    "\n",
    "print(f\"ðŸ“Š K-means Results:\")\n",
    "print(f\"   MSE: {mse_val:.2f}\")\n",
    "print(f\"   Mean color cost: {mean_cost:.2f}\")\n",
    "print(f\"   Max color cost: {max_cost:.2f}\")\n",
    "print(f\"   ðŸŽ¯ Total score: {score:.2f}\")\n",
    "print(f\"   ðŸŽ¨ Colors in palette: {len(kmeans_palette)}\")\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(sample_img)\n",
    "axes[0].set_title(\"ðŸ–¼ï¸ Original Image\", fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(kmeans_result)\n",
    "axes[1].set_title(\"ðŸŽ¨ K-means Quantized\", fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Show the color palette\n",
    "palette_display = kmeans_palette.reshape(1, -1, 3)\n",
    "axes[2].imshow(palette_display, aspect=\"auto\")\n",
    "axes[2].set_title(\"ðŸŒˆ Color Palette (37 colors)\", fontweight=\"bold\")\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Problems with K-means\n",
    "\n",
    "While K-means is simple and fast, it has several limitations for our specific problem:\n",
    "\n",
    "1. **Wrong Objective**: K-means minimizes within-cluster variance, but our objective function is much more complex!\n",
    "2. **No Color Preferences**: K-means doesn't know that some colors (RGB vertices) are \"cheaper\"\n",
    "3. **Global Solution**: K-means finds a global solution, but different images might need different strategies\n",
    "4. **No Learning**: K-means starts from scratch for each image\n",
    "\n",
    "**The Solution**: Deep Learning! ðŸ¤–\n",
    "\n",
    "Let's see how we can use neural networks to directly optimize our complex objective function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ¤– Deep Learning Approach\n",
    "\n",
    "Now comes the exciting part! Instead of using traditional algorithms, we'll train a neural network to generate optimal color palettes.\n",
    "\n",
    "### The Big Idea ðŸ’¡\n",
    "\n",
    "What if we could train a neural network that:\n",
    "1. **Takes an image as input**\n",
    "2. **Outputs exactly 37 colors** that form the optimal palette\n",
    "3. **Is trained to minimize our exact objective function**\n",
    "4. **Learns to balance image quality vs. color costs**\n",
    "\n",
    "This is exactly what we'll build!\n",
    "\n",
    "### Why Neural Networks?\n",
    "\n",
    "ðŸŽ¯ **Direct Optimization**: We can train on our exact objective function  \n",
    "ðŸ§  **Adaptive**: Different images get different strategies  \n",
    "ðŸš€ **Powerful**: CNNs can extract complex visual features  \n",
    "âš¡ **End-to-End**: One network handles the entire pipeline  \n",
    "\n",
    "### The Architecture Strategy\n",
    "\n",
    "Our neural network will be a **Convolutional Neural Network (CNN)** with:\n",
    "- **Input**: RGB image (H Ã— W Ã— 3)\n",
    "- **Feature Extraction**: Convolutional layers to understand image content\n",
    "- **Global Understanding**: Pooling to reduce spatial dimensions\n",
    "- **Output**: Exactly 37 RGB colors (37 Ã— 3 = 111 values)\n",
    "\n",
    "The key insight: **We train a separate network for each image** (overfitting is good here!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸ§  Building the Neural Network\n",
    "\n",
    "Let's implement our Convolutional Neural Network! This is the heart of our solution.\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "Our CNN will have the following structure:\n",
    "1. **Conv Layer 1**: 3 â†’ 32 channels, extract basic features\n",
    "2. **Conv Layer 2**: 32 â†’ 64 channels, extract more complex features  \n",
    "3. **Conv Layer 3**: 64 â†’ 128 channels, extract high-level features\n",
    "4. **Global Pooling**: Reduce spatial dimensions\n",
    "5. **Fully Connected**: 128Ã—64Ã—64 â†’ 512 â†’ 256 â†’ 111 neurons\n",
    "6. **Sigmoid Output**: Ensure colors are in [0,1] range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorQuantizationCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for generating optimal color palettes.\n",
    "\n",
    "    This network takes an image and outputs exactly 37 RGB colors that\n",
    "    form the optimal palette for quantizing that specific image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_colors=37):\n",
    "        super(ColorQuantizationCNN, self).__init__()\n",
    "        self.n_colors = n_colors\n",
    "\n",
    "        # Convolutional layers for feature extraction\n",
    "        # Each layer extracts increasingly complex features\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # MaxPooling for dimensionality reduction\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers for color generation\n",
    "        # Note: Input size depends on image size after pooling\n",
    "        # For 128x128 input: 128 * (128/8) * (128/8) = 128 * 16 * 16\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, n_colors * 3)  # 37 colors Ã— 3 channels = 111\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input image tensor (B, H, W, C) in range [0, 255]\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B, n_colors, 3) with colors in range [0, 1]\n",
    "        \"\"\"\n",
    "        # Convert from (B, H, W, C) to (B, C, H, W) - PyTorch format\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Normalize to [0, 1] range\n",
    "        x = x.float() / 255.0\n",
    "\n",
    "        # Convolutional feature extraction\n",
    "        # Each conv-relu-pool reduces spatial size by 2\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 128x128 -> 64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 64x64 -> 32x32\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 32x32 -> 16x16\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (B, 128*16*16)\n",
    "\n",
    "        # Fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output layer with sigmoid to ensure [0,1] range\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        # Reshape to (B, n_colors, 3) for easier handling\n",
    "        x = x.view(x.size(0), self.n_colors, 3)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create our model\n",
    "model = ColorQuantizationCNN(n_colors=37).to(device)\n",
    "print(f\"ðŸ§  Neural network created!\")\n",
    "print(f\"ðŸ“Š Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"ðŸ”¥ Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Test the model with a sample input\n",
    "with torch.no_grad():\n",
    "    # Create a dummy input (batch_size=1, height=128, width=128, channels=3)\n",
    "    dummy_input = torch.randint(0, 256, (1, 128, 128, 3)).to(device)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"âœ… Model test successful!\")\n",
    "    print(f\"   Input shape: {dummy_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output range: [{output.min():.3f}, {output.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸŽ¯ Custom Loss Function\n",
    "\n",
    "The magic happens in our loss function! We need to implement the exact same objective that will be used for evaluation.\n",
    "\n",
    "### Key Components:\n",
    "1. **Quantization Process**: Convert colors to pixels using nearest neighbor\n",
    "2. **MSE Calculation**: Compare original vs quantized image\n",
    "3. **Color Cost**: Distance to RGB cube vertices\n",
    "4. **Unique Colors**: Ensure exactly 37 different colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_with_palette(image, palette):\n",
    "    \"\"\"\n",
    "    Quantize image using the given color palette.\n",
    "\n",
    "    Args:\n",
    "        image: Original image tensor (B, H, W, C) in range [0, 1]\n",
    "        palette: Color palette tensor (B, n_colors, C) in range [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Quantized image tensor with same shape as input\n",
    "    \"\"\"\n",
    "    B, H, W, C = image.shape\n",
    "    _, n_colors, _ = palette.shape\n",
    "\n",
    "    # Reshape image to (B, H*W, C) for easier computation\n",
    "    image_flat = image.view(B, -1, C)\n",
    "\n",
    "    # Compute distance from each pixel to each palette color\n",
    "    # Using broadcasting: (B, H*W, 1, C) - (B, 1, n_colors, C)\n",
    "    distances = torch.norm(image_flat.unsqueeze(2) - palette.unsqueeze(1), dim=3)\n",
    "\n",
    "    # Find closest palette color for each pixel\n",
    "    closest_indices = torch.argmin(distances, dim=2)\n",
    "\n",
    "    # Replace each pixel with its closest palette color\n",
    "    quantized_flat = palette.gather(1, closest_indices.unsqueeze(-1).expand(-1, -1, C))\n",
    "\n",
    "    # Reshape back to original image shape\n",
    "    quantized = quantized_flat.view(B, H, W, C)\n",
    "\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def ensure_unique_colors(palette):\n",
    "    \"\"\"\n",
    "    Ensure all colors in palette are unique.\n",
    "\n",
    "    This is crucial - we need exactly 37 different colors!\n",
    "    \"\"\"\n",
    "    B, n_colors, C = palette.shape\n",
    "\n",
    "    for b in range(B):\n",
    "        # Convert to 8-bit integers for exact comparison\n",
    "        colors_int = (palette[b] * 255).round().int()\n",
    "\n",
    "        seen_colors = set()\n",
    "        for i in range(n_colors):\n",
    "            color_tuple = tuple(colors_int[i].tolist())\n",
    "\n",
    "            # If we've seen this color before, modify it slightly\n",
    "            while color_tuple in seen_colors:\n",
    "                # Modify red channel slightly\n",
    "                colors_int[i, 0] = (colors_int[i, 0] + 1) % 256\n",
    "                color_tuple = tuple(colors_int[i].tolist())\n",
    "\n",
    "            seen_colors.add(color_tuple)\n",
    "\n",
    "        # Convert back to [0, 1] range\n",
    "        palette[b] = colors_int.float() / 255.0\n",
    "\n",
    "    return palette\n",
    "\n",
    "\n",
    "def pytorch_loss_function(original_image, palette):\n",
    "    \"\"\"\n",
    "    PyTorch version of our loss function for training.\n",
    "\n",
    "    This implements the exact same objective as the evaluation:\n",
    "    Loss = 2*MSE + 21*max_color_cost + 42*mean_color_cost\n",
    "    \"\"\"\n",
    "    # Ensure unique colors\n",
    "    palette = ensure_unique_colors(palette.clone())\n",
    "\n",
    "    # Quantize the image\n",
    "    quantized_image = quantize_with_palette(original_image, palette)\n",
    "\n",
    "    # Convert to [0, 255] range for cost calculation\n",
    "    original_255 = (original_image * 255).round()\n",
    "    quantized_255 = (quantized_image * 255).round()\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse_loss = torch.mean((quantized_255 - original_255) ** 2)\n",
    "\n",
    "    # Calculate color costs\n",
    "    # RGB cube vertices\n",
    "    vertices = torch.tensor(\n",
    "        [\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 255],\n",
    "            [0, 255, 0],\n",
    "            [0, 255, 255],\n",
    "            [255, 0, 0],\n",
    "            [255, 0, 255],\n",
    "            [255, 255, 0],\n",
    "            [255, 255, 255],\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Get unique colors from quantized image\n",
    "    quantized_flat = quantized_255.view(-1, 3)\n",
    "    unique_colors = torch.unique(quantized_flat, dim=0)\n",
    "\n",
    "    # Calculate distance to nearest vertex for each unique color\n",
    "    distances = torch.norm(unique_colors.unsqueeze(1) - vertices.unsqueeze(0), dim=2)\n",
    "    min_distances = torch.min(distances, dim=1)[0]\n",
    "\n",
    "    mean_color_cost = torch.mean(min_distances)\n",
    "    max_color_cost = torch.max(min_distances)\n",
    "\n",
    "    # Final loss (same as evaluation function)\n",
    "    total_loss = 2 * mse_loss + 21 * max_color_cost + 42 * mean_color_cost\n",
    "\n",
    "    return total_loss, mse_loss, mean_color_cost, max_color_cost\n",
    "\n",
    "\n",
    "print(\"ðŸŽ¯ Loss functions implemented!\")\n",
    "print(\"   - quantize_with_palette(): Convert image using palette\")\n",
    "print(\"   - ensure_unique_colors(): Guarantee 37 unique colors\")\n",
    "print(\"   - pytorch_loss_function(): Training objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸ’¼ Complete Solution\n",
    "\n",
    "Now let's put it all together! This is the main function that will solve the color quantization problem.\n",
    "\n",
    "### The Training Strategy\n",
    "\n",
    "For each image, we:\n",
    "1. **Create a fresh neural network** (overfitting is good!)\n",
    "2. **Train for many epochs** to find optimal colors\n",
    "3. **Track the best result** during training\n",
    "4. **Return the best quantized image**\n",
    "\n",
    "This approach works because each image has unique characteristics, and we want a personalized solution for each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_image(image, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train a neural network specifically for one image.\n",
    "\n",
    "    This is the core of our approach - individual training for each image!\n",
    "\n",
    "    Args:\n",
    "        image: Input image (numpy array, uint8, HxWx3)\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "\n",
    "    Returns:\n",
    "        Best quantized image (numpy array, uint8, HxWx3)\n",
    "    \"\"\"\n",
    "    # Prepare the image\n",
    "    img_tensor = (\n",
    "        torch.tensor(image, dtype=torch.float32).unsqueeze(0).to(device) / 255.0\n",
    "    )\n",
    "\n",
    "    # Create a fresh model for this image\n",
    "    model = ColorQuantizationCNN(n_colors=37).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Track the best result\n",
    "    best_loss = float(\"inf\")\n",
    "    best_image = None\n",
    "\n",
    "    print(f\"ðŸš€ Training neural network for image...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass: generate palette\n",
    "        palette = model(img_tensor)\n",
    "\n",
    "        # Calculate loss\n",
    "        total_loss, mse_loss, mean_cost, max_cost = pytorch_loss_function(\n",
    "            img_tensor, palette\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert to numpy for evaluation\n",
    "        with torch.no_grad():\n",
    "            palette_clean = ensure_unique_colors(palette.clone())\n",
    "            quantized = quantize_with_palette(img_tensor, palette_clean)\n",
    "            quantized_np = (quantized.squeeze().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "            # Calculate actual score using our numpy functions\n",
    "            actual_score = quantization_score(image, quantized_np)\n",
    "\n",
    "            # Keep track of best result\n",
    "            if actual_score < best_loss:\n",
    "                best_loss = actual_score\n",
    "                best_image = quantized_np.copy()\n",
    "\n",
    "        # Progress updates\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{num_epochs}: Loss = {actual_score:.2f}\")\n",
    "\n",
    "    print(f\"âœ… Training complete! Best score: {best_loss:.2f}\")\n",
    "    return best_image\n",
    "\n",
    "\n",
    "# Test our complete solution on the sample image\n",
    "print(\"ðŸ”¬ Testing complete solution...\")\n",
    "neural_result = train_for_image(sample_img, num_epochs=30, learning_rate=0.001)\n",
    "\n",
    "# Compare with K-means\n",
    "neural_score = quantization_score(sample_img, neural_result)\n",
    "kmeans_score = quantization_score(sample_img, kmeans_result)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Comparison:\")\n",
    "print(f\"   K-means score: {kmeans_score:.2f}\")\n",
    "print(f\"   Neural Net score: {neural_score:.2f}\")\n",
    "print(f\"   ðŸŽ‰ Improvement: {kmeans_score - neural_score:.2f} points!\")\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(sample_img)\n",
    "axes[0].set_title(\"ðŸ–¼ï¸ Original\", fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(kmeans_result)\n",
    "axes[1].set_title(f\"ðŸŽ¨ K-means\\\\nScore: {kmeans_score:.1f}\", fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(neural_result)\n",
    "axes[2].set_title(f\"ðŸ¤– Neural Net\\\\nScore: {neural_score:.1f}\", fontweight=\"bold\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ® Interactive Exercises\n",
    "\n",
    "Now it's your turn to experiment and learn! Try these challenges to deepen your understanding.\n",
    "\n",
    "### ðŸŽ¯ Exercise 1: Experiment with Hyperparameters\n",
    "\n",
    "Try modifying the training parameters and see how they affect the results:\n",
    "\n",
    "1. **Learning Rate**: Try values like 0.01, 0.001, 0.0001\n",
    "2. **Number of Epochs**: Try 10, 50, 100 epochs\n",
    "3. **Network Architecture**: Add/remove layers or change layer sizes\n",
    "\n",
    "Use the cell below to experiment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª Experiment Playground - Try different hyperparameters!\n",
    "\n",
    "\n",
    "# Create a different sample image for experimentation\n",
    "def create_gradient_image(size=128):\n",
    "    \"\"\"Create a smooth gradient image\"\"\"\n",
    "    img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            img[i, j] = [\n",
    "                int(255 * i / size),  # Red gradient\n",
    "                int(255 * j / size),  # Green gradient\n",
    "                int(255 * (i + j) / (2 * size)),  # Blue gradient\n",
    "            ]\n",
    "    return img\n",
    "\n",
    "\n",
    "gradient_img = create_gradient_image(128)\n",
    "\n",
    "# Try different hyperparameters\n",
    "hyperparams_to_try = [\n",
    "    {\"lr\": 0.01, \"epochs\": 20, \"name\": \"High LR, Few Epochs\"},\n",
    "    {\"lr\": 0.001, \"epochs\": 50, \"name\": \"Medium LR, Medium Epochs\"},\n",
    "    {\"lr\": 0.0001, \"epochs\": 30, \"name\": \"Low LR, Few Epochs\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"ðŸ”¬ Testing different hyperparameters...\")\n",
    "\n",
    "for params in hyperparams_to_try:\n",
    "    print(f\"\\nðŸ“Š Testing: {params['name']}\")\n",
    "    result = train_for_image(\n",
    "        gradient_img, num_epochs=params[\"epochs\"], learning_rate=params[\"lr\"]\n",
    "    )\n",
    "    score = quantization_score(gradient_img, result)\n",
    "    results.append((result, score, params[\"name\"]))\n",
    "    print(f\"   Final score: {score:.2f}\")\n",
    "\n",
    "# Visualize all results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(gradient_img)\n",
    "axes[0].set_title(\"ðŸ–¼ï¸ Original Gradient Image\", fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Results from different hyperparameters\n",
    "for i, (result, score, name) in enumerate(results):\n",
    "    axes[i + 1].imshow(result)\n",
    "    axes[i + 1].set_title(f\"ðŸ¤– {name}\\\\nScore: {score:.1f}\", fontweight=\"bold\")\n",
    "    axes[i + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nðŸŽ‰ Experiment complete! Which hyperparameters worked best?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ðŸ“– Summary and Next Steps\n",
    "\n",
    "Congratulations! ðŸŽ‰ You've learned how to use deep learning for color quantization!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **ðŸŽ¨ Color Quantization Fundamentals**:\n",
    "   - Reducing millions of colors to just 37\n",
    "   - Balancing image quality vs. color preferences\n",
    "   - Complex objective functions with multiple terms\n",
    "\n",
    "2. **ðŸ¤– Deep Learning Approach**:\n",
    "   - CNN architecture for color palette generation\n",
    "   - Individual training per image (overfitting as a feature!)\n",
    "   - Direct optimization of the evaluation objective\n",
    "\n",
    "3. **ðŸ’» Implementation Skills**:\n",
    "   - PyTorch CNN design and training\n",
    "   - Custom loss functions for specialized objectives\n",
    "   - Color space manipulations and quantization algorithms\n",
    "\n",
    "4. **ðŸ”¬ Advanced Techniques**:\n",
    "   - Ensuring unique color constraints\n",
    "   - Hyperparameter tuning for optimization\n",
    "   - Comparing neural vs. traditional methods\n",
    "\n",
    "### For the Solution Implementation:\n",
    "\n",
    "You now have all the knowledge to implement the complete solution! The key components are:\n",
    "\n",
    "```python\n",
    "def your_quantization_algorithm(img, n_clusters=37, num_epochs=100, learning_rate=0.0001):\n",
    "    # 1. Create CNN model for this specific image\n",
    "    model = ColorQuantizationCNN(n_clusters).to(device)\n",
    "    \n",
    "    # 2. Train with custom loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 3. Track best result during training\n",
    "    best_score = float('inf')\n",
    "    best_image = None\n",
    "    \n",
    "    # 4. Training loop with our exact objective function\n",
    "    # ... (similar to train_for_image function above)\n",
    "    \n",
    "    return best_image\n",
    "```\n",
    "\n",
    "### ðŸš€ Advanced Topics to Explore:\n",
    "\n",
    "- **Different Network Architectures**: ResNet, DenseNet, Vision Transformers\n",
    "- **Advanced Optimizers**: AdamW, SGD with momentum, learning rate scheduling\n",
    "- **Multi-Image Training**: Learning shared features across image types\n",
    "- **Perceptual Loss Functions**: Using pre-trained networks for better visual quality\n",
    "- **Real-Time Quantization**: Optimizing for speed vs. quality trade-offs\n",
    "\n",
    "### ðŸ“š Useful Resources:\n",
    "\n",
    "- ðŸ“– [Deep Learning for Computer Vision](https://www.deeplearningbook.org/)\n",
    "- ðŸ› ï¸ [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- ðŸ“‘ [Article about the median cut method for color quantization](https://gowtham000.hashnode.dev/median-cut-a-popular-colour-quantization-strategy)\n",
    "- ðŸŽ¨ [Computer Graphics: Color Theory](https://en.wikipedia.org/wiki/Color_theory)\n",
    "\n",
    "**Good luck with your implementation!** ðŸŒŸ\n",
    "\n",
    "Remember: The key insight is that neural networks can directly optimize complex, non-standard objective functions that traditional algorithms struggle with. This makes them perfect for problems like ours where the evaluation criteria are sophisticated and multi-faceted!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SelfStudy OAI",
   "language": "python",
   "name": "selfstudy-oai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
