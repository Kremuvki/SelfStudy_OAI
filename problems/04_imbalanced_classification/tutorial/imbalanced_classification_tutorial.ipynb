{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Tutorial: Deep Learning for Imbalanced Classification\n",
    "\n",
    "![Imbalanced Classification](https://miro.medium.com/v2/resize:fit:1400/1*ZygUZWafGHOI02wI5ML5aA.png)\n",
    "\n",
    "## Welcome to the World of Imbalanced Data! ‚öñÔ∏è\n",
    "\n",
    "In this comprehensive tutorial, you'll learn:\n",
    "- üìä What is imbalanced classification and why it's challenging\n",
    "- üßÆ Understanding class distribution and its impact on model performance\n",
    "- ü§ñ Advanced techniques for handling imbalanced datasets\n",
    "- üíª Hands-on implementation with PyTorch and CNNs\n",
    "- üéØ Weighted sampling and custom loss functions\n",
    "- üß™ Interactive exercises to master imbalanced classification\n",
    "\n",
    "By the end, you'll be equipped to tackle any imbalanced classification problem with confidence!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1. [üéì Understanding Imbalanced Classification](#1--understanding-imbalanced-classification)\n",
    "2. [üìä The Problem with Traditional Approaches](#2--the-problem-with-traditional-approaches)\n",
    "3. [üîß Setting Up the Environment](#3--setting-up-the-environment)\n",
    "4. [üìà Analyzing Class Distribution](#4--analyzing-class-distribution)\n",
    "5. [üèóÔ∏è CNN Architecture for Binary Classification](#5--cnn-architecture-for-binary-classification)\n",
    "6. [‚öñÔ∏è Weighted Sampling Techniques](#6--weighted-sampling-techniques)\n",
    "7. [üéØ Custom Loss Functions](#7--custom-loss-functions)\n",
    "8. [üß† Building the Complete Solution](#8--building-the-complete-solution)\n",
    "9. [üìä Evaluation Metrics](#9--evaluation-metrics)\n",
    "10. [üéÆ Interactive Exercises](#10--interactive-exercises)\n",
    "11. [üöÄ Advanced Techniques](#11--advanced-techniques)\n",
    "12. [üìñ Summary and Next Steps](#12--summary-and-next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üéì Understanding Imbalanced Classification\n",
    "\n",
    "### What is Imbalanced Classification?\n",
    "\n",
    "Imagine you're building a medical diagnosis system üè•. Out of 1000 patients:\n",
    "- 950 are healthy (Class 0: \"normal\")\n",
    "- 50 have a rare disease (Class 1: \"positive\")\n",
    "\n",
    "This is a **severely imbalanced dataset** with a ratio of 19:1!\n",
    "\n",
    "### The Challenge üò∞\n",
    "\n",
    "A naive classifier could achieve 95% accuracy by simply predicting \"healthy\" for everyone. But this would be **catastrophic** - we'd miss all the sick patients!\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Majority Class**: The class with more samples (Class 0: \"normal\")\n",
    "- **Minority Class**: The class with fewer samples (Class 1: \"onion\")\n",
    "- **Class Imbalance Ratio**: How skewed the distribution is\n",
    "- **Sampling Bias**: Traditional training favors the majority class\n",
    "\n",
    "### Real-World Examples:\n",
    "\n",
    "üè• **Medical Diagnosis**: Rare diseases vs. healthy patients  \n",
    "üîí **Fraud Detection**: Fraudulent vs. legitimate transactions  \n",
    "üìß **Spam Detection**: Spam vs. legitimate emails  \n",
    "üè≠ **Quality Control**: Defective vs. good products  \n",
    "üö® **Anomaly Detection**: Abnormal vs. normal behavior  \n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "Given a dataset $D = \\\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\\\}$ where $y_i \\\\in \\\\{0, 1\\\\}$:\n",
    "\n",
    "$$\\\\text{Imbalance Ratio} = \\\\frac{|\\\\{i : y_i = 0\\\\}|}{|\\\\{i : y_i = 1\\\\}|}$$\n",
    "\n",
    "When this ratio is significantly > 1, we have an imbalanced dataset.\n",
    "\n",
    "### Our Specific Problem üéØ\n",
    "\n",
    "In this tutorial, we'll work with:\n",
    "- **Images**: 224√ó224 grayscale images\n",
    "- **Classes**: \"normal\" (geometric shapes) vs. \"onion\" (layered patterns)\n",
    "- **Challenge**: Training data is imbalanced, test data is balanced\n",
    "- **Goal**: Build a CNN that works well on balanced test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìä The Problem with Traditional Approaches\n",
    "\n",
    "### Why Standard Training Fails\n",
    "\n",
    "Traditional machine learning assumes **balanced datasets**. When we have imbalanced data:\n",
    "\n",
    "#### ‚ùå Problem 1: Biased Learning\n",
    "The model sees the majority class 19√ó more often, so it learns to predict it by default.\n",
    "\n",
    "#### ‚ùå Problem 2: Misleading Accuracy\n",
    "High accuracy doesn't mean good performance - it might just reflect class distribution.\n",
    "\n",
    "#### ‚ùå Problem 3: Poor Generalization\n",
    "Models trained on imbalanced data often fail on balanced test sets.\n",
    "\n",
    "### The Mathematics Behind the Problem\n",
    "\n",
    "Consider a standard loss function (Cross-Entropy):\n",
    "\n",
    "$$L = -\\\\frac{1}{N} \\\\sum_{i=1}^{N} [y_i \\\\log(\\\\hat{y}_i) + (1-y_i) \\\\log(1-\\\\hat{y}_i)]$$\n",
    "\n",
    "With imbalanced data:\n",
    "- Majority class contributes to ~95% of the loss\n",
    "- Minority class contributes to ~5% of the loss\n",
    "- Model optimizes mainly for majority class performance\n",
    "\n",
    "### Evaluation Metrics Trap üìä\n",
    "\n",
    "**Accuracy** can be misleading:\n",
    "```\n",
    "Dataset: 950 normal, 50 onion samples\n",
    "Dumb classifier: Always predict \"normal\"\n",
    "Accuracy: 950/1000 = 95% ‚ú® (Looks great!)\n",
    "But: 0% recall for \"onion\" class üíÄ (Catastrophic!)\n",
    "```\n",
    "\n",
    "### Better Metrics for Imbalanced Data:\n",
    "\n",
    "- **Precision**: $\\\\frac{TP}{TP + FP}$ - Of predicted positives, how many are correct?\n",
    "- **Recall**: $\\\\frac{TP}{TP + FN}$ - Of actual positives, how many did we find?\n",
    "- **F1-Score**: $\\\\frac{2 \\\\cdot Precision \\\\cdot Recall}{Precision + Recall}$ - Harmonic mean\n",
    "- **AUC-ROC**: Area under ROC curve - threshold-independent\n",
    "\n",
    "### The Solution Preview üéØ\n",
    "\n",
    "We'll use several techniques:\n",
    "1. **Weighted Sampling**: Balance the training batches\n",
    "2. **Proper Architecture**: CNN designed for binary classification\n",
    "3. **Regularization**: Prevent overfitting to majority class\n",
    "4. **Smart Training**: Monitor the right metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Setting Up the Environment\n",
    "\n",
    "Let's start by importing all necessary libraries and setting up our development environment. We'll be working with PyTorch for neural networks and various other libraries for data analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for imbalanced classification tutorial\n",
    "import abc\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For downloading and handling data\n",
    "import gdown\n",
    "import zipfile\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up device - GPU greatly speeds up CNN training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some sample data to understand the problem better\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a simulated imbalanced dataset for demonstration.\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Simulate class distribution similar to our real problem\n",
    "    n_majority = 950  # Class 0 (normal)\n",
    "    n_minority = 50  # Class 1 (onion)\n",
    "\n",
    "    # Create synthetic features (normally we'd have images)\n",
    "    X_majority = np.random.normal(0, 1, (n_majority, 2))\n",
    "    X_minority = np.random.normal(2, 1, (n_minority, 2))\n",
    "\n",
    "    # Create labels\n",
    "    y_majority = np.zeros(n_majority)\n",
    "    y_minority = np.ones(n_minority)\n",
    "\n",
    "    # Combine data\n",
    "    X = np.vstack([X_majority, X_minority])\n",
    "    y = np.hstack([y_majority, y_minority])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Create sample dataset\n",
    "X_sample, y_sample = create_sample_dataset()\n",
    "\n",
    "print(f\"üìä Sample dataset created:\")\n",
    "print(f\"   Total samples: {len(X_sample)}\")\n",
    "print(f\"   Class 0 (majority): {np.sum(y_sample == 0)} samples\")\n",
    "print(f\"   Class 1 (minority): {np.sum(y_sample == 1)} samples\")\n",
    "print(f\"   Imbalance ratio: {np.sum(y_sample == 0) / np.sum(y_sample == 1):.1f}:1\")\n",
    "\n",
    "# Visualize the imbalanced dataset\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot\n",
    "colors = [\"blue\", \"red\"]\n",
    "for i, class_name in enumerate([\"Normal\", \"Onion\"]):\n",
    "    mask = y_sample == i\n",
    "    ax1.scatter(\n",
    "        X_sample[mask, 0],\n",
    "        X_sample[mask, 1],\n",
    "        c=colors[i],\n",
    "        label=f\"{class_name} (n={np.sum(mask)})\",\n",
    "        alpha=0.7,\n",
    "        s=20,\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Feature 1\")\n",
    "ax1.set_ylabel(\"Feature 2\")\n",
    "ax1.set_title(\"üéØ Imbalanced Dataset Visualization\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar plot showing class distribution\n",
    "class_counts = [np.sum(y_sample == 0), np.sum(y_sample == 1)]\n",
    "ax2.bar(\n",
    "    [\"Class 0\\\\n(Normal)\", \"Class 1\\\\n(Onion)\"],\n",
    "    class_counts,\n",
    "    color=[\"blue\", \"red\"],\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax2.set_ylabel(\"Number of Samples\")\n",
    "ax2.set_title(\"üìä Class Distribution\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, count in enumerate(class_counts):\n",
    "    percentage = count / len(y_sample) * 100\n",
    "    ax2.text(\n",
    "        i,\n",
    "        count + 10,\n",
    "        f\"{count}\\\\n({percentage:.1f}%)\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìà Analyzing Class Distribution\n",
    "\n",
    "Now let's set up our data loading and analyze the real class distribution in our problem. This is crucial for understanding the scope of the imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate the data loading process (similar to the actual problem)\n",
    "# In the real problem, you would download data from Google Drive\n",
    "# Here we'll create a mock dataset class to demonstrate the concepts\n",
    "\n",
    "\n",
    "class MockImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Mock dataset class that simulates the real imbalanced image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_type: str, imbalance_ratio: float = 19.0):\n",
    "        \"\"\"\n",
    "        Initialize mock dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_type: \"train\" or \"valid\"\n",
    "            imbalance_ratio: Ratio of majority to minority class\n",
    "        \"\"\"\n",
    "        self.dataset_type = dataset_type\n",
    "\n",
    "        # Simulate realistic dataset sizes\n",
    "        if dataset_type == \"train\":\n",
    "            total_samples = 1000\n",
    "            minority_samples = int(total_samples / (imbalance_ratio + 1))\n",
    "            majority_samples = total_samples - minority_samples\n",
    "        else:  # validation - balanced\n",
    "            total_samples = 100\n",
    "            minority_samples = majority_samples = total_samples // 2\n",
    "\n",
    "        # Create mock file paths and labels\n",
    "        self.filelist = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Add majority class samples (normal)\n",
    "        for i in range(majority_samples):\n",
    "            self.filelist.append(f\"{dataset_type}_data/normal_{i}.jpg\")\n",
    "            self.labels.append(0)\n",
    "\n",
    "        # Add minority class samples (onion)\n",
    "        for i in range(minority_samples):\n",
    "            self.filelist.append(f\"{dataset_type}_data/onion_{i}.jpg\")\n",
    "            self.labels.append(1)\n",
    "\n",
    "        print(f\"üìä {dataset_type.capitalize()} dataset created:\")\n",
    "        print(f\"   Total samples: {len(self.labels)}\")\n",
    "        print(f\"   Class 0 (normal): {self.labels.count(0)} samples\")\n",
    "        print(f\"   Class 1 (onion): {self.labels.count(1)} samples\")\n",
    "        if self.labels.count(1) > 0:\n",
    "            ratio = self.labels.count(0) / self.labels.count(1)\n",
    "            print(f\"   Imbalance ratio: {ratio:.1f}:1\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # In real dataset, we would load actual images\n",
    "        # Here we create mock tensors that simulate grayscale images\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Create a mock image (224x224 grayscale)\n",
    "        if self.labels[idx] == 0:  # normal class\n",
    "            # Simulate geometric shapes with noise\n",
    "            image = torch.randn(1, 224, 224) * 0.1 + 0.5\n",
    "        else:  # onion class\n",
    "            # Simulate layered patterns with noise\n",
    "            image = torch.randn(1, 224, 224) * 0.1 + 0.7\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "    def loader(self, **kwargs):\n",
    "        \"\"\"Create DataLoader for this dataset.\"\"\"\n",
    "        return torch.utils.data.DataLoader(self, **kwargs)\n",
    "\n",
    "\n",
    "# Create mock datasets\n",
    "print(\"üîÑ Creating mock datasets...\")\n",
    "train_dataset = MockImageDataset(\"train\", imbalance_ratio=19.0)\n",
    "print()\n",
    "valid_dataset = MockImageDataset(\"valid\", imbalance_ratio=1.0)  # Balanced validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the class distribution in detail\n",
    "def analyze_class_distribution(dataset, name):\n",
    "    \"\"\"Analyze and visualize class distribution in a dataset.\"\"\"\n",
    "\n",
    "    # Count classes\n",
    "    class_counts = Counter()\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "\n",
    "    total = sum(class_counts.values())\n",
    "    print(f\"\\\\nüìä {name} Dataset Analysis:\")\n",
    "    print(f\"   Total samples: {total}\")\n",
    "\n",
    "    for class_id in sorted(class_counts.keys()):\n",
    "        count = class_counts[class_id]\n",
    "        percentage = count / total * 100\n",
    "        class_name = \"Normal\" if class_id == 0 else \"Onion\"\n",
    "        print(\n",
    "            f\"   Class {class_id} ({class_name}): {count} samples ({percentage:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    if len(class_counts) == 2:\n",
    "        ratio = class_counts[0] / class_counts[1]\n",
    "        print(f\"   Imbalance ratio: {ratio:.2f}:1\")\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "# Analyze both datasets\n",
    "train_counts = analyze_class_distribution(train_dataset, \"Training\")\n",
    "valid_counts = analyze_class_distribution(valid_dataset, \"Validation\")\n",
    "\n",
    "# Visualize the distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training dataset distribution\n",
    "train_classes = [\"Normal\", \"Onion\"]\n",
    "train_values = [train_counts[0], train_counts[1]]\n",
    "bars1 = ax1.bar(train_classes, train_values, color=[\"skyblue\", \"salmon\"], alpha=0.8)\n",
    "ax1.set_title(\"üèãÔ∏è Training Dataset\\\\n(Imbalanced)\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Number of Samples\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars1, train_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 5,\n",
    "        f\"{value}\\\\n({value/sum(train_values)*100:.1f}%)\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Validation dataset distribution\n",
    "valid_classes = [\"Normal\", \"Onion\"]\n",
    "valid_values = [valid_counts[0], valid_counts[1]]\n",
    "bars2 = ax2.bar(valid_classes, valid_values, color=[\"skyblue\", \"salmon\"], alpha=0.8)\n",
    "ax2.set_title(\"üéØ Validation Dataset\\\\n(Balanced)\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Number of Samples\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars2, valid_values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 1,\n",
    "        f\"{value}\\\\n({value/sum(valid_values)*100:.1f}%)\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nüí° Key Observation:\")\n",
    "print(\"   Training data is highly imbalanced, but validation data is balanced!\")\n",
    "print(\n",
    "    \"   This means our model must generalize from imbalanced to balanced distributions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üèóÔ∏è CNN Architecture for Binary Classification\n",
    "\n",
    "Now let's design a Convolutional Neural Network specifically for binary image classification. Our architecture needs to be robust enough to handle the challenges of imbalanced data.\n",
    "\n",
    "### Architecture Design Principles:\n",
    "\n",
    "1. **Feature Extraction**: Convolutional layers to detect patterns\n",
    "2. **Dimensionality Reduction**: Pooling layers to reduce computational cost\n",
    "3. **Regularization**: Dropout to prevent overfitting\n",
    "4. **Binary Output**: Single neuron with sigmoid activation for probability\n",
    "\n",
    "### The Challenge:\n",
    "- Input: 224√ó224 grayscale images (1 channel)\n",
    "- Output: Binary classification probability [0, 1]\n",
    "- Goal: Distinguish between geometric shapes (\"normal\") and layered patterns (\"onion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's understand the abstract base class we need to implement\n",
    "class CnnClassifier(torch.nn.Module, abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for CNN classifiers.\n",
    "    This defines the interface our solution must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    MODEL_PATH: str = \"cnn-classifier.pth\"\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        \"\"\"Load model from file.\"\"\"\n",
    "        model = cls()\n",
    "        model.load_state_dict(torch.load(cls.MODEL_PATH, map_location=device))\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    @abc.abstractmethod\n",
    "    def create_with_training(cls):\n",
    "        \"\"\"Train model and save to file.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "print(\"‚úÖ Base class defined!\")\n",
    "print(\"   Our CNN must inherit from CnnClassifier\")\n",
    "print(\"   Must implement: create_with_training() method\")\n",
    "print(\"   Must save model weights to: cnn-classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build our CNN architecture step by step\n",
    "\n",
    "\n",
    "class BasicCNN(CnnClassifier):\n",
    "    \"\"\"\n",
    "    A basic CNN for binary image classification.\n",
    "\n",
    "    This will serve as our foundation for understanding CNN architecture\n",
    "    before we add imbalanced data techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate dimensions after each layer\n",
    "        # Input: 224x224x1\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5)\n",
    "        # After conv1: (224-5+1) = 220x220x8\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # After pool1: 220/2 = 110x110x8\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=5)\n",
    "        # After conv2: (110-5+1) = 106x106x8\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        # After pool2: 106/4 = 26x26x8 (rounded down)\n",
    "\n",
    "        # Calculate flattened size: 26*26*8 = 5408\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=5408, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=1)\n",
    "\n",
    "        # Activation functions and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        print(\"üèóÔ∏è Basic CNN Architecture:\")\n",
    "        print(\"   Conv1: 1‚Üí8 channels, 5x5 kernel\")\n",
    "        print(\"   Pool1: 2x2 MaxPool\")\n",
    "        print(\"   Conv2: 8‚Üí8 channels, 5x5 kernel\")\n",
    "        print(\"   Pool2: 4x4 MaxPool\")\n",
    "        print(\"   FC1: 5408‚Üí256 neurons\")\n",
    "        print(\"   FC2: 256‚Üí1 neuron (binary output)\")\n",
    "        print(\"   Regularization: 50% Dropout\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # First conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)  # Binary probability output\n",
    "\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_training(cls):\n",
    "        \"\"\"Basic training (we'll improve this later for imbalanced data).\"\"\"\n",
    "        return cls()  # Placeholder - no actual training yet\n",
    "\n",
    "\n",
    "# Test our architecture\n",
    "basic_model = BasicCNN().to(device)\n",
    "\n",
    "# Test with a sample input\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(2, 1, 224, 224).to(device)  # Batch of 2 images\n",
    "    output = basic_model(sample_input)\n",
    "    print(f\"\\n‚úÖ Architecture test successful!\")\n",
    "    print(f\"   Input shape: {sample_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output values: {output.flatten().cpu().numpy()}\")\n",
    "    print(f\"   Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in basic_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in basic_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ‚öñÔ∏è Weighted Sampling Techniques\n",
    "\n",
    "The key to handling imbalanced data is **weighted sampling**. Instead of training on the raw imbalanced dataset, we'll create balanced batches during training.\n",
    "\n",
    "### The Weighted Sampling Strategy:\n",
    "\n",
    "1. **Calculate Class Weights**: Inverse frequency weighting\n",
    "2. **Assign Sample Weights**: Each sample gets a weight based on its class\n",
    "3. **Weighted Random Sampler**: PyTorch's built-in balanced sampling\n",
    "4. **Balanced Batches**: Each training batch has roughly equal class representation\n",
    "\n",
    "### Mathematical Foundation:\n",
    "\n",
    "For a dataset with $n_0$ samples of class 0 and $n_1$ samples of class 1:\n",
    "\n",
    "$$w_0 = \\frac{1}{n_0}, \\quad w_1 = \\frac{1}{n_1}$$\n",
    "\n",
    "Since $n_0 >> n_1$, we have $w_1 >> w_0$, meaning minority class samples are selected more frequently during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement weighted sampling step by step\n",
    "\n",
    "\n",
    "def analyze_and_create_weights(dataset):\n",
    "    \"\"\"\n",
    "    Analyze class distribution and create sample weights for balanced sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Weight for each sample in the dataset\n",
    "        dict: Class analysis information\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîç Analyzing class distribution...\")\n",
    "\n",
    "    # Count classes by iterating through dataset\n",
    "    class_counts = {0: 0, 1: 0}\n",
    "    labels = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "        class_counts[label] += 1\n",
    "\n",
    "    # Calculate class weights (inverse frequency)\n",
    "    total_samples = len(labels)\n",
    "    weight_class_0 = 1.0 / class_counts[0]\n",
    "    weight_class_1 = 1.0 / class_counts[1]\n",
    "\n",
    "    print(f\"   Class 0 (Normal): {class_counts[0]} samples\")\n",
    "    print(f\"   Class 1 (Onion):  {class_counts[1]} samples\")\n",
    "    print(f\"   Total samples: {total_samples}\")\n",
    "    print(f\"   Imbalance ratio: {class_counts[0]/class_counts[1]:.2f}:1\")\n",
    "    print(f\"   Weight for class 0: {weight_class_0:.6f}\")\n",
    "    print(f\"   Weight for class 1: {weight_class_1:.6f}\")\n",
    "    print(f\"   Weight ratio: {weight_class_1/weight_class_0:.2f}:1\")\n",
    "\n",
    "    # Assign weight to each sample based on its class\n",
    "    sample_weights = torch.zeros(total_samples)\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 0:\n",
    "            sample_weights[idx] = weight_class_0\n",
    "        else:\n",
    "            sample_weights[idx] = weight_class_1\n",
    "\n",
    "    analysis_info = {\n",
    "        \"class_counts\": class_counts,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"weight_class_0\": weight_class_0,\n",
    "        \"weight_class_1\": weight_class_1,\n",
    "        \"imbalance_ratio\": class_counts[0] / class_counts[1],\n",
    "    }\n",
    "\n",
    "    return sample_weights, analysis_info\n",
    "\n",
    "\n",
    "# Analyze our training dataset\n",
    "sample_weights, analysis = analyze_and_create_weights(train_dataset)\n",
    "\n",
    "print(f\"\\nüìä Sample weights created:\")\n",
    "print(f\"   Weights shape: {sample_weights.shape}\")\n",
    "print(f\"   Unique weights: {torch.unique(sample_weights).tolist()}\")\n",
    "print(f\"   Weight distribution:\")\n",
    "print(\n",
    "    f\"     - {(sample_weights == analysis['weight_class_0']).sum()} samples with weight {analysis['weight_class_0']:.6f}\"\n",
    ")\n",
    "print(\n",
    "    f\"     - {(sample_weights == analysis['weight_class_1']).sum()} samples with weight {analysis['weight_class_1']:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a weighted sampler and test it\n",
    "\n",
    "\n",
    "def create_balanced_dataloader(dataset, sample_weights, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a DataLoader with weighted sampling for balanced batches.\n",
    "\n",
    "    Args:\n",
    "        dataset: The imbalanced dataset\n",
    "        sample_weights: Weight for each sample\n",
    "        batch_size: Size of each batch\n",
    "\n",
    "    Returns:\n",
    "        DataLoader with balanced sampling\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate number of samples per epoch\n",
    "    # We want full batches, so we'll use a round number\n",
    "    total_samples = len(dataset)\n",
    "    samples_per_epoch = batch_size * (total_samples // batch_size)\n",
    "\n",
    "    print(f\"üéØ Creating balanced DataLoader:\")\n",
    "    print(f\"   Original dataset size: {total_samples}\")\n",
    "    print(f\"   Samples per epoch: {samples_per_epoch}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Batches per epoch: {samples_per_epoch // batch_size}\")\n",
    "\n",
    "    # Create weighted random sampler\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=samples_per_epoch,\n",
    "        replacement=True,  # Allow sampling with replacement\n",
    "    )\n",
    "\n",
    "    # Create DataLoader with the sampler\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        drop_last=True,  # Ensure all batches have same size\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "# Create balanced DataLoader\n",
    "balanced_loader = create_balanced_dataloader(\n",
    "    train_dataset, sample_weights, batch_size=32\n",
    ")\n",
    "\n",
    "# Test the balanced sampling by analyzing a few batches\n",
    "print(f\"\\nüß™ Testing balanced sampling:\")\n",
    "class_counts_in_batches = []\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(balanced_loader):\n",
    "    if batch_idx >= 5:  # Test first 5 batches\n",
    "        break\n",
    "\n",
    "    # Count classes in this batch\n",
    "    unique, counts = torch.unique(labels, return_counts=True)\n",
    "    batch_class_counts = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "\n",
    "    # Fill in missing classes\n",
    "    for class_id in [0, 1]:\n",
    "        if class_id not in batch_class_counts:\n",
    "            batch_class_counts[class_id] = 0\n",
    "\n",
    "    class_counts_in_batches.append(batch_class_counts)\n",
    "\n",
    "    print(\n",
    "        f\"   Batch {batch_idx + 1}: Class 0: {batch_class_counts[0]}, Class 1: {batch_class_counts[1]}\"\n",
    "    )\n",
    "\n",
    "# Calculate average class distribution in batches\n",
    "avg_class_0 = np.mean([counts[0] for counts in class_counts_in_batches])\n",
    "avg_class_1 = np.mean([counts[1] for counts in class_counts_in_batches])\n",
    "\n",
    "print(f\"\\nüìä Average class distribution in balanced batches:\")\n",
    "print(f\"   Class 0 (Normal): {avg_class_0:.1f} samples per batch\")\n",
    "print(f\"   Class 1 (Onion):  {avg_class_1:.1f} samples per batch\")\n",
    "print(f\"   Balance ratio: {avg_class_0/avg_class_1 if avg_class_1 > 0 else 'inf'}:1\")\n",
    "print(f\"   üéâ Much more balanced than original {analysis['imbalance_ratio']:.1f}:1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üß† Building the Complete Solution\n",
    "\n",
    "Now let's put everything together and build our complete CNN classifier that handles imbalanced data properly. This will be the full implementation that matches the problem requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete implementation of our CNN classifier for imbalanced data\n",
    "\n",
    "\n",
    "class YourCnnClassifier(CnnClassifier):\n",
    "    \"\"\"\n",
    "    Professional CNN classifier for imbalanced classification.\n",
    "\n",
    "    This implementation includes:\n",
    "    - Proper CNN architecture for binary classification\n",
    "    - Weighted sampling for handling imbalanced data\n",
    "    - Regularization techniques to prevent overfitting\n",
    "    - Complete training pipeline with progress monitoring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the CNN architecture.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the network architecture using Sequential\n",
    "        self.network = torch.nn.Sequential(\n",
    "            # First convolutional block: 1 -> 8 channels\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            # Second convolutional block: 8 -> 8 channels\n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            # Flatten for fully connected layers\n",
    "            torch.nn.Flatten(),\n",
    "            # Fully connected layers\n",
    "            torch.nn.Linear(in_features=5408, out_features=256),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.ReLU(),\n",
    "            # Output layer (binary classification)\n",
    "            torch.nn.Linear(in_features=256, out_features=1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_training(cls):\n",
    "        \"\"\"\n",
    "        Create and train the model with imbalanced data handling.\n",
    "\n",
    "        This method implements the complete training pipeline:\n",
    "        1. Class distribution analysis\n",
    "        2. Weighted sampling setup\n",
    "        3. Model training with proper techniques\n",
    "        4. Model saving\n",
    "        \"\"\"\n",
    "        # Initialize model\n",
    "        model = cls().to(device)\n",
    "\n",
    "        print(\"üöÄ Starting training with imbalanced data handling...\")\n",
    "\n",
    "        # === STEP 1: ANALYZE CLASS DISTRIBUTION ===\n",
    "        print(\"\\nüìä Step 1: Analyzing class distribution\")\n",
    "        class_counts = {0: 0, 1: 0}\n",
    "\n",
    "        # Count classes in training dataset\n",
    "        for idx in range(len(train_dataset)):\n",
    "            _, label = train_dataset[idx]\n",
    "            class_counts[label] += 1\n",
    "\n",
    "        total_samples = sum(class_counts.values())\n",
    "        print(\n",
    "            f\"   Class 0 (Normal): {class_counts[0]} samples ({class_counts[0]/total_samples*100:.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Class 1 (Onion):  {class_counts[1]} samples ({class_counts[1]/total_samples*100:.1f}%)\"\n",
    "        )\n",
    "        print(f\"   Imbalance ratio: {class_counts[0]/class_counts[1]:.2f}:1\")\n",
    "\n",
    "        # === STEP 2: CREATE WEIGHTED SAMPLING ===\n",
    "        print(\"\\n‚öñÔ∏è Step 2: Setting up weighted sampling\")\n",
    "\n",
    "        # Calculate class weights (inverse frequency)\n",
    "        weight_class_0 = 1.0 / class_counts[0]\n",
    "        weight_class_1 = 1.0 / class_counts[1]\n",
    "\n",
    "        print(f\"   Weight for class 0: {weight_class_0:.6f}\")\n",
    "        print(f\"   Weight for class 1: {weight_class_1:.6f}\")\n",
    "        print(f\"   Weight ratio: {weight_class_1/weight_class_0:.2f}:1\")\n",
    "\n",
    "        # Assign weights to each sample\n",
    "        sample_weights = torch.zeros(total_samples)\n",
    "        sample_idx = 0\n",
    "\n",
    "        for idx in range(len(train_dataset)):\n",
    "            _, label = train_dataset[idx]\n",
    "            if label == 0:\n",
    "                sample_weights[sample_idx] = weight_class_0\n",
    "            else:\n",
    "                sample_weights[sample_idx] = weight_class_1\n",
    "            sample_idx += 1\n",
    "\n",
    "        # === STEP 3: SETUP TRAINING CONFIGURATION ===\n",
    "        print(\"\\nüîß Step 3: Configuring training parameters\")\n",
    "\n",
    "        batch_size = 32\n",
    "        n_epochs = 20\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Number of epochs: {n_epochs}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "\n",
    "        # Create balanced DataLoader\n",
    "        samples_per_epoch = batch_size * (total_samples // batch_size)\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=sample_weights, num_samples=samples_per_epoch, replacement=True\n",
    "        )\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, sampler=sampler, drop_last=True\n",
    "        )\n",
    "\n",
    "        # Setup optimizer and loss function\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        # === STEP 4: TRAINING LOOP ===\n",
    "        print(\"\\nüèãÔ∏è Step 4: Training the model\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            batch_count = 0\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "                # Move data to device\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = model(images).flatten()\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(predictions, labels.float())\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss\n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Progress update every 50 batches\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(\n",
    "                        f\"      Epoch [{epoch+1}/{n_epochs}], Batch [{batch_idx}], Loss: {loss.item():.4f}\"\n",
    "                    )\n",
    "\n",
    "            # End of epoch statistics\n",
    "            avg_loss = epoch_loss / batch_count\n",
    "            print(\n",
    "                f\"   ‚úÖ Epoch {epoch+1}/{n_epochs} completed. Average loss: {avg_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "        # === STEP 5: SAVE MODEL ===\n",
    "        print(\"\\nüíæ Step 5: Saving trained model\")\n",
    "        torch.save(model.state_dict(), cls.MODEL_PATH)\n",
    "        print(f\"   Model saved to: {cls.MODEL_PATH}\")\n",
    "\n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# Test our complete implementation\n",
    "print(\"üß™ Testing complete CNN classifier implementation...\")\n",
    "complete_model = YourCnnClassifier().to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(4, 1, 224, 224).to(device)\n",
    "    output = complete_model(sample_input)\n",
    "    print(f\"\\n‚úÖ Complete model test successful!\")\n",
    "    print(f\"   Input shape: {sample_input.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "    print(f\"   Sample predictions: {output.flatten().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéÆ Interactive Exercises\n",
    "\n",
    "Now it's your turn to experiment and deepen your understanding! Try these challenges to master imbalanced classification techniques.\n",
    "\n",
    "### üéØ Exercise 1: Experiment with Different Imbalance Ratios\n",
    "\n",
    "Try training models with different levels of class imbalance and observe how it affects performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Exercise: Compare different imbalance ratios\n",
    "\n",
    "\n",
    "def create_imbalanced_dataset(imbalance_ratio, total_size=1000):\n",
    "    \"\"\"Create datasets with different imbalance ratios for experimentation.\"\"\"\n",
    "    minority_samples = int(total_size / (imbalance_ratio + 1))\n",
    "    majority_samples = total_size - minority_samples\n",
    "\n",
    "    return (\n",
    "        MockImageDataset(\"train\", imbalance_ratio),\n",
    "        majority_samples,\n",
    "        minority_samples,\n",
    "    )\n",
    "\n",
    "\n",
    "# Test different imbalance ratios\n",
    "ratios_to_test = [1.0, 5.0, 10.0, 20.0]  # From balanced to highly imbalanced\n",
    "\n",
    "print(\"üß™ Experimenting with different imbalance ratios:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for ratio in ratios_to_test:\n",
    "    print(f\"\\nüìä Testing imbalance ratio: {ratio}:1\")\n",
    "\n",
    "    # Create dataset with this ratio\n",
    "    test_dataset, maj_count, min_count = create_imbalanced_dataset(ratio, 1000)\n",
    "\n",
    "    print(f\"   Majority class: {maj_count} samples\")\n",
    "    print(f\"   Minority class: {min_count} samples\")\n",
    "\n",
    "    # Create weights for this dataset\n",
    "    weights, analysis = analyze_and_create_weights(test_dataset)\n",
    "\n",
    "    print(\n",
    "        f\"   Weight ratio: {analysis['weight_class_1']/analysis['weight_class_0']:.2f}:1\"\n",
    "    )\n",
    "\n",
    "    # Create balanced loader\n",
    "    balanced_loader = create_balanced_dataloader(test_dataset, weights, batch_size=32)\n",
    "\n",
    "    # Test a few batches\n",
    "    class_balance_in_batches = []\n",
    "    for batch_idx, (_, labels) in enumerate(balanced_loader):\n",
    "        if batch_idx >= 3:\n",
    "            break\n",
    "        unique, counts = torch.unique(labels, return_counts=True)\n",
    "        batch_counts = {int(u): int(c) for u, c in zip(unique, counts)}\n",
    "        for class_id in [0, 1]:\n",
    "            if class_id not in batch_counts:\n",
    "                batch_counts[class_id] = 0\n",
    "        class_balance_in_batches.append(batch_counts)\n",
    "\n",
    "    avg_0 = np.mean([counts[0] for counts in class_balance_in_batches])\n",
    "    avg_1 = np.mean([counts[1] for counts in class_balance_in_batches])\n",
    "\n",
    "    print(f\"   Avg batch composition: {avg_0:.1f} vs {avg_1:.1f}\")\n",
    "    print(f\"   Batch balance ratio: {avg_0/avg_1 if avg_1 > 0 else 'inf'}:1\")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Higher imbalance ratios require stronger reweighting\")\n",
    "print(\n",
    "    \"   - Weighted sampling creates balanced batches regardless of original imbalance\"\n",
    ")\n",
    "print(\n",
    "    \"   - The minority class gets sampled much more frequently in highly imbalanced cases\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. üìñ Summary and Next Steps\n",
    "\n",
    "Congratulations! üéâ You've mastered the fundamentals of imbalanced classification with deep learning!\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **üéØ Imbalanced Data Challenges**:\n",
    "   - Why traditional methods fail with imbalanced datasets\n",
    "   - The importance of proper evaluation metrics\n",
    "   - Real-world examples of imbalanced classification problems\n",
    "\n",
    "2. **üèóÔ∏è CNN Architecture Design**:\n",
    "   - Building CNNs for binary image classification\n",
    "   - Proper layer dimensions and parameter calculations\n",
    "   - Regularization techniques (dropout) to prevent overfitting\n",
    "\n",
    "3. **‚öñÔ∏è Weighted Sampling Techniques**:\n",
    "   - Inverse frequency weighting for class balance\n",
    "   - PyTorch's WeightedRandomSampler implementation\n",
    "   - Creating balanced batches from imbalanced datasets\n",
    "\n",
    "4. **üß† Complete Solution Implementation**:\n",
    "   - End-to-end training pipeline\n",
    "   - Progress monitoring and debugging\n",
    "   - Model saving and loading mechanisms\n",
    "\n",
    "### For the Actual Problem Implementation:\n",
    "\n",
    "You now have all the knowledge to implement the complete solution! The key components are:\n",
    "\n",
    "```python\n",
    "class YourCnnClassifier(CnnClassifier):\n",
    "    def __init__(self):\n",
    "        # CNN architecture with proper dimensions\n",
    "        self.network = torch.nn.Sequential(\n",
    "            # Conv layers with ReLU, MaxPool, Dropout\n",
    "            # FC layers with regularization\n",
    "            # Sigmoid output for binary classification\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_with_training(cls):\n",
    "        # 1. Analyze class distribution\n",
    "        # 2. Create weighted sampling\n",
    "        # 3. Set up balanced DataLoader\n",
    "        # 4. Train with proper loss function\n",
    "        # 5. Save model weights\n",
    "        return trained_model\n",
    "```\n",
    "\n",
    "### üöÄ Advanced Techniques to Explore:\n",
    "\n",
    "- **Focal Loss**: Alternative loss function for imbalanced data\n",
    "- **SMOTE**: Synthetic minority oversampling technique\n",
    "- **Ensemble Methods**: Combining multiple models\n",
    "- **Transfer Learning**: Using pre-trained models\n",
    "- **Data Augmentation**: Generating synthetic samples\n",
    "\n",
    "### üìö Useful Resources:\n",
    "\n",
    "- üìñ [Imbalanced Learn Documentation](https://imbalanced-learn.org/)\n",
    "- üõ†Ô∏è [PyTorch WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler)\n",
    "- üìë [Research: Learning from Imbalanced Data](https://link.springer.com/article/10.1007/s10115-007-0089-4)\n",
    "- üé® [Evaluation Metrics for Imbalanced Classification](https://machinelearningmastery.com/metrics-evaluate-imbalanced-classification/)\n",
    "\n",
    "### üí° Key Takeaways:\n",
    "\n",
    "1. **Weighted Sampling is Crucial**: Balance your training batches, not just your dataset\n",
    "2. **Proper Evaluation Matters**: Use precision, recall, and F1-score, not just accuracy\n",
    "3. **Regularization is Essential**: Prevent overfitting with dropout and proper architecture\n",
    "4. **Monitor Training Progress**: Track loss and class distribution in batches\n",
    "5. **Test on Balanced Data**: Your model should generalize to balanced test sets\n",
    "\n",
    "**Good luck with your implementation!** üåü\n",
    "\n",
    "Remember: The key insight is that **training on balanced batches** (via weighted sampling) allows your model to learn both classes effectively, even when the original dataset is highly imbalanced. This technique is fundamental to many real-world applications where class imbalance is common.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
