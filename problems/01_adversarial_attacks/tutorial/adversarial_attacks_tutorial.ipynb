{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Tutorial: Adversarial Attacks with FGSM\n",
    "\n",
    "![Adversarial Examples](https://docs.pytorch.org/tutorials/_images/fgsm_panda_image.png)\n",
    "\n",
    "## Welcome to the Fascinating World of Adversarial Attacks! üéØ\n",
    "\n",
    "In this comprehensive tutorial, you'll learn:\n",
    "- ü§ñ What are adversarial attacks and why they matter\n",
    "- üî¨ The mathematics behind Fast Gradient Sign Method (FGSM)\n",
    "- üíª Hands-on implementation in PyTorch\n",
    "- üé® Visualization of attack effects\n",
    "- üß™ Interactive exercises to build your skills\n",
    "\n",
    "By the end, you'll be ready to implement sophisticated adversarial attacks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "1. [üéì Understanding Adversarial Attacks](#1--understanding-adversarial-attacks)\n",
    "2. [üßÆ Mathematical Foundation](#2--mathematical-foundation)\n",
    "3. [üîß Setting Up the Environment](#3--setting-up-the-environment)\n",
    "4. [üèóÔ∏è Building a Target Model](#4--building-a-target-model)\n",
    "5. [üéØ Your First Attack](#5--your-first-attack)\n",
    "6. [üìà Fast Gradient Sign Method (FGSM)](#6--fast-gradient-sign-method-fgsm)\n",
    "7. [üíº Building the Complete Solution](#7--building-the-complete-solution)\n",
    "8. [üé® Visualizing Attack Effects](#8--visualizing-attack-effects)\n",
    "9. [üéÆ Interactive Exercises](#9--interactive-exercises)\n",
    "10. [üöÄ Next Steps and Advanced Topics](#10--next-steps-and-advanced-topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üéì Understanding Adversarial Attacks\n",
    "\n",
    "### What are Adversarial Attacks?\n",
    "\n",
    "Imagine you have a perfect photo of a panda üêº. A human can clearly see it's a panda. But what if we could add tiny, nearly invisible changes to the image that make a neural network think it's a gibbon? That's an **adversarial attack**!\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Original Image**: The clean, unmodified input\n",
    "- **Perturbation**: Small noise added to the image  \n",
    "- **Adversarial Example**: Original + Perturbation\n",
    "- **Attack Success**: When the model misclassifies the adversarial example\n",
    "\n",
    "### Mathematical Definition:\n",
    "\n",
    "$$x_{adv} = x + \\delta$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original input\n",
    "- $\\delta$ is the perturbation (very small!)\n",
    "- $x_{adv}$ is the adversarial example\n",
    "\n",
    "### Why Should We Care?\n",
    "\n",
    "üöó **Autonomous vehicles**: Malicious signs could fool self-driving cars  \n",
    "üè• **Medical diagnosis**: Adversarial examples in medical imaging  \n",
    "üîí **Security systems**: Fooling facial recognition systems  \n",
    "üõ°Ô∏è **Model robustness**: Understanding vulnerabilities helps build better defenses  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üßÆ Mathematical Foundation\n",
    "\n",
    "Before diving into implementation, let's understand the math behind adversarial attacks.\n",
    "\n",
    "### The Loss Function Perspective\n",
    "\n",
    "Neural networks learn by minimizing a loss function $L(\\theta, x, y)$ where:\n",
    "- $\\theta$ are the model parameters\n",
    "- $x$ is the input\n",
    "- $y$ is the true label\n",
    "\n",
    "### The Adversarial Objective\n",
    "\n",
    "For attacks, we want to **maximize** the loss by modifying the input:\n",
    "\n",
    "$$\\max_{\\delta} L(\\theta, x + \\delta, y) \\text{ subject to } ||\\delta||_\\infty \\leq \\epsilon$$\n",
    "\n",
    "This means: \"Find the smallest perturbation $\\delta$ that maximizes the model's confusion\"\n",
    "\n",
    "### Gradient-Based Attacks\n",
    "\n",
    "The key insight: **gradients tell us how to change inputs to increase loss!**\n",
    "\n",
    "$$\\delta = \\epsilon \\cdot \\text{sign}(\\nabla_x L(\\theta, x, y))$$\n",
    "\n",
    "This is the heart of **Fast Gradient Sign Method (FGSM)**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight\n",
    "\n",
    "**Why does this work?**\n",
    "\n",
    "Neural networks are trained to be smooth and continuous. Small changes in input should lead to small changes in output. But this smoothness is also their weakness - we can exploit the gradient information to find directions that maximally confuse the model!\n",
    "\n",
    "Think of it like finding the steepest hill to climb - gradients point us in the right direction!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Setting Up the Environment\n",
    "\n",
    "Let's start by importing all the necessary libraries and setting up our environment. We'll be working with PyTorch for neural networks and various other libraries for data manipulation and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for our adversarial attacks tutorial\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# For image similarity metrics\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our target neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN for MNIST digit classification.\n",
    "    This will be our target for adversarial attacks!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # First convolutional layer: 1 input channel (grayscale), 32 output channels\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0)\n",
    "        # Second convolutional layer: 32 input channels, 64 output channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 11 * 11, 128)  # 64 channels * 11x11 feature maps\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first conv layer + ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Apply second conv layer + ReLU + pooling\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        # Flatten for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Apply first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Apply output layer with log_softmax\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Create our model\n",
    "net = SimpleNet()\n",
    "print(\"üß† Neural network created!\")\n",
    "print(f\"üìä Model has {sum(p.numel() for p in net.parameters())} parameters\")\n",
    "\n",
    "# Move to device\n",
    "net = net.to(device)\n",
    "print(f\"üî• Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèóÔ∏è Building a Target Model\n",
    "\n",
    "Before we can attack a neural network, we need one to attack! Let's create a simple convolutional neural network for MNIST digit classification. This will be our target model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Create Some Sample Data\n",
    "\n",
    "For this tutorial, we'll create a simple sample dataset to work with. In the real problem, you'll use the provided dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "# In practice, you'll load this from the provided dataset\n",
    "\n",
    "\n",
    "def create_sample_data(n_samples=100):\n",
    "    \"\"\"Create sample MNIST-like data for demonstration\"\"\"\n",
    "    # Create random 28x28 images (normalized to [-1, 1])\n",
    "    data = np.random.randn(n_samples, 28, 28) * 0.3\n",
    "    # Create random labels (0-9)\n",
    "    labels = np.random.randint(0, 10, n_samples)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def normalize_samples(samples):\n",
    "    \"\"\"Normalize samples to [-1, 1] range (same as in the problem)\"\"\"\n",
    "    assert len(samples.shape) == 3\n",
    "    samples = samples.reshape(-1, 28 * 28)\n",
    "    minimum_values = np.min(samples, axis=1)\n",
    "    maximum_values = np.max(samples, axis=1)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    range_values = maximum_values - minimum_values\n",
    "    range_values[range_values == 0] = 1\n",
    "\n",
    "    normalized_samples = (\n",
    "        2 * (samples - minimum_values[:, np.newaxis]) / range_values[:, np.newaxis]\n",
    "    ) - 1\n",
    "\n",
    "    normalized_samples = normalized_samples.reshape(normalized_samples.shape[0], 28, 28)\n",
    "    return normalized_samples\n",
    "\n",
    "\n",
    "# Create sample data\n",
    "X_sample, y_sample = create_sample_data(10)\n",
    "X_sample = normalize_samples(X_sample)\n",
    "\n",
    "print(f\"üìä Created sample dataset:\")\n",
    "print(f\"   Shape: {X_sample.shape}\")\n",
    "print(f\"   Value range: [{X_sample.min():.3f}, {X_sample.max():.3f}]\")\n",
    "print(f\"   Labels: {y_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üéØ Your First Attack\n",
    "\n",
    "Now let's implement our first adversarial attack! We'll start with a simple version to understand the core concepts.\n",
    "\n",
    "### Understanding Gradients in Attacks\n",
    "\n",
    "The key insight is that we need to:\n",
    "1. Forward pass: Get the model's prediction\n",
    "2. Compute loss: Calculate how wrong we want the model to be\n",
    "3. Backward pass: Get gradients with respect to input pixels\n",
    "4. Create perturbation: Use gradients to modify the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attack_demo(model, image, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    A simple demonstration of how adversarial attacks work.\n",
    "    This is a basic version to understand the concepts.\n",
    "    \"\"\"\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert numpy to tensor and add batch dimension\n",
    "    image_tensor = (\n",
    "        torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    )\n",
    "    image_tensor.requires_grad = True  # This is crucial for computing gradients!\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = output.argmax(dim=1)\n",
    "    print(f\"üéØ Original prediction: class {predicted_class.item()}\")\n",
    "\n",
    "    # Compute loss - we want to maximize it!\n",
    "    loss = F.nll_loss(output, predicted_class)\n",
    "\n",
    "    # Backward pass to get gradients\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Get the gradients\n",
    "    data_grad = image_tensor.grad.data\n",
    "\n",
    "    # Create the perturbation using the sign of gradients\n",
    "    perturbation = epsilon * data_grad.sign()\n",
    "\n",
    "    # Apply the perturbation\n",
    "    perturbed_image = image_tensor + perturbation\n",
    "\n",
    "    # Clamp to valid range [-1, 1]\n",
    "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
    "\n",
    "    # Test the perturbed image\n",
    "    with torch.no_grad():\n",
    "        perturbed_output = model(perturbed_image)\n",
    "        perturbed_prediction = perturbed_output.argmax(dim=1)\n",
    "\n",
    "    print(f\"üí• After attack: class {perturbed_prediction.item()}\")\n",
    "    print(\n",
    "        f\"üéâ Attack {'successful' if predicted_class != perturbed_prediction else 'failed'}!\"\n",
    "    )\n",
    "\n",
    "    return perturbed_image.squeeze().cpu().numpy(), perturbation.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "# Let's try our first attack!\n",
    "print(\"üöÄ Attempting our first adversarial attack...\")\n",
    "original_img = X_sample[0]\n",
    "attacked_img, perturbation = simple_attack_demo(net, original_img, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìà Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "Now let's dive deeper into FGSM and implement the advanced version with masking and optimization techniques, similar to what you saw in the solution.\n",
    "\n",
    "### Key Improvements in Advanced FGSM:\n",
    "\n",
    "1. **Gradient Masking**: Focus only on the most influential pixels\n",
    "2. **Quantile-based Selection**: Use statistical thresholds to choose pixels\n",
    "3. **Batch Processing**: Efficiently process multiple images\n",
    "4. **Robust Perturbation**: Better control over attack strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    Advanced FGSM implementation with gradient masking.\n",
    "\n",
    "    This is the core FGSM function similar to what you'll implement in the solution.\n",
    "\n",
    "    Args:\n",
    "        image: Tensor with image to perturb [batch_size, channels, height, width]\n",
    "        epsilon: Attack strength - maximum perturbation value (usually 0.1-0.3)\n",
    "        data_grad: Gradient of loss function with respect to image pixels\n",
    "\n",
    "    Returns:\n",
    "        perturbed_image: The adversarially perturbed image\n",
    "    \"\"\"\n",
    "    # Take absolute value to find most influential pixels\n",
    "    abs_data_grad = data_grad.abs()\n",
    "\n",
    "    # ADVANCED TECHNIQUE: Gradient masking\n",
    "    # Instead of using all pixels, focus on the most important ones\n",
    "    # Use 70th percentile (top 30% of most influential pixels)\n",
    "    threshold = torch.quantile(abs_data_grad, 0.7)\n",
    "    mask = (abs_data_grad >= threshold).float()\n",
    "    masked_data_grad = data_grad * mask\n",
    "\n",
    "    # Take the sign of the gradient - this is the core of FGSM!\n",
    "    sign_data_grad = masked_data_grad.sign()\n",
    "\n",
    "    # Create perturbation: epsilon controls the attack strength\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "\n",
    "    # Clamp to valid pixel range [-1, 1]\n",
    "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
    "\n",
    "    return perturbed_image\n",
    "\n",
    "\n",
    "# Test the advanced FGSM function\n",
    "print(\"üî¨ Testing advanced FGSM with gradient masking...\")\n",
    "\n",
    "# Convert sample image to tensor\n",
    "test_image = (\n",
    "    torch.tensor(X_sample[0], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    ")\n",
    "test_image.requires_grad = True\n",
    "\n",
    "# Forward pass\n",
    "net.eval()\n",
    "output = net(test_image)\n",
    "pred_class = output.argmax(dim=1)\n",
    "print(f\"Original prediction: {pred_class.item()}\")\n",
    "\n",
    "# Compute loss and gradients\n",
    "loss = F.nll_loss(output, pred_class)\n",
    "net.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Apply advanced FGSM\n",
    "attacked_image = fgsm_attack(test_image, epsilon=0.25, data_grad=test_image.grad.data)\n",
    "\n",
    "# Test result\n",
    "with torch.no_grad():\n",
    "    attacked_output = net(attacked_image)\n",
    "    attacked_pred = attacked_output.argmax(dim=1)\n",
    "    print(f\"After advanced attack: {attacked_pred.item()}\")\n",
    "    print(f\"Attack success: {pred_class.item() != attacked_pred.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Interactive Exercise 2\n",
    "\n",
    "**Your Turn!** Try modifying the epsilon value and threshold in the FGSM function above.\n",
    "\n",
    "1. What happens when you increase epsilon from 0.25 to 0.3?\n",
    "2. What happens when you change the quantile threshold from 0.7 to 0.5?\n",
    "3. Can you make the attack more subtle but still effective?\n",
    "\n",
    "*(Experiment with the code above and write your observations below)*\n",
    "\n",
    "**Your observations**: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üíº Building the Complete Solution\n",
    "\n",
    "Now let's build the complete `perturbe_dataset` function that processes an entire dataset, just like in the final solution. This will tie everything together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbe_dataset(original_dataset):\n",
    "    \"\"\"\n",
    "    Complete function to perturb an entire dataset using FGSM attacks.\n",
    "\n",
    "    This is the main function you'll need to implement for the problem!\n",
    "\n",
    "    Args:\n",
    "        original_dataset: Numpy array of shape (num_images, 28, 28)\n",
    "\n",
    "    Returns:\n",
    "        perturbed_dataset: Adversarially modified dataset (same shape)\n",
    "    \"\"\"\n",
    "    global net  # Use the global network\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # INPUT VALIDATION\n",
    "    assert (\n",
    "        len(original_dataset.shape) == 3\n",
    "    ), \"Dataset must have 3 dimensions: (batch, height, width)\"\n",
    "\n",
    "    # PREPARE DATA\n",
    "    # Convert numpy to PyTorch tensor and add channel dimension\n",
    "    original_dataset = (\n",
    "        torch.tensor(original_dataset, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    )\n",
    "    perturbed_dataset = deepcopy(original_dataset)\n",
    "\n",
    "    # PREPARE MODEL\n",
    "    net.eval()  # Set to evaluation mode (important!)\n",
    "    net.to(device)\n",
    "\n",
    "    # ATTACK HYPERPARAMETER\n",
    "    epsilon = 0.29  # Attack strength - experiment with this!\n",
    "\n",
    "    print(f\"üî• Starting adversarial attack with epsilon = {epsilon}\")\n",
    "    print(f\"üìä Number of images to attack: {perturbed_dataset.size(0)}\")\n",
    "\n",
    "    # MAIN ATTACK LOOP\n",
    "    for i in range(perturbed_dataset.size(0)):\n",
    "        # Get single image and enable gradient computation\n",
    "        data = perturbed_dataset[i].unsqueeze(0).detach()  # Add batch dimension\n",
    "        data.requires_grad = True  # CRUCIAL: Enable gradient computation\n",
    "\n",
    "        # FORWARD PASS\n",
    "        output = net(data)  # Get model predictions\n",
    "\n",
    "        # Find predicted class\n",
    "        label = output.max(1, keepdim=True)[1].view(-1)\n",
    "\n",
    "        # COMPUTE LOSS\n",
    "        loss = F.nll_loss(output, label)\n",
    "\n",
    "        # BACKWARD PASS: Compute gradients\n",
    "        net.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients w.r.t. input\n",
    "        data_grad = data.grad.data  # Extract gradients\n",
    "\n",
    "        # APPLY FGSM ATTACK\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Save the perturbed image\n",
    "        perturbed_dataset[i] = perturbed_data.squeeze()\n",
    "\n",
    "        # Progress indicator\n",
    "        if (i + 1) % 2 == 0:  # Every 2 images for demo\n",
    "            print(f\"‚úÖ Processed {i + 1}/{perturbed_dataset.size(0)} images\")\n",
    "\n",
    "    # CONVERT BACK TO NUMPY\n",
    "    perturbed_dataset = perturbed_dataset.cpu().detach().numpy()\n",
    "    perturbed_dataset = perturbed_dataset.squeeze(1)  # Remove channel dimension\n",
    "\n",
    "    # OUTPUT VALIDATION\n",
    "    assert len(perturbed_dataset.shape) == 3, \"Output must have 3 dimensions\"\n",
    "    assert perturbed_dataset.shape[1] == 28, \"Height must be 28 pixels\"\n",
    "    assert perturbed_dataset.shape[2] == 28, \"Width must be 28 pixels\"\n",
    "\n",
    "    print(\"üéØ Adversarial attack completed successfully!\")\n",
    "\n",
    "    return perturbed_dataset\n",
    "\n",
    "\n",
    "# Test our complete solution!\n",
    "print(\"üöÄ Testing complete perturbe_dataset function...\")\n",
    "sample_subset = X_sample[:5]  # Use first 5 images for demo\n",
    "attacked_dataset = perturbe_dataset(sample_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üé® Visualizing Attack Effects <a id=\"section8\"></a>\n",
    "\n",
    "Visual learners rejoice! Let's create some visualizations to see how adversarial attacks actually affect images and understand what's happening under the hood.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. üé® Visualizing Attack Effects\n",
    "\n",
    "Visual learners rejoice! Let's create some visualizations to see how adversarial attacks actually affect images and understand what's happening under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more meaningful sample image for visualization\n",
    "def create_pattern_image():\n",
    "    \"\"\"Create a simple pattern image that's easier to see changes in\"\"\"\n",
    "    img = np.zeros((28, 28))\n",
    "    # Create a simple pattern - diagonal lines and squares\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if i == j or i + j == 27:  # Diagonals\n",
    "                img[i, j] = 1.0\n",
    "            elif 8 <= i <= 19 and 8 <= j <= 19:  # Center square\n",
    "                if i % 2 == j % 2:\n",
    "                    img[i, j] = 0.5\n",
    "    return img\n",
    "\n",
    "\n",
    "# Create pattern image\n",
    "pattern_img = create_pattern_image()\n",
    "pattern_img_normalized = normalize_samples(pattern_img.reshape(1, 28, 28))[0]\n",
    "\n",
    "print(\"üì∏ Created pattern image for better visualization\")\n",
    "print(\n",
    "    f\"Value range: [{pattern_img_normalized.min():.3f}, {pattern_img_normalized.max():.3f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attack_comparison(\n",
    "    original, attacked, perturbation, title=\"Adversarial Attack Visualization\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization comparing original vs attacked images\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # Original image\n",
    "    axes[0, 0].imshow(original, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    axes[0, 0].set_title(\"üñºÔ∏è Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "\n",
    "    # Attacked image\n",
    "    axes[0, 1].imshow(attacked, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    axes[0, 1].set_title(\"üí• Attacked Image\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 1].axis(\"off\")\n",
    "\n",
    "    # Perturbation (amplified for visibility)\n",
    "    perturbation_amplified = perturbation * 10  # Amplify for visualization\n",
    "    axes[0, 2].imshow(perturbation_amplified, cmap=\"RdBu\", vmin=-1, vmax=1)\n",
    "    axes[0, 2].set_title(\n",
    "        \"üîç Perturbation (10x amplified)\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 2].axis(\"off\")\n",
    "\n",
    "    # Difference map\n",
    "    difference = np.abs(attacked - original)\n",
    "    axes[1, 0].imshow(difference, cmap=\"hot\", vmin=0, vmax=0.5)\n",
    "    axes[1, 0].set_title(\"üìä Absolute Difference\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "\n",
    "    # Histogram of pixel values\n",
    "    axes[1, 1].hist(\n",
    "        original.flatten(), bins=50, alpha=0.7, label=\"Original\", color=\"blue\"\n",
    "    )\n",
    "    axes[1, 1].hist(\n",
    "        attacked.flatten(), bins=50, alpha=0.7, label=\"Attacked\", color=\"red\"\n",
    "    )\n",
    "    axes[1, 1].set_title(\"üìà Pixel Value Distributions\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_xlabel(\"Pixel Value\")\n",
    "    axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Statistics\n",
    "    axes[1, 2].axis(\"off\")\n",
    "    max_pert = np.max(np.abs(perturbation))\n",
    "    mean_pert = np.mean(np.abs(perturbation))\n",
    "    ssim_val = ssim(original, attacked, data_range=2)\n",
    "    pixels_changed = np.sum(np.abs(perturbation) > 1e-6)\n",
    "    attack_strength = np.sqrt(np.mean(perturbation**2))\n",
    "\n",
    "    stats_text = f\"\"\"üìä Attack Statistics:\n",
    "\n",
    "‚Ä¢ Max perturbation: {max_pert:.4f}\n",
    "‚Ä¢ Mean perturbation: {mean_pert:.4f}\n",
    "‚Ä¢ SSIM similarity: {ssim_val:.4f}\n",
    "‚Ä¢ Pixels changed: {pixels_changed}/{original.size}\n",
    "‚Ä¢ Attack strength: {attack_strength:.4f}\"\"\"\n",
    "\n",
    "    axes[1, 2].text(\n",
    "        0.1,\n",
    "        0.5,\n",
    "        stats_text,\n",
    "        fontsize=11,\n",
    "        verticalalignment=\"center\",\n",
    "        fontfamily=\"monospace\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\"),\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"üìä Visualization function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the attack on our pattern image\n",
    "print(\"üé® Creating visual demonstration of adversarial attack...\")\n",
    "\n",
    "# Convert pattern image to tensor\n",
    "demo_image = (\n",
    "    torch.tensor(pattern_img_normalized, dtype=torch.float32)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "demo_image.requires_grad = True\n",
    "\n",
    "# Apply attack\n",
    "net.eval()\n",
    "output = net(demo_image)\n",
    "pred_class = output.argmax(dim=1)\n",
    "loss = F.nll_loss(output, pred_class)\n",
    "\n",
    "net.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Create attacked version with moderate epsilon\n",
    "demo_attacked = fgsm_attack(demo_image, epsilon=0.2, data_grad=demo_image.grad.data)\n",
    "demo_perturbation = (demo_attacked - demo_image).squeeze().cpu().detach().numpy()\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "demo_original = demo_image.squeeze().cpu().detach().numpy()\n",
    "demo_attacked_np = demo_attacked.squeeze().cpu().detach().numpy()\n",
    "\n",
    "print(f\"üìä Original prediction: class {pred_class.item()}\")\n",
    "\n",
    "# Test attacked version\n",
    "with torch.no_grad():\n",
    "    attacked_output = net(demo_attacked)\n",
    "    attacked_pred = attacked_output.argmax(dim=1)\n",
    "    print(f\"üí• Attacked prediction: class {attacked_pred.item()}\")\n",
    "\n",
    "# Visualize the results\n",
    "visualize_attack_comparison(\n",
    "    demo_original,\n",
    "    demo_attacked_np,\n",
    "    demo_perturbation,\n",
    "    \"FGSM Attack Visualization - Pattern Image\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_epsilon_effects(original_img, epsilons=[0.1, 0.2, 0.3]):\n",
    "    \"\"\"\n",
    "    Compare the effect of different epsilon values\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, len(epsilons) + 1, figsize=(15, 8))\n",
    "    fig.suptitle(\n",
    "        \"üî¨ Effect of Different Epsilon Values\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = (\n",
    "        torch.tensor(original_img, dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    # Get gradients\n",
    "    net.eval()\n",
    "    output = net(image_tensor)\n",
    "    pred_class = output.argmax(dim=1)\n",
    "    loss = F.nll_loss(output, pred_class)\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = image_tensor.grad.data\n",
    "\n",
    "    # Original image\n",
    "    axes[0, 0].imshow(original_img, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    axes[0, 0].set_title(\"Original\", fontweight=\"bold\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "\n",
    "    # Test different epsilon values\n",
    "    results = []\n",
    "    for i, eps in enumerate(epsilons):\n",
    "        # Apply attack\n",
    "        attacked = fgsm_attack(image_tensor, eps, data_grad)\n",
    "        attacked_np = attacked.squeeze().cpu().detach().numpy()\n",
    "        perturbation = (attacked - image_tensor).squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # Test prediction\n",
    "        with torch.no_grad():\n",
    "            attacked_output = net(attacked)\n",
    "            attacked_pred = attacked_output.argmax(dim=1)\n",
    "            confidence = torch.softmax(attacked_output, dim=1).max().item()\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"epsilon\": eps,\n",
    "                \"attacked\": attacked_np,\n",
    "                \"perturbation\": perturbation,\n",
    "                \"prediction\": attacked_pred.item(),\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Plot attacked image\n",
    "        axes[0, i + 1].imshow(attacked_np, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        axes[0, i + 1].set_title(\n",
    "            f\"Œµ = {eps}\\\\nPred: {attacked_pred.item()}\", fontweight=\"bold\"\n",
    "        )\n",
    "        axes[0, i + 1].axis(\"off\")\n",
    "\n",
    "        # Plot perturbation\n",
    "        axes[1, i + 1].imshow(perturbation * 10, cmap=\"RdBu\", vmin=-1, vmax=1)\n",
    "        axes[1, i + 1].set_title(\n",
    "            f\"Perturbation (10x)\\\\nConf: {confidence:.2f}\", fontsize=10\n",
    "        )\n",
    "        axes[1, i + 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Demonstrate epsilon comparison\n",
    "print(\"üî¨ Comparing different epsilon values...\")\n",
    "epsilon_results = compare_epsilon_effects(pattern_img_normalized, [0.1, 0.2, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéÆ Interactive Exercises\n",
    "\n",
    "Now that you understand the concepts and have seen the visualizations, let's practice with some hands-on exercises!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3: Implement Your Own Attack Variant\n",
    "\n",
    "**Challenge**: Modify the FGSM attack to use different strategies. Try implementing:\n",
    "\n",
    "1. **Random Sign Attack**: Instead of using gradient signs, use random signs\n",
    "2. **Targeted Attack**: Try to make the model predict a specific wrong class\n",
    "3. **Iterative FGSM**: Apply FGSM multiple times with smaller epsilon values\n",
    "\n",
    "Use the cell below to experiment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Experiment playground - Try your own attack variations!\n",
    "\n",
    "\n",
    "def random_sign_attack(image, epsilon):\n",
    "    \"\"\"\n",
    "    Random sign attack - uses random directions instead of gradients\n",
    "    \"\"\"\n",
    "    random_signs = torch.sign(torch.randn_like(image))\n",
    "    perturbed = image + epsilon * random_signs\n",
    "    return torch.clamp(perturbed, -1, 1)\n",
    "\n",
    "\n",
    "def targeted_fgsm_attack(image, epsilon, data_grad, target_class):\n",
    "    \"\"\"\n",
    "    Targeted FGSM - tries to make the model predict a specific class\n",
    "    Instead of maximizing loss, we minimize loss for the target class\n",
    "    \"\"\"\n",
    "    # For targeted attacks, we subtract (not add) the gradient sign\n",
    "    # This moves toward the target class instead of away from the original\n",
    "    abs_data_grad = data_grad.abs()\n",
    "    threshold = torch.quantile(abs_data_grad, 0.7)\n",
    "    mask = (abs_data_grad >= threshold).float()\n",
    "    masked_data_grad = data_grad * mask\n",
    "\n",
    "    sign_data_grad = masked_data_grad.sign()\n",
    "    perturbed_image = image - epsilon * sign_data_grad  # Note the minus sign!\n",
    "    return torch.clamp(perturbed_image, -1, 1)\n",
    "\n",
    "\n",
    "def iterative_fgsm_attack(image, epsilon, model, num_steps=5):\n",
    "    \"\"\"\n",
    "    Iterative FGSM - apply attack in multiple small steps\n",
    "    \"\"\"\n",
    "    alpha = epsilon / num_steps  # Step size\n",
    "    perturbed = image.clone()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        perturbed.requires_grad = True\n",
    "        output = model(perturbed)\n",
    "        pred_class = output.argmax(dim=1)\n",
    "        loss = F.nll_loss(output, pred_class)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Take a small step\n",
    "        data_grad = perturbed.grad.data\n",
    "        perturbed = perturbed + alpha * data_grad.sign()\n",
    "\n",
    "        # Ensure we stay within epsilon budget from original\n",
    "        perturbation = perturbed - image\n",
    "        perturbation = torch.clamp(perturbation, -epsilon, epsilon)\n",
    "        perturbed = image + perturbation\n",
    "        perturbed = torch.clamp(perturbed, -1, 1)\n",
    "        perturbed = perturbed.detach()\n",
    "\n",
    "    return perturbed\n",
    "\n",
    "\n",
    "print(\"üéÆ Attack variants ready for experimentation!\")\n",
    "print(\n",
    "    \"Try calling: random_sign_attack(), targeted_fgsm_attack(), or iterative_fgsm_attack()\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 4: Compare Attack Methods\n",
    "\n",
    "**Your Turn!** Use the code below to compare different attack methods on the same image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different attack methods\n",
    "def compare_attack_methods():\n",
    "    \"\"\"Compare FGSM, Random, and Iterative attacks\"\"\"\n",
    "    test_img = (\n",
    "        torch.tensor(pattern_img_normalized, dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "    test_img.requires_grad = True\n",
    "\n",
    "    # Get original prediction and gradients\n",
    "    net.eval()\n",
    "    output = net(test_img)\n",
    "    original_pred = output.argmax(dim=1).item()\n",
    "    loss = F.nll_loss(output, output.argmax(dim=1))\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    grad = test_img.grad.data\n",
    "\n",
    "    epsilon = 0.2\n",
    "\n",
    "    # Apply different attacks\n",
    "    fgsm_result = fgsm_attack(test_img, epsilon, grad)\n",
    "    random_result = random_sign_attack(test_img, epsilon)\n",
    "    iterative_result = iterative_fgsm_attack(test_img, epsilon, net, num_steps=5)\n",
    "\n",
    "    # Test predictions\n",
    "    attacks = [\n",
    "        (\"Original\", test_img, original_pred),\n",
    "        (\"FGSM\", fgsm_result, None),\n",
    "        (\"Random\", random_result, None),\n",
    "        (\"Iterative\", iterative_result, None),\n",
    "    ]\n",
    "\n",
    "    # Get predictions for attacked images\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, len(attacks)):\n",
    "            name, img, _ = attacks[i]\n",
    "            pred = net(img).argmax(dim=1).item()\n",
    "            attacks[i] = (name, img, pred)\n",
    "\n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    fig.suptitle(\"üî¨ Comparison of Attack Methods\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    for i, (name, img, pred) in enumerate(attacks):\n",
    "        img_np = img.squeeze().cpu().detach().numpy()\n",
    "        axes[i].imshow(img_np, cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        axes[i].set_title(f\"{name}\\\\nPrediction: {pred}\", fontweight=\"bold\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print success rates\n",
    "    print(\"\\\\nüìä Attack Results:\")\n",
    "    for name, _, pred in attacks[1:]:  # Skip original\n",
    "        success = \"‚úÖ Success\" if pred != original_pred else \"‚ùå Failed\"\n",
    "        print(f\"  {name}: {success} (Original: {original_pred} ‚Üí New: {pred})\")\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "print(\"üéØ Comparing different attack methods...\")\n",
    "compare_attack_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üöÄ Next Steps and Advanced Topics\n",
    "\n",
    "Congratulations! üéâ You've learned the fundamentals of adversarial attacks and seen them in action. Here's what you can explore next:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Attack Methods:\n",
    "- **PGD (Projected Gradient Descent)**: Iterative refinement of FGSM\n",
    "- **C&W Attack**: Optimization-based approach with different loss functions  \n",
    "- **DeepFool**: Minimal perturbation attacks\n",
    "- **Adversarial Patches**: Physical-world attacks\n",
    "\n",
    "### Defense Techniques:\n",
    "- **Adversarial Training**: Train models on adversarial examples\n",
    "- **Gradient Masking**: Hide gradient information  \n",
    "- **Input Preprocessing**: Denoise or transform inputs\n",
    "- **Certified Defenses**: Provable robustness guarantees\n",
    "\n",
    "### Useful Resources:\n",
    "- üìö [Adversarial Examples in the Physical World](https://arxiv.org/abs/1607.02533)\n",
    "- üìö [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)\n",
    "- üõ†Ô∏è [CleverHans Library](https://github.com/cleverhans-lab/cleverhans)\n",
    "- üõ†Ô∏è [Adversarial Robustness Toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **üîç Adversarial Attacks Fundamentals**:\n",
    "   - Small perturbations can fool neural networks\n",
    "   - Attacks exploit the smoothness of neural networks\n",
    "   - Balance between attack effectiveness and perturbation size\n",
    "\n",
    "2. **üßÆ FGSM (Fast Gradient Sign Method)**:\n",
    "   - Uses gradient information to find optimal perturbations\n",
    "   - Simple yet effective: `Œ¥ = Œµ √ó sign(‚àá_x L(Œ∏, x, y))`\n",
    "   - Can be enhanced with gradient masking and quantile thresholds\n",
    "\n",
    "3. **üíª Implementation Skills**:\n",
    "   - PyTorch gradient computation with `requires_grad=True`\n",
    "   - Batch processing for efficient attacks\n",
    "   - Proper tensor manipulation and device handling\n",
    "\n",
    "4. **üõ°Ô∏è Advanced Techniques**:\n",
    "   - Gradient masking for focused attacks\n",
    "   - Iterative approaches for stronger attacks\n",
    "   - Evaluation metrics (SSIM, accuracy drop)\n",
    "\n",
    "### For the Problem Solution:\n",
    "You now have all the knowledge to implement the `perturbe_dataset` function! The key components are:\n",
    "- Use the `fgsm_attack` function with gradient masking\n",
    "- Process each image individually with gradient computation\n",
    "- Choose appropriate epsilon value (around 0.29)\n",
    "- Ensure proper input/output format validation\n",
    "\n",
    "**Good luck with your implementation!** üöÄ\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
